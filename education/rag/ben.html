<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-06-12">

<title>Beyond the Basics of RAG â€“ Parlance</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../education/rag/jason.html" rel="next">
<link href="../../education/rag/jo.html" rel="prev">
<link href="../../b.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QXGQ6F7NKT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QXGQ6F7NKT', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Beyond the Basics of RAG â€“ Parlance">
<meta property="og:description" content="LLMs are powerful, but have limitations: their knowledge is fixed in their weights, and their context window is limited. Worse: when they donâ€™t know something, they might just make it up. RAG, for Retrieval Augmented Generation, has emerged as a way to mitigate both of those problems. However, implementing RAG effectively is more complex than it seems. The nitty gritty parts of what makes good retrieval good are rarely talked about: No, cosine similarity is, in fact, not all you need. In this workshop, we explore what helps build a robust RAG pipeline, and how simple insights from retrieval research can greatly improve your RAG efforts. Weâ€™ll cover key topics like BM25, re-ranking, indexing, domain specificity, evaluation beyond LGTM@few, and filtering. Be prepared for a whole new crowd of incredibly useful buzzwords to enter your vocabulary.">
<meta property="og:image" content="https://parlance-labs.com/education/rag/ben.jpg">
<meta property="og:site_name" content="Parlance">
<meta name="twitter:title" content="Beyond the Basics of RAG â€“ Parlance">
<meta name="twitter:description" content="LLMs are powerful, but have limitations: their knowledge is fixed in their weights, and their context window is limited. Worse: when they donâ€™t know something, they might just make it up. RAG, for Retrieval Augmented Generation, has emerged as a way to mitigate both of those problems. However, implementing RAG effectively is more complex than it seems. The nitty gritty parts of what makes good retrieval good are rarely talked about: No, cosine similarity is, in fact, not all you need. In this workshop, we explore what helps build a robust RAG pipeline, and how simple insights from retrieval research can greatly improve your RAG efforts. Weâ€™ll cover key topics like BM25, re-ranking, indexing, domain specificity, evaluation beyond LGTM@few, and filtering. Be prepared for a whole new crowd of incredibly useful buzzwords to enter your vocabulary.">
<meta name="twitter:image" content="https://parlance-labs.com/education/rag/ben.jpg">
<meta name="twitter:creator" content="@HamelHusain">
<meta name="twitter:site" content="@HamelHusain">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Parlance</span>
    </a>
  </div>
          <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../services.html"> 
<span class="menu-text">Work With Us</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://hamel.dev"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../team.html"> 
<span class="menu-text">Team</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../education/"> 
<span class="menu-text">Education</span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
            <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../education/rag/index.html">RAG</a></li><li class="breadcrumb-item"><a href="../../education/rag/ben.html">Beyond the Basics of RAG</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Educational Resources</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/evals/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Evals</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/evals/allaire.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inspect, An OSS framework for LLM evals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/evals/ankur.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLM Eval For Text2SQL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/evals/schoelkopf.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A Deep Dive on LLM Evaluation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/rag/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">RAG</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/rag/jo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Back to Basics for RAG</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/rag/ben.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Beyond the Basics of RAG</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/rag/jason.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Systematically improving RAG applications</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/index.html#fine-tuning" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fine-Tuning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/index.html#should-you-fine-tune" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Should you fine-tune?</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning_course/workshop_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">When and Why to Fine Tune an LLM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/kyle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fine-tuning when youâ€™ve already deployed LLMs in prod</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/emmanuel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Why Fine Tuning is Dead</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/index.html#how-to-fine-tune" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to fine-tune</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/daniel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Creating, curating, and cleaning data for LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/mistral_ft_sophia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Best Practices For Fine Tuning Mistral</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/abhishek.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Train (almost) any LLM using ðŸ¤— autotrain</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/steven.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fine Tuning OpenAI Models - Best Practices</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning_course/workshop_4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deploying Fine-Tuned Models</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/index.html#advanced-topics-in-fine-tuning" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced topics in fine-tuning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/napkin_math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Napkin Math For Fine Tuning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/slaying_ooms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Slaying OOMs with PyTorch FSDP and torchao</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/pawel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fine Tuning LLMs for Function Calling</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/applications/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Applications</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
 <span class="menu-text">education/applications/**/*.qmd</span>
  </li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/prompt_eng/berryman.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapters" id="toc-chapters" class="nav-link active" data-scroll-target="#chapters">Chapters</a></li>
  <li><a href="#slides" id="toc-slides" class="nav-link" data-scroll-target="#slides">Slides</a></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources">Additional Resources</a></li>
  <li><a href="#full-transcript" id="toc-full-transcript" class="nav-link" data-scroll-target="#full-transcript">Full Transcript</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/parlance-labs/website/edit/main/education/rag/ben.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    <div class="quarto-margin-footer"><div class="margin-footer-item">
<button onclick="window.location.href='https://hamel.ck.page/7d15a4b6e7'" style="background-color: #C75C56; color: white; padding: 12px 24px; border: none; border-radius: 6px; font-size: 16px; cursor: pointer; transition: background-color 0.3s ease;">
Subscribe To Our Newsletter
</button>
</div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../education/rag/index.html">RAG</a></li><li class="breadcrumb-item"><a href="../../education/rag/ben.html">Beyond the Basics of RAG</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Beyond the Basics of RAG</h1>
  <div class="quarto-categories">
    <div class="quarto-category">RAG</div>
    <div class="quarto-category">llm-conf-2024</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 12, 2024</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>LLMs are powerful, but have limitations: their knowledge is fixed in their weights, and their context window is limited. Worse: when they donâ€™t know something, they might just make it up. RAG, for Retrieval Augmented Generation, has emerged as a way to mitigate both of those problems. However, implementing RAG effectively is more complex than it seems. The nitty gritty parts of what makes good retrieval good are rarely talked about: No, cosine similarity is, in fact, not all you need. In this workshop, we explore what helps build a robust RAG pipeline, and how simple insights from retrieval research can greatly improve your RAG efforts. Weâ€™ll cover key topics like BM25, re-ranking, indexing, domain specificity, evaluation beyond LGTM@few, and filtering. Be prepared for a whole new crowd of incredibly useful buzzwords to enter your vocabulary.</p>
  </div>
</div>


</header>


<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0nA5QG3087g" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>This talk was given by <a href="https://x.com/bclavie">Ben ClaviÃ©</a> at the <a href="https://maven.com/parlance-labs/fine-tuning">Mastering LLMs Conference</a>.</p>
<div class="mobile-only callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Subscribe For More Educational Content
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you enjoyed this content, subscribe to receive updates on new educational content for LLMs.</p>
<center>
<script async="" data-uid="6379a28bdb" src="https://hamel.ck.page/6379a28bdb/index.js"></script>
</center>
</div>
</div>
<section id="chapters" class="level2">
<h2 class="anchored" data-anchor-id="chapters">Chapters</h2>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=0">00:00</a> Introduction</strong></p>
<p>Hamel introduces Ben Clavier, a researcher at Answer.ai with a strong background in information retrieval and the creator of the RAGatouille library.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=48">00:48</a> Benâ€™s Background</strong></p>
<p>Ben shares his journey into AI and information retrieval, his work at Answer.ai, and the open-source libraries he maintains, including ReRankers.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=140">02:20</a> Agenda</strong></p>
<p>Ben defines Retrieval-Augmented Generation (RAG), clarifies common misconceptions, and explains that RAG is not a silver bullet or an end-to-end system.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=301">05:01</a> RAG Basics and Limitations</strong></p>
<p>Ben explains the basic mechanics of RAG, emphasizing that it is simply the process of stitching retrieval and generation together, and discusses common failure points.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=389">06:29</a> RAG MVP Pipeline</strong></p>
<p>Ben breaks down the simple RAG pipeline, including model loading, data encoding, cosine similarity search, and obtaining relevant documents.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=474">07:54</a> Vector Databases</strong></p>
<p>Ben explains the role of vector databases in handling large-scale document retrieval efficiently and their place in the RAG pipeline.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=526">08:46</a> Bi-Encoders</strong></p>
<p>Ben describes bi-encoders, their efficiency in pre-computing document representations, and their role in quick query encoding and retrieval.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=684">11:24</a> Cross-Encoders and Re-Ranking</strong></p>
<p>Ben introduces cross-encoders, their computational expense, and their ability to provide more accurate relevance scores by encoding query-document pairs together.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=878">14:38</a> Importance of Keyword Search</strong></p>
<p>Ben highlights the enduring relevance of keyword search methods like BM25 and their role in handling specific terms and acronyms effectively.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=924">15:24</a> Integration of Full-Text Search</strong></p>
<p>Ben discusses the integration of full-text search (TF-IDF) with vector search to handle detailed and specific queries better, especially in technical domains.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=994">16:34</a> TF-IDF and BM25</strong></p>
<p>Ben explains TF-IDF, BM25, and their implementation in modern retrieval systems, emphasizing their effectiveness despite being older techniques.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=1173">19:33</a> Combined Retrieval Approach</strong></p>
<p>Ben illustrates a combined retrieval approach using both embeddings and keyword search, recommending a balanced weighting of scores.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=1182">19:22</a> Metadata Filtering</strong></p>
<p>Ben emphasizes the importance of metadata in filtering documents, providing examples and explaining how metadata can significantly improve retrieval relevance.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=1357">22:37</a> Full Pipeline Overview</strong></p>
<p>Ben presents a comprehensive RAG pipeline incorporating bi-encoders, cross-encoders, full-text search, and metadata filtering, showing how to implement these steps in code.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=1565">26:05</a> Q&amp;A Session Introduction</strong></p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=1574">26:14</a> Fine-Tuning Bi-Encoder and Cross-Encoder Models</strong></p>
<p>Ben discusses the importance of fine-tuning bi-encoder and cross-encoder models for improved retrieval accuracy, emphasizing the need to make the bi-encoder more loose and the cross-encoder more precise.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=1619">26:59</a> Combining Scores from Different Retrieval Methods</strong></p>
<p>A participant asks about combining scores from different retrieval methods. Ben explains the pros and cons of weighted averages versus taking top candidates from multiple rankers, emphasizing the importance of context and data specifics.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=1741">29:01</a> The Importance of RAG as Context Lengths Get Longer</strong></p>
<p>Ben reflects on how RAG may evolve or change as context lengths of LLMs get larger, but emphasizing that long context lengths are not a silver bullet.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=1806">30:06</a> Chunking Strategies for Long Documents</strong></p>
<p>Ben discusses effective chunking strategies for long documents, including overlapping chunks and ensuring chunks do not cut off sentences, while considering the importance of latency tolerance in production systems.</p>
<p><strong><a href="https://youtu.be/0nA5QG3087g?t=1856">30:56</a> Fine-Tuning Encoders and Advanced Retrieval with ColBERT</strong></p>
<p>Ben also discusses when to fine-tune your encoders, and explains ColBERT for advanced retrieval.</p>
</section>
<section id="slides" class="level2">
<h2 class="anchored" data-anchor-id="slides">Slides</h2>
<p><object data="ben.pdf" type="application/pdf" width="100%" height="600"><a href="ben.pdf" download="">Download PDF file.</a></object></p>
</section>
<section id="additional-resources" class="level2">
<h2 class="anchored" data-anchor-id="additional-resources">Additional Resources</h2>
<p>The following resources were mentioned during the talk:</p>
<ul>
<li>Easily use and train state of the art late-interaction retrieval methods (ColBERT) in any RAG pipeline. https://github.com/bclavie/RAGatouille</li>
<li>A lightweight unified API for various reranking models: https://github.com/AnswerDotAI/rerankers</li>
<li>A Hackersâ€™ Guide to Language Models: https://www.youtube.com/watch?v=jkrNMKz9pWU</li>
<li>GLiNER: Generalist Model for Named Entity Recognition using Bidirectional - Transformer: https://arxiv.org/abs/2311.08526</li>
<li>Fine-Tuning with Sentence Transformers: https://www.sbert.net/docs/sentence_transformer/training_overview.html</li>
<li>Elastic, Dense vector field type: https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html</li>
</ul>
</section>
<section id="full-transcript" class="level2">
<h2 class="anchored" data-anchor-id="full-transcript">Full Transcript</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expand to see transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><br>[0:01] Hamel: Ben Clavier is one of the cracked researchers who work at Answer.ai. Youâ€™ve heard from several researchers from Answer.ai already in this conference. Ben has a background in information retrieval, amongst other things, and he has an open source package called Ragatouille, which you should check out. He also comes from a deep background in information retrieval. and brings that to RAG. And heâ€™s also one of the clearest thinkers on the topic. But yeah, Iâ€™ll hand it over to you, Ben, to kind of give more color to your background, anything that I missed. <br>[0:45] Hamel: And yeah, we can just jump into it. <br>[0:48] Ben: Okay, letâ€™s go. So I think thatâ€™s pretty much the key aspect of my background. You pretty much read this slide out. So I do R&amp;D at Ansoya with Jeremy. Youâ€™ve seen Jono in this course and thereâ€™s a lot of other awesome people. Weâ€™re a distributed R&amp;D lab, so we do AI research and we try to be as open source as possible because we want people to use what we build. Prior to joining ANSR, I did a lot of NLP and kind of stumbled upon information retrieval because itâ€™s very, very useful and everybody wants information retrieval. <br>[1:20] Ben: Itâ€™s more for clarifying what information retrieval is, which I hope todayâ€™s talk will help. And yeah, so my claim to fame or claim to moderate fame at least is the Ragatool library, which makes it much easier to use a family of models called Colbert. which we will very briefly mention today, but wonâ€™t have time to go into detail. But hopefully, like, if you want to know more about that, like, do feel free to ping me on Discord. Iâ€™m generally either very responsive or you need to ping me again. Pretty much how I work. <br>[1:50] Ben: And I also maintain the ReRankers library, which weâ€™ll discuss in one of the later slides. And yeah, if you know me, I want to follow me. I want to hear more. But what I do is pretty much all on Twitter. Iâ€™m not on LinkedIn at all. Iâ€™m just everything go through Twitter. A lot of memes and shitposts, but some very informative stuff once in a while. So. So yeah, and letâ€™s get started with what weâ€™re going to talk about today. So itâ€™s only half an hour, so weâ€™re not going to talk about a lot. <br>[2:20] Ben: Iâ€™m going to talk about why I think Iâ€™ll do like call retrieval basics as they should exist in your pipelines, because RAG is a very nebulous term and that will be the first slide and Hamel will be very happy about that slide, I think. But RAG is not a silver bullet. RAG is not a new thing from December 2022. RAG is not even an end-to-end system. Weâ€™ll cover that, but I think itâ€™s very important to like ground it a bit when we talk about RAG because it means a lot of different things to different people. <br>[2:47] Ben: Then we will cover what we call the compact MVP, which is what most people do when they are starting out with RAG. Itâ€™s actually an example from Jeremy. Itâ€™s like the simplest possible implementation of RAG, as in just using a vector search. And then the other topics are basically things that I think you should have in your rack pipeline as part of your MVP. And Iâ€™ll show that like thereâ€™s a lot of scary concepts because theyâ€™re all big walls like by encoder, cross encoder, TFIDF, BM25, filtering. <br>[3:14] Ben: That sounds like a lot, but then Iâ€™m going to try and show it that theyâ€™re very simple concepts and you can have pretty much the same MVP by adding just 10 lines of code, by choosing like by the state of the art retrieval components in every bit. And the bonus, which I donâ€™t think weâ€™ll have time to cover when I try this again, was talking about Colbert because I like talking about Colbert. So I might do it at the end if we have some time, but I might not. And yeah, thatâ€™s it for the agenda. <br>[3:40] Ben: And then I also think itâ€™s important to have the counter agenda, which is what we wonâ€™t be talking about today, because those are just as important for RAG. But they are not what we put in the very basics. And here weâ€™re very much about the basics. So one of them is. How to monitor and improve RAC systems because RACs are systems and theyâ€™re living systems and theyâ€™re very much things you should monitor and continuously improve on. I think Jaydon covered that quite well in his talk yesterday or last week. Yeah, last week. <br>[4:07] Ben: So I would invite you to watch that and watch Jaydon and Danâ€™s upcoming course if it does materialize. Evaluations, theyâ€™re also extremely important, but we wonâ€™t talk about them at all today. I know that Joe will talk about them at length in his talk. Benchmarks and paper references. So Iâ€™ll make a lot of claims that you will just have to trust me on because I donâ€™t want to have too many references or too many academic looking tables and this trying to keep it quite lively and airy. <br>[4:33] Ben: I wonâ€™t give you a rundown of all the best performing models and why you should use them. I wonâ€™t talk about training, data augmentation, et cetera. And I wonâ€™t talk about all the other cool approaches like Splayed, Colbert, and details because they go beyond the basics. But those are all very important topics, so if youâ€™re interested, do look up, thereâ€™s a lot of good resources out there. Do feel free to ask me. And with that, letâ€™s get started with the rant, which is my favorite part. <br>[5:01] Ben: This is a thing that Hamel has been doing on Twitter recently as part of his flame posting campaign, Iâ€™ll say, which is basically, thereâ€™s so much in AI, so much in especially the LLM world that uses worlds that are like a lot scarier than they need to be. And RUGâ€™s probably that because to me when I hear retrieval of matter generation or RUG, it sounds like thatâ€™s an end-to-end system, thatâ€™s a very definite set of components, thatâ€™s a thing that works on its own. <br>[5:27] Ben: And itâ€™s not, itâ€™s literally just doing retrieval to put stuff into your prompt context, like before your prompt or after your prompt, you want to get some context, so youâ€™re doing retrieval. But that means thatâ€™s not an end-to-end system, despite what Jason will have you believe on his Twitter, heâ€™s not created it, but it does make a lot of money from it. And itâ€™s basically just the act of stitching together retrieval, so the R part of RAG and generation, so the G part of RAG. like to ground the later. <br>[5:53] Ben: So you want your generation to be grounded to use some context. So youâ€™re doing retrieval on the wire of documents you have and pass it to your LLM. But thereâ€™s no magic going on. Itâ€™s very much like a pipeline that take the output of model A and gives it to model B. The generation part is whatâ€™s handled by large language models and good rags and actually three different components. Itâ€™s your good retrieval pipeline. Itâ€™s a good generative model and itâ€™s a good way of linking them up. So it can be formatting your prompt or whatever. <br>[6:20] Ben: And itâ€™s very important to think about it when youâ€™re saying my rack doesnâ€™t work. You need to be more specific like my rack doesnâ€™t work is the same as saying my car doesnâ€™t work. Itâ€™s like yeah, but something specific is broken. You need to figure out what is the retrieval part is the LLM struggling to make use of the context, etc. Thereâ€™s a lot of failure cases there. And with that being said, letâ€™s look at what the compact MVP is. <br>[6:44] Ben: So that is basically what you will see, I think, if youâ€™ve read any Medium blog post about the advent of Frag in early 2023. Thatâ€™s the pipeline that everyone used. And thatâ€™s also because the easiest pipeline to put into production is very simple. You have a query. You have an embedding model. You have documents. The documents get embedded and pulled into a single vector. Then you do cosine similarity search between the vectors for your query and for the documents. And that gets you a result. That gets you a score. <br>[7:10] Ben: And this is a bit of a teaser for an upcoming slide when I say this is called the Bayan Khodor approach, but just so you get the term in mind and Iâ€™ll define it because thatâ€™s one of those things that is like a scary term. Thatâ€™s actually very, very simple when you break it down. But first, letâ€™s look at what this actually means in code, this whole pipeline. So the first thing you want to do is load your model. <br>[7:29] Ben: Then you get your data, you encode it, you store your vectors, and you get your query, you encode it. And then here we use NumPy, you do a cosine similarity search, eg a dot product between normalized vectors to get the most similar documents. And the documents whose embedding are similar to your query embedding is what you would consider as your relevant documents. And thatâ€™s pretty much it. Thanks. modified from something that Jeremy did to showcase how simple RAG actually is in his Hackers Guide to LLMs. <br>[8:02] Ben: But thatâ€™s what you want to do to retrieve context in the simplest possible way. And you will have noticed that thereâ€™s no vector DB in this. This is all numpy arrays. And this is all numpy arrays because when you use vector DBs, the huge point of using a vector DB is to allow you to efficiently search through a lot of documents because what a vector DB does generally, not all of them, but most of them, wrap stuff like HNSW, IVFPQ, which are indexing types. <br>[8:31] Ben: That allows you to do is to find and retrieve relevant documents without having to compute cosine similarity against every single document. It tries to do an approximate search of an exact search. This is not something that you need if youâ€™re embedding 500 documents. Your CPU can do that in milliseconds. You donâ€™t actually need a vector DB if youâ€™re trying to go to the simplest possible stage. But if you wanted one, it would go right here on the graph, right after you embed your documents, you would put them in the vector DB. <br>[9:00] Ben: And the second thing I think to discuss about is like this tiny graph is why am I calling embeddings by encoders? Because that step that I call by encoder, you will have seen a lot of times, but you will always see it generally called embeddings or model. And by encoder is the term that the IR literature uses to refer to that. And itâ€™s simply because you encode things separately, like you do two encoding stages. So itâ€™s a by encoding. And thatâ€™s used to create single vector representations where you pre-compute all your documentary presentations. <br>[9:30] Ben: So when youâ€™re using by encoders, you encode your documents whenever you want. Like when youâ€™re creating your database, when youâ€™re adding documents, those get encoded at a time thatâ€™s completely separate from intrants. And then only at intrants will you, like in the second aspect of this column, will you embed your query to compare to your pre-computed documentary presentations. So thatâ€™s really computationally efficient because at inference youâ€™re only ever encoding one thing which is the query and everything else has been done before. And so that is part of why itâ€™s done that quickly. <br>[10:02] Ben: And I did want to take a slight break because I can see there are questions, but theyâ€™re not showing up on my screen. So if there are any on this quick MVP, then. <br>[10:11] Participant 3: Yeah, let me look through some of the questions. Iâ€™m going to give you a few of them and you can decide whether you want to take them now or later. So we got one. Itâ€™s a 7,000 query, a 7,000 question and answer data set. Can I optimize RAG to accurately retrieve and quote exact answers? Weâ€™ll also effectively hand in queries that are slightly different from the original data. I think thereâ€™s actually two parts to that. So one is to quote the exact answer, which is something about the information retrieval part. <br>[10:45] Participant 3: But itâ€™s rather just like, what do you tell the LLM to do? But the information retrieval part is probably well. <br>[10:56] Ben: I will actually cover how to better deal with out of context things in the upcoming slide. <br>[11:04] Participant 3: Why do you keep going? <br>[11:08] Ben: None of these questions can be saved right now. Perfect. The next one is if thatâ€™s very computationally efficient, there is an obvious trade-off here. And that is your documents are entirely unaware of your query and your queries are entirely unaware of your documents. <br>[11:24] Ben: which means that youâ€™re very very like subject to how it was trained is basically if your queries look a bit different from your training data or if like if thereâ€™s very very specific information that will be in certain documents and not other sometimes you want to know what how the query is phrased you want to know what the query is looking for when youâ€™re encoding your document so that it can like kind of paint that representation and represent it more towards information that youâ€™re interested in And thatâ€™s done with what we call re-ranking. <br>[11:53] Ben: So re-ranking is another one of those scary stages that weâ€™ll see in your pipeline. And the most common way to do re-ranking is using something that we call cross-encoder. And cross-encoder is another one of those scary words, like by encoder that you feel should be like a very advanced concept, but itâ€™s actually very simple. This graph here represents the whole difference between them. The bi-encoder is basically this two column system that we described where documents get encoded in their corner, queries get encoded in their own corner, and they only meet very, very late. <br>[12:20] Ben: Like you only do cosine similarity between vectors, but the documents never seen the query and vice versa. The cross-encoder is different. The cross-encoder is a model that will take your document and your query together. So youâ€™re going to give it both your document or like a series of documents, depending on the type of model, but to keep it simple, we do it one by one. So you always give it a query document pair. And you put it through this cross-encoder model, which is effectively a classifier with a single label. <br>[12:46] Ben: And the probability of the label being positive is what your model considers as how similar the documents are or how relevant it is. This is extremely powerful because it means that the model knows everything about what youâ€™re looking for when itâ€™s encoding the document. It can give you a very accurate score or at least a more accurate score. The problem is that you can see how that wouldnâ€™t scale because itâ€™s not very computationally realistic to compute this query document score for every single query document pair every time you want to retrieve a document. <br>[13:15] Ben: Say youâ€™ve got Wikipedia embedded, youâ€™ve got, I donâ€™t know, like 10 million paragraphs. Youâ€™re not going to compute 10 million scores. through a model for like using 300 million parameters for every single document for you. You would eventually return something and it would be a very, very relevant document, but it will also take 15 minutes, which is probably not what you want in production. <br>[13:37] Ben: So you probably also have heard, or you might also have heard if youâ€™re really into retrieval, or not heard at all if youâ€™re not into retrieval of other re-ranking approaches like RankGPT or RankLLM using LLMs to rank documents has been a big thing lately. For people really into retrieval, you will know of MonoT5, et cetera. So those are not cross-encoders, but thatâ€™s not really relevant to us because the core idea is the same, and thatâ€™s basically what we always do with re-ranking in the pipeline. <br>[14:04] Ben: you use a powerful model that is computationally expensive to score only a subset of your documents. And thatâ€™s why itâ€™s re-ranking and not ranking, because this can only work if you give it like, I donâ€™t know, 10, 50, not more than that document. So you always have a first stage retrieval, which here is our vector search. And then the re-ranker does the ranking for you, so it creates an ordered list. Thereâ€™s a lot of ways to try those models out. <br>[14:28] Ben: Some of them have an API base, so itâ€™s just an API called to cohere or Jena. Some of them you run your machine. If you want to try them out, and this is basically the self-promotion moment, I do maintain at answer.ai library just called rerankers with the QR code here, where itâ€™s basically a unified API so you can test any ranking method in your pipeline and swap them out freely. And thatâ€™s what your pipeline looks like now. <br>[14:52] Ben: Itâ€™s the same with just that one extra step at the end where you re-rank things before getting your results. So weâ€™ve added re-ranking, but thereâ€™s something else thatâ€™s missing here. And thatâ€™s something actually addresses the first question, at least partially, is that the semantic search via embeddings is powerful and Iâ€™m not saying donâ€™t choose vectors. Vectors are cool, like models are cool, deep learning is cool. But itâ€™s very, very hard if you think about it, because youâ€™re asking your model to take, I donâ€™t know, 512 tokens, even more if youâ€™re doing long context. <br>[15:24] Ben: And youâ€™re like, okay, put all of this into this one vector. We are just using a single vector. Youâ€™ve got like, I donâ€™t know, 384, 1024 at most floats, and that must represent all the information in this document. Thatâ€™s naturally lossy. Thereâ€™s no way youâ€™re going to keep all of the information here. And what you do when youâ€™re training on embedding is that youâ€™re teaching the embedding to represent information that is useful in their training. <br>[15:49] Ben: So the model doesnâ€™t learn to represent all of the documentâ€™s information because thatâ€™s pretty much impossible since embeddings are essentially a form of compression. What the model actually learned is to replant the information that is useful to the training queries. So your training data is very, very important here. Itâ€™s like replanting the documents in a way that will help you use the queries in the way that phrase in your training data to retrieve a given document. <br>[16:16] Ben: So when you use that on your own data, itâ€™s likely that youâ€™re going to be missing some information, or when you go slightly out of distribution. Thereâ€™s another thing which is humans love to use keywords, especially if youâ€™re going into the legal domain, the biomedical domain, anything specific. We have a lot of acronyms that might not even be in the training data, but we use a lot of acronyms. We use a lot of very advanced medical words. People love jargon. People love to use technical words because theyâ€™re very, very useful. <br>[16:44] Ben: And thatâ€™s why you should, and I know it sounds like Iâ€™m talking from the 70s, because thatâ€™s actually a method from the 70s, but you should always have keyword search in your pipeline. You should always also have full text search on top of like anything that you do with vectors. And keyword search, which you can call full text search or like tfidifbm25, itâ€™s powered by what we call tfidif. <br>[17:06] Ben: which is a very basic NLP concept that essentially stands for term frequency, inverse document frequency, and it assigns every single word in a document or a group of words because sometimes we do them two by two, or three by three even. It gives them a weight based on how rare they are. So a word that appears everywhere like V or A has a very, very small weight and a word thatâ€™s highly specific to certain documents has a very high weight. <br>[17:32] Ben: And the main method to use TF-IDF for retrieval is called BM25, which stands for Best Matching 25. It was invented in the 70s. Itâ€™s been updated since then, but itâ€™s basically just been iterations of it. And youâ€™ll often hear IR researchers say that the reason that the fieldâ€™s not taken off like NLP has or Computer Vision has is because the baseline is just too good. Weâ€™re still competing with BM25, although itâ€™s been 50 years now. Oh my god, itâ€™s been 50 years. <br>[18:00] Ben: Yeah, so the M25 existed for like, basically my entire lifetime before my birth, and itâ€™s still used in production pipeline today. Thatâ€™s how good it is. And the good thing is itâ€™s just word counting with a match with like a waiting formula. So the compute time is virtually unnoticeable. Like you can add that to your pipeline, you will absolutely never fail it. <br>[18:20] Ben: And I said I wouldnâ€™t add anything from papers, but I feel like because Iâ€™m making a very strong claim that this method from 70 is strong, I should add a table and add the table from the bare paper, which is the retrieval part of MTEB, which is basically the main embeddings benchmark. And they compared it to a lot of models that were very popular for retrieval, like DPR and very strong vector retrievers. <br>[18:45] Ben: And basically, you can see that unless you go into very over-trained embeddings like E5, BGE, BM25 is competitive with virtually all deep learning-based approaches, at least at the time of the paper, which was only just three years ago. We now have embeddings that are better, but we donâ€™t have any embeddings that are better to the point where theyâ€™re not made better by being used in conjunction with BM25. <br>[19:10] Ben: Knowing that this is how you want your pipeline to look, youâ€™ll notice that thereâ€™s now a whole new pathway for both the query and the documents, who are on top of being encoded by the embedder, theyâ€™re also encoded by TF-IDF to get full text search, and that will help you retrieve keywords, etc. Humans use keywords in queries all the time, itâ€™s something you should do. At the end, you will combine the scores. You can do that in a lot of ways. <br>[19:33] Ben: I wonâ€™t go into too much details, but what a lot of people do is give a weight of 0.7 to the cosine similarity score and 0.3 to the full text hash. But Iâ€™m pretty sure we could do a whole talk for an hour on different methods of combining that. Okay. I do have five more minutes. <br>[19:50] Ben: So the last one that you want to add to a simple pipeline, the thing that I think really completes your MVP plus plus is using metadata and using metadata filtering because academic benchmarks donâ€™t because in academic benchmarks documents exist mostly in a vacuum like they donâ€™t exist in the real world theyâ€™re not tied to a specific company etc when youâ€™re using rag in production itâ€™s very, very rare that someone comes to you and says, these documents came to me in a dream and caught them. Like they came from somewhere. Theyâ€™ve been generated by a department. <br>[20:21] Ben: Theyâ€™ve been generated for a reason. They might be old Excel sheets or whatever, but they have business sense or they have in context sense. And the metadata is actually sometimes a lot more informative than the document content, especially in RAG contexts. So if you take the query here, which is, can you get me the Cruise Division financial report for Q422? Thereâ€™s a lot of ways in which this can go wrong if youâ€™re just looking at it from the semantic or even using keywords aspect. <br>[20:52] Ben: When you say, when you see like this, the model must capture the financial report. So you, the model must figure out you want the financial report, but also cruise division Q4 and 2022 and embedding models are bad at numbers. So you might get a financial report, but maybe for another division, or maybe for the cruise division of 1998, itâ€™s very hard to just hope that your vector will capture all of this. <br>[21:15] Ben: But thereâ€™s another failure case, which will happen, especially with weaker LLMs, is that if you just have like top Ks and top five documents, and you retrieve the top five documents for your query. Even if your model is very good, if you just let it retrieve the top five documents, no matter what, you will end up with financial reports, at least five of them, and thereâ€™s most likely only one for Q4 22. <br>[21:37] Ben: So at that point, youâ€™re just passing all five to the model and being like, good luck, use the right one, which might confuse it, especially because tables can be hard, et cetera. And Iâ€™m not saying that your vector search will fail, but statistically it will. In most cases, it will fail. If it donâ€™t fail for this query, it will fail for a similar one. But thatâ€™s actually very, very easy to mitigate. You just have to think outside of the vector and just use more traditional methods. You can use entity detection models. <br>[22:07] Ben: One thatâ€™s very good for this is Gleaner, which is a very recent model that does basically zero-shot entity detection. You give it arbitrary entity types, so document type, time period, and the department. And this is like a live thing of Glenore. You can run the demo on the bottom, but here we just extract financial report, time period, and department. And when you generate your database for RAG, all you need to do is basically specify the time period. <br>[22:32] Ben: So when you get an Excel sheet, you will just pass the name for it or pass the date in it and give metadata 2024 Q2, and Q4, sorry, 2022 Q2, the Q4. Okay, mixed up there. Then you just need to ensure that this is stored alongside your document. At query time, you can always pre-filter your document set to only query things that make sense. You will only query documents for this relevant time period. <br>[22:56] Ben: You ensure that even if you give your model the wrong thing, it will at least be the right time frame so it can maybe try and make sense of it. And with this final component, this is what your pipeline looks like. You can see the new component here, which is metadata filtering, which doesnâ€™t apply to queries. Queries go right through it. The documents get filtered by that, and we wonâ€™t perform search on documents that will not meet the metadata that we want. <br>[23:20] Ben: And okay, I do agree that this looks a lot scarier than the friendly one at the start, which just had your embedder and then cosine similarity search and the results. It is actually not very scary. This is your full pipeline. This implements everything weâ€™ve just talked about. Itâ€™s about 25 lines of code if you remove the commands. It does look a bit more unfriendly because thereâ€™s a lot more moving parts, I think. Thereâ€™s a lot more steps, but if you want, we can just break it down a bit further. We use LensDB for this. <br>[23:50] Ben: This is not necessarily an endorsement of LensDB as a vector DB, although I do like LensDB because it makes all of these components, which are very important, very easy to use. But I try not to take side in the vector DB wars because Iâ€™ve used WeaveYard, Iâ€™ve used Chroma, Iâ€™ve used LensDB, Iâ€™ve used Pencode, they all have their place. But I think LensDB, if youâ€™re trying to build an MVP, is the one you should always use for MVPs right now because it has those components built in. <br>[24:14] Ben: And here you can see just how easy it actually is. So we still load the By Encoder, just in a slightly different way, same as earlier. We define our document metadata. Here is just a string category, but it could be a timestamp, it could be just about anything. Then we encode a lot of documents just like we did previously. Here weâ€™ve created, so itâ€™s not an index, this is still a hard search, this is not an approximate search. Then we create a full text search index, which is generating those TF-IDF. <br>[24:40] Ben: Why I mentioned before, we give a way to every single term in the documents. Then we load the reranker. Here weâ€™re using the query ranker because itâ€™s simple to use an API. And at the very end, youâ€™ve just got your query and your search where we restrict it to the category equals films. So we will only ever search into the document thatâ€™s about a film, not about an author, not about a director. We get the top 10 results and we just have a quick ranking step. And thatâ€™s pretty much it. <br>[25:06] Ben: Weâ€™ve taken the pipeline at the start, which only had the biancoder component to a pipeline that now has the biancoder component, metadata filtering, full text search, and a reranker at the end. So weâ€™ve added like basically the four most important components of RetriVault into a single pipeline. And it really donâ€™t take much more space in your code. And Yeah, that is pretty much the end of this talk. So thereâ€™s a lot more to cover in RAC. This is definitely not the full cover of RAC, but this is the most important thing. <br>[25:36] Ben: This is what you need to know about how to make a good pipeline very quickly. All the other improvements are very, very valuable, but they have a decreasing cost effort ratio. This takes virtually no effort to put in place. Definitely worth learning about sparse methods, multi-vector methods, because they are very adapted to a lot of situations. Colbert, for instance, is very strong out of domain. Sparse is very strong in domain. <br>[25:58] Ben: You should watch Jasonâ€™s talk about rack systems and Joeâ€™s upcoming talk about retrieval evaluations because those are by a clear trifecta of the most important things. And yeah, any questions now? <br>[26:12] Participant 3: Hamel and I were just messaging sayingâ€¦ We love this talk. Everything is presented so clearly. Weâ€™ve also got quite a few questions. <br>[26:30] Hamel: My favorite talk so far. Not big favorites, but yeah. <br>[26:36] Ben: Thank you. <br>[26:37] Participant 3: Go ahead. <br>[26:41] Hamel: Okay, questions. Did you have one that you were looking at already, Dan? I can tell it. <br>[26:47] Participant 3: Yeah, weâ€™ve got one that I quite like. Can the way that you fine-tune your bi-encoder model affect how you should approach fine-tuning for your cross-encoder and vice versa? <br>[26:58] Ben: Yes. I donâ€™t think I can give a really comprehensive answer because it will really depend on your domain, but you generally want them to be complementary. So if youâ€™re in a situation where youâ€™ve got the compute and the data to fine-tune both, you always want toâ€¦ by encoder to be a bit more loose. Like you want it to retrieve potential candidates and then you want to trust your reranker, like your cross-encoder to actually do the filtering. <br>[27:21] Ben: So if youâ€™re going to use both and have full control over both, you might want to fine tune it in a way that will basically make sure that your top K candidates can be a bit more representative and trust the reranker. <br>[27:35] Participant 3: Let me ask, this wasnâ€™t an audience question, but a related question. You showed us where the, when you choose questions to feed into the re-ranker, thatâ€™s sort of a weighted average of what you get from the TF-IDF or BM-25 with what you get from the just simple vector search. What do you think of as the advantage or disadvantage of that over saying weâ€™re going to take the top X from one cat from one of the rankers and the top X from the others? <br>[28:14] Participant 3: And that way, if you think one of these is, for some questions, especially bad, you have a way of short-circuiting its influence on what gets sent to the re-ranker. <br>[28:27] Ben: Yeah, I think that also makes complete sense. And thatâ€™s another, thatâ€™s a cop-out answer I use a lot, but that also depends a lot on your data. Like a lot of the time you want to look at whatâ€™s your actual context and how itâ€™s actually being used. Because in some situations that actually works better, like especially if you work with biomedical data, because thereâ€™s so much like specific documents, itâ€™s quite often the embedding wonâ€™t be that amazing on some questions. <br>[28:52] Ben: So you just want to take the top five from both and get the re-ranker to do it, because the re-ranker is quite aware. So itâ€™s a perfectly valid approach to combine them that way. <br>[29:04] Participant 3: You want to pick a question, Hamel? <br>[29:10] Hamel: Yeah, Iâ€™ve been looking through them. You guys have beenâ€¦ Okay, Jeremyâ€™s asking, can we get a link to the code example? Yeah, sure. Your slides in Maven. We can also, can I share your slides in Discord as well, Ben? <br>[29:25] Ben: Yes, please. <br>[29:26] Hamel: Yeah. Iâ€™ll go ahead and share the slides in <br>[29:28] Ben: Discord. And Iâ€™ll share the GitHub gist for the code examples I thought of. <br>[29:34] Participant 3: And Iâ€™ll embed the link to the slides in Maven for people who want to talk some point deep into the future and might lose track of it in Discord. Thereâ€™s a question somewhere in here Iâ€™ll find in a moment, but we got this question for Jason, the speed and then the speaker just before you, Paige Bailey said. RAG, you know, in the world of million token context lengths is not going to be as important. Whatâ€™s your take on the relative importance of RAG in the future? <br>[30:20] Ben: So Iâ€™m still very hopeful about RAG in the future. And I think I see it as some sort of like, so your LLM to me is like your CPU and your context window will be your RAM. And so like, even if youâ€™ve got 32 gigs of RAM, nobodyâ€™s ever said, yeah, throw away your hard drive. You donâ€™t need that. Like in a lot of contexts, you will still want to have like some sort of storage where you can retrieve the relevant documents. <br>[30:42] Ben: Having to use a long context window is never going to be a silver bullet. Just like RAG is never a silver bullet. But Iâ€™m actually really happy because it just means I can retrieve much longer documents and get more efficient rack systems. Because to me, itâ€™s a bit of a trade off where if youâ€™ve got a longer context, it just means youâ€™ve got a lot more freedom with how quick your retrieval system can be. Because if you need to use top 10 or top 15, thatâ€™s fine. You can fit them in. <br>[31:06] Ben: Whereas when you can only fit the top three documents, you need your retrieval system to be really good, which might mean really slow. Yeah. <br>[31:12] Participant 3: So, yeah. <br>[31:13] Ben: So, yeah. <br>[31:26] Participant 3: We had a question from Wade Gilliam. What are your thoughts on different chunking strategies? <br>[31:36] Ben: I probably donâ€™t think about chunking as much as I should. I am very hopeful for future avenues using LLMs to pre-chunk. I donâ€™t think those work very well right now, but in my test Iâ€™ve never been impressed. Also, I do tend to use Colbert more often than Bancoders, and Colbert is a lot more resistant to chunking, so itâ€™s something that I donâ€™t care about as much. But generally I would try toâ€¦ <br>[32:01] Ben: So my go-to is always to chunk based on like around 300 tokens per chunk, and try to do it in a way where you never cut off a sentence in the middle, and always keep like the last 50 tokens and the next 50 tokens of the previous and next chunk. Because information overlap is very useful to give content, like please donâ€™t be afraid to duplicate information in your chunks. <br>[32:22] Hamel: I have a question about the buy encoder. Do you ever try to fine tune that using some kind of like label data to get that to be really good? Or do you usually kind of use that off the shelf and then use a re-ranker? And how do you usually go about it or how do you make the trade off? <br>[32:43] Ben: So again, context dependent, but if you have data, you should always fine-tune all your encoders, be it the bi-encoder, the cross-encoder. I think Colbert, because itâ€™s single vector, you can get away with not fine-tuning for a bit longer because itâ€™s multi-vector, so you can get away with not fine-tuning for a bit longer. But if you have data, itâ€™s all about like basically the resources you have. So in this talk, weâ€™re doing an MVP, this is something you can put together in an afternoon. If your company says you have $500. <br>[33:10] Ben: Spend 480 of that on OpenAI to generate synthetic questions and find your encoders that will always get you better results. Like always find your encoders if you can. And so, yes, so a couple of questions about fitting Colbert in and Iâ€™m using this entire executive decision to answer those. So Colbert in this pipeline, some people use it as a re-ranker, but then thatâ€™s not optimal. Thatâ€™s very much when you donâ€™t want to have to change your existing pipeline. <br>[33:50] Ben: If you were to design a pipeline from scratch and wanted to use Colbert, you would have it instead of the BI encoder and it would perform basically the same role as the BI encoder, which is first-edge retrieval. And if you wanted to use Colbert, and especially if you donâ€™t have the budget to fine-tune and need a re-ranking step, sometimes it can actually be better to use Colbert as a re-ranker still. Because the multi-vector approach can be better at capturing keywords, etc. But thatâ€™s very context-dependent. So ideally, you would have it as ShowByEncoder. <br>[34:22] Participant 3: For a lot of people here who probably arenâ€™t familiar with Colbert, Colbert, can you give theâ€¦ Quick summary of it? <br>[34:32] Ben: Yeah, sorry, I got carried away because I saw the question. So Colbert is an approach which is effectively a biancoder, but instead of cramming everything into a single vector, you represent each document as a bag of embeddings. So like, if youâ€™ve got 100 tokens, instead of having one big 124 vector, you will have a lot of small 128 vectors, one for each token. And then you will score that at the end. You will do the same for the query. So if your query is 32 tokens, you will have 32 query token. <br>[35:02] Ben: And for each query token, you will compare it to every token in the document and keep the highest score. And then you will sum up those highest scores and that will be the score for that given document. Thatâ€™s called max similarity. And the reason thatâ€™s so powerful is not because it does very well on data itâ€™s been trained on. You can beat it with a normal Bayer encoder, but it does very well at extrapolating to out of domain because you just give the model so much more room to replant each token in its context. <br>[35:29] Ben: So itâ€™s much easier if youâ€™re in a non-familiar setting, youâ€™ve not compressed as much information. And I do have self promotion. I do have a pretty cool Colbert thing coming out later this week to compress the Colbert space by reducing the tokens that actually needs to save by about 50 to 60% without losing any performance. So thatâ€™s a bit of a teaser, but look forward to the blog post if youâ€™re interested. <br>[35:57] Participant 3: And to find the blog post, you suggest people follow you on Twitter or? <br>[36:02] Ben: Yeah, definitely follow me on Twitter. Because it was pretty much the only place where you can reliably reach me. <br>[36:14] Hamel: Someoneâ€™s asking what are some good tools to fine tune embeddings for retrieval? Would you recommend Ragatouille or anything else? Like whatâ€™s yourâ€¦ <br>[36:24] Ben: Iâ€™d recommend sentence transformers, especially with the 3.0 release recently. Itâ€™s now much, much funnier to use. Itâ€™s basically, thereâ€™s no need to reinvent the wheel. Theyâ€™ve got all the basics implemented very well there, so sentence transformers. <br>[36:44] Participant 3: Question from Divya. Can you give any pointers on how one fine-tunes their embedding model? <br>[36:53] Ben: Sorry, can you repeat that? I could have said it. <br>[36:55] Participant 3: Yeah. The question is, can you give any pointers or describe the flow for when you fine tune your embedding model? <br>[37:04] Ben: Okay. So thatâ€™s probably a bit more involved than this talk, but essentially when you fine tune your embedding model, what youâ€™ll want is queries. You need to have queries and you need your documents and youâ€™re going to tell the model. For this given query, this document is relevant. And for this given query, this document is not relevant because sometimes thereâ€™s a triplet loss. And a triplet loss is what you will do when you have one positive document and one negative document. And youâ€™ll kind of be teaching the model, this is useful, this is not useful. <br>[37:32] Ben: And Iâ€™m not going to go down too much because this rabbit hole can take you quite far. But sometimes when you have triplets, you also want to use what we call hard negatives. which is you want to actually use retrieval to generate your negative examples because you want them to be quite close to what the positive example is, but not quite the right thing. Because thatâ€™s why you teach the model more, but was actually useful to answer your query. <br>[37:57] Ben: So the workflow is probably, as always, look at your data, figure out what kind of queries your user will actually be doing. If you donâ€™t have user queries. Go into production, write some, write some queries yourself and give that to an LLM, generate more queries and you can have a pretty solid ritual pipeline like that. <br>[38:16] Hamel: Someoneâ€™s asking in the Discord, and I get this question all the time, is please share your thoughts on graph rag. <br>[38:25] Ben: I have never actually done graph rag. I see this mentioned all the time, but itâ€™s not something that has come up for me at all. So I donâ€™t have strong thoughts about. I think itâ€™s cool, but thatâ€™s pretty much the full extent of my knowledge. <br>[38:49] Hamel: Someoneâ€™s asking, okay, when you have long context windows, does that allow you to do something different with RAG, like retrieve longer documents or do any other different kinds of strategies than you were able to before? Does it change anything? How you go about this? <br>[39:09] Ben: Yeah, I think itâ€™s a bit what I mentioned before. To me it changes two main things. One is I can use longer documents, which means I can use longer models, or I can stitch chunks together. Because sometimes if your retrieval model isnâ€™t very good at retrieving long documents, which is often the case, you might just want, if I get a chunk from this document, give the model the full document. Like if I just get a chunk from it past the full context and you just hope the model is able to read it. <br>[39:34] Ben: And if youâ€™ve got a good long context model, it can. So it changes how you decide to feed the information into the model. And then the other aspect is, like I said, it changes the retrieval overhead because if you need to be very good, like I was saying, if you need only the top three documents to be relevant, youâ€™re going to spend a lot of time and money on retrieval pipeline. If youâ€™re like, oh, as long as my recall at 10 or my recall at 15 is good, thatâ€™s fine. <br>[39:56] Ben: You can afford to have much lighter models and spend a lot less time and resources on retrieval. Thereâ€™s a lot of diminishing returns in retrieval when getting a good recall at 10. So recall at 10 is how likely you are to retrieve the relevant document in the first 10 results. is generally very easy. Recall at 100 is very, very easy. And then recall at 5 is getting harder. And recall at 3 and recall at 1 are like the really tough ones because a lot of the training data is noisy. <br>[40:23] Ben: So it can even be hard to know what a good recall at 1 is. So longer context makes that irrelevant. And thatâ€™s why itâ€™s great for RUG. <br>[40:49] Hamel: Someoneâ€™s asking, and I donâ€™t even know what this means, whatâ€™s your view on PIDE versus React versus StepBack? <br>[40:58] Ben: Iâ€™ve only used React out of those. And so those are like adjunct systems of function coding. Itâ€™s like to give your LLM the ability to call tools, at least React is. I donâ€™t have strong thoughts on those in the context of retrieval, so I canâ€™t really answer the question. Yeah, I think. I would occasionally use React from the model to be able to trigger a search itself, but I think thatâ€™s still an open area of research. And I think Griffin from AnswerIA is also in the chat and heâ€™s very interested in that. <br>[41:31] Ben: Itâ€™s basically how do you get a model to tell you that it doesnâ€™t know? Because sometimes you donâ€™t need retrieval, the model already knows. Sometimes you do need retrieval, but thatâ€™s still a very open question. Like how do you decide when to search? So no strong thoughts there yet. <br>[41:50] Participant 3: You may or may not have a good answer for this one. Is there an end-to-end project, open-source project, that someone could look at as a way to see or evaluate the difference in result quality when they do result from just buying code or MVP and compare that to the final compact MVP++ that you showed? <br>[42:14] Ben: No, actually, thatâ€™s a very good point. I donâ€™t think there is one that systematically goes through every step. And thatâ€™s probably something that I would like to build at some point or find one because just like most things in retrieval, everything is kind of conventional wisdom. Like youâ€™ve seen it piece and pieces in a lot of projects and you just know that thatâ€™s how it is. But unless you dig deep into the papers or like do it yourself, itâ€™s quite rare to find very good resources showing that. <br>[42:54] Participant 3: A related question, do you have a tutorial that you typically point people to on fine-tuning their encoder? <br>[43:08] Ben: That would be the sentence transformers documentation, but itâ€™s not the friendliest tutorial, so thatâ€™s a half answer. Thatâ€™s what I would point you to, but itâ€™s still a bit hard to get into, sadly. <br>[43:40] Hamel: Wade is asking if you have go-to embedding models. <br>[43:48] Ben: I think my go-to these days when Iâ€™m demoing something is the Korea one, because itâ€™s nice to be able to work with an API. It works really well. Itâ€™s cheap. But other than that, I would just call bear if Iâ€™m using something in my own pipeline. I would use multi-vectors. But it really depends on the use case, because you would often find that some things work well for you and some things donâ€™t. I do have strong opinions on not usingâ€¦ <br>[44:16] Ben: So if you go to the MTB leaderboard, which is the embedding leaderboard right now, youâ€™ll see a lot of LLMs as encoders. And I would advise against that because the latency is not worth it. Donâ€™t need 7 billion parameters to encode stuff. And at least some of the early ones actually generalized worse, like Neil Schremer from Cohere had a really interesting table where the E5 mistral was worth an E5 large, despite being seven times as big. <br>[44:47] Ben: So probably just stick to the small ones between 100 and at most a billion parameters, but that would be my only advice about that. Try all the good ones like GT, BG, E5. <br>[45:00] Hamel: Chris Levy is askingâ€¦ this question about Elasticsearch, which I also get quite a lot. So he asks, Anyone here have experience building RAG application with just keyword BM25 as a retriever at work? It makes use of Elasticsearch. And he said itâ€™s all over the tech stack that people are already using Elasticsearch. Is there basically heâ€™s asking, is there a way to keep using Elasticsearch with RAG that you know about or that you have encountered? Or do you mainly use like vector database like LanceDB and things like that? <br>[45:32] Hamel: Have you tried seeing people using Elasticsearch and trying to bootstrap off of that? <br>[45:37] Ben: Yeah, Iâ€™ve used Elasticsearch a bit and itâ€™s perfectly possible. You do lose obviously the semantic search aspect, although I think now Elasticsearch has a vector DB offering, so you could add vectors to it. You could always plug in, you could always just do BM25 and then plug in a re-ranker at the end. Thatâ€™s often, if you read papers on like cross encoders, generally the way they evaluate them is actually doing just that, like do BM25 to retrieve 50 to 100 documents and then rank them using the re-ranker. <br>[46:07] Ben: which if you can afford to just set up your re-ranking pipeline or call the Core API is a really good way to go about it because you donâ€™t need to embed your whole documents to sample how good it would be with deep learning because there are domains where you do not need deep learning, BM25 is still good enough in some bits and you know like I think itâ€™s become very apparent like BM25 has never told anyone they should eat three rocks a day whereas embeddings have so <br>[46:35] Hamel: Dimitri is asking, is it worthwhile to weigh the BM25 similarity score during the re-ranking step as well? <br>[46:45] Ben: Probably not. You generally just want to use BM25 to retrieve candidates, but you donâ€™t need to give those scores to your cross-encoder. <br>[46:59] Participant 3: Thereâ€™s a question. Iâ€™m going to change it slightly. Someone asks about retrieving from many documents rather than finding the best one. Maybe the tweak there is if you have a theory that information within any single document is so correlated that you actually want to try and get some diversity, are you familiar with or have you used approaches where you I specifically try in some loss function somewhere, encourage that diversity and encourage pulling from many documents rather than from one. <br>[47:37] Ben: I have not done that myself. I know that thereâ€™s different loss methods to optimize for diversity versus clear accuracy. But I donâ€™t think I would be able to give you a clear answer without sounding really confident about something I donâ€™t know much about. <br>[47:59] Participant 3: Have you used hierarchical reg? Any thoughts on it? <br>[48:03] Ben: I have not, and I donâ€™t think itâ€™s very needed for the current pipelines. I think thereâ€™s a lot of other steps you can improve on. <br>[48:18] Participant 3: Since I think we have several Answer AI people here, I donâ€™t know if this is a question or a request, Iâ€™m eager to learn if Answer AI will come up with any books on LLM applications in the future. <br>[48:33] Ben: I donâ€™t think so, but never say never. Jeremy, if you want to chime in. Because I canâ€™t make any promises because my boss is watching. <br>[49:00] Participant 3: You see anything else to Ben, did you say that you canâ€™t see the questions? <br>[49:05] Ben: Yeah, theyâ€™re all blank for me. I saw one earlier, but they really show up sporadically. <br>[49:10] Participant 3: Yeah. Not sure whatâ€™s happened with her. And I think people also cannot upvote these. So a couple of quirks today. You see any others here, Emil, that you think we should pull in? <br>[49:25] Hamel: um no not necessarily i think like probably going to the discord is pretty good now yep tons of activity there as well um I mean, thereâ€™s infinite number of questions, so we can keep going. Okay, Lorien is asking, whatâ€™s the best strategy when chunks, when the documents, when chunks of documents donâ€™t fit into the context window? Do you do RAG in a MapReduce style, summarize aggressively? What are the techniques that youâ€™ve seen work most effectively? <br>[50:25] Ben: So thatâ€™s, I think, a very broad question, because itâ€™s like, why do they not fit? Is it because like every document is really long? Is it because you need a lot of different documents, etc, etc? So. And also another important aspect is whatâ€™s the latency tolerance? Because quite a lot of the time you can make RAG infinitely better, but users wonâ€™t stay waiting like 20 seconds for an answer. So you need to figure out like, how much time do I have? <br>[50:52] Ben: One way that you can often see what Iâ€™ve done in production actually is retrieve the full documents but have another database that maps every document to its summary. So you will have done your LLM summarization at the previous step. You will retrieve the relevant chucks, and then you will pass the relevant summaries to the context window. But that kind of depends on your actual setting. I have another call at 10, which is in five minutes for me. So if youâ€™ve got another final question. <br>[51:35] Hamel: I really enjoyed this presentation. <br>[51:39] Ben: Thank you. <br>[51:42] Participant 3: Yeah, this was really great. Everything is super clear and well presented, so thanks so much. <br>[51:54] Ben: Thank you. Cheers. <br>[51:59] Participant 3: Thanks, everyone.</p>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/parlance-labs\.com\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../education/rag/jo.html" class="pagination-link" aria-label="Back to Basics for RAG">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Back to Basics for RAG</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../education/rag/jason.html" class="pagination-link" aria-label="Systematically improving RAG applications">
        <span class="nav-page-text">Systematically improving RAG applications</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/HamelHusain">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hamelsmu">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/parlance-labs/website/edit/main/education/rag/ben.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>