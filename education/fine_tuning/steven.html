<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-07-20">

<title>Fine Tuning OpenAI Models - Best Practices – Parlance</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../education/fine_tuning_course/workshop_4.html" rel="next">
<link href="../../education/fine_tuning/abhishek.html" rel="prev">
<link href="../../b.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-2bb31de2310bb46caae739c59a8eb5d6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QXGQ6F7NKT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QXGQ6F7NKT', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-N36MQM5R');</script>
<!-- End Google Tag Manager -->


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Fine Tuning OpenAI Models - Best Practices – Parlance">
<meta property="og:description" content="How to fine-tune OpenAI models like a pro.">
<meta property="og:image" content="https://parlance-labs.com/education/fine_tuning/steven.png">
<meta property="og:site_name" content="Parlance">
<meta property="og:image:height" content="360">
<meta property="og:image:width" content="640">
<meta name="twitter:title" content="Fine Tuning OpenAI Models - Best Practices – Parlance">
<meta name="twitter:description" content="How to fine-tune OpenAI models like a pro.">
<meta name="twitter:image" content="https://parlance-labs.com/education/fine_tuning/steven.png">
<meta name="twitter:creator" content="@HamelHusain">
<meta name="twitter:site" content="@HamelHusain">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image-height" content="360">
<meta name="twitter:image-width" content="640">
</head>

<body class="nav-sidebar docked nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Parlance</span>
    </a>
  </div>
          <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../services.html"> 
<span class="menu-text">Services</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://hamel.dev"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../team.html"> 
<span class="menu-text">Team</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../education/"> 
<span class="menu-text">Education</span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
            <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../education/index.html#fine-tuning">Fine-Tuning</a></li><li class="breadcrumb-item"><a href="../../education/index.html#how-to-fine-tune">How to fine-tune</a></li><li class="breadcrumb-item"><a href="../../education/fine_tuning/steven.html">Fine Tuning OpenAI Models - Best Practices</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Educational Resources</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/evals/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Evals</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/evals/allaire.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inspect, An OSS framework for LLM evals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/evals/ankur.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLM Eval For Text2SQL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/evals/schoelkopf.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A Deep Dive on LLM Evaluation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/rag/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">RAG</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/rag/jo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Back to Basics for RAG</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/rag/ben.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Beyond the Basics of RAG</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/rag/jason.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Systematically improving RAG applications</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/index.html#fine-tuning" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fine-Tuning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/index.html#should-you-fine-tune" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Should you fine-tune?</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning_course/workshop_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">When and Why to Fine Tune an LLM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/kyle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fine-tuning when you’ve already deployed LLMs in prod</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/emmanuel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Why Fine Tuning is Dead</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/index.html#how-to-fine-tune" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to fine-tune</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/daniel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Creating, curating, and cleaning data for LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/mistral_ft_sophia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Best Practices For Fine Tuning Mistral</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/abhishek.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Train (almost) any LLM using 🤗 autotrain</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/steven.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Fine Tuning OpenAI Models - Best Practices</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning_course/workshop_4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deploying Fine-Tuned Models</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/index.html#advanced-topics-in-fine-tuning" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced topics in fine-tuning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/napkin_math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Napkin Math For Fine Tuning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/slaying_ooms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Slaying OOMs with PyTorch FSDP and torchao</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/pawel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fine Tuning LLMs for Function Calling</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/applications/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Applications</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
 <span class="menu-text">education/applications/**/*.qmd</span>
  </li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/prompt_eng/berryman.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapters" id="toc-chapters" class="nav-link active" data-scroll-target="#chapters">Chapters</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  <li><a href="#notes" id="toc-notes" class="nav-link" data-scroll-target="#notes">Notes</a>
  <ul class="collapse">
  <li><a href="#dataset-for-fine-tuning" id="toc-dataset-for-fine-tuning" class="nav-link" data-scroll-target="#dataset-for-fine-tuning">Dataset For Fine-tuning</a></li>
  <li><a href="#best-practices" id="toc-best-practices" class="nav-link" data-scroll-target="#best-practices">Best Practices</a></li>
  </ul></li>
  <li><a href="#full-transcript" id="toc-full-transcript" class="nav-link" data-scroll-target="#full-transcript">Full Transcript</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/parlance-labs/website/edit/main/education/fine_tuning/steven.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    <div class="quarto-margin-footer"><div class="margin-footer-item">
<button onclick="window.location.href='https://hamel.ck.page/7d15a4b6e7'" style="background-color: #C75C56; color: white; padding: 12px 24px; border: none; border-radius: 6px; font-size: 12px; cursor: pointer; transition: background-color 0.3s ease;">
Subscribe To Our Newsletter
</button>
</div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N36MQM5R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../education/index.html#fine-tuning">Fine-Tuning</a></li><li class="breadcrumb-item"><a href="../../education/index.html#how-to-fine-tune">How to fine-tune</a></li><li class="breadcrumb-item"><a href="../../education/fine_tuning/steven.html">Fine Tuning OpenAI Models - Best Practices</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Fine Tuning OpenAI Models - Best Practices</h1>
  <div class="quarto-categories">
    <div class="quarto-category">fine-tuning</div>
    <div class="quarto-category">llm-conf-2024</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 20, 2024</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>How to fine-tune OpenAI models like a pro.</p>
  </div>
</div>


</header>


<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Q0GSZD0Na1s" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="mobile-only callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Subscribe For More Educational Content
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you enjoyed this content, subscribe to receive updates on new educational content for LLMs.</p>
<center>
<script async="" data-uid="6379a28bdb" src="https://hamel.ck.page/6379a28bdb/index.js"></script>
</center>
</div>
</div>
<section id="chapters" class="level2">
<h2 class="anchored" data-anchor-id="chapters">Chapters</h2>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=0s">00:00</a> What is Fine-Tuning</strong><br>
Fine-tuning a model involves training it on specific input/output examples to enable it to respond appropriately to similar inputs in the future. This section includes an analysis of when and when not to fine-tune.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=170s">02:50</a> Custom Models</strong><br>
While the API is the main offering, custom models are also available. These are tailored and crafted around user data and their specific use cases.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=371s">06:11</a> Optimizing LLMs for Accuracy</strong><br>
Steven discusses prompt engineering, retrieval-augmented generation (RAG), fine-tuning, and how these techniques can be used at different stages and for various use cases to improve model accuracy.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=680s">11:20</a> Fine-Tuning Failure Case</strong><br>
A case study on when fine-tuning failed.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=788s">13:08</a> Preparing the Dataset</strong><br>
This section shows the training data format along with some general guidelines on the type of data to be used for fine-tuning.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=868s">14:28</a> Using the Weight Parameter</strong><br>
The weight parameter allows you to control which assistant messages to prioritize during training.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=1176s">19:36</a> Best Practices</strong><br>
Best practices for fine-tuning involve carefully curating your training examples, iterating on the available hyperparameters, establishing a baseline, and more.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=1253s">20:53</a> Hyperparameters</strong><br>
Steven discusses the various hyperparameters available for fine-tuning, including epochs, batch size, and learning rate multiplier.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=1446s">24:06</a> Fine-Tuning Example</strong><br>
A real-world example illustrates how fine-tuning a model can boost its performance, showing how a smaller fine-tuned model can outperform a much larger non-fine-tuned model.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=1799s">29:49</a> Fine-Tuning OpenAI Models vs.&nbsp;Open Source Models</strong><br>
OpenAI models are state-of-the-art with support for features like tool calling and function calling, eliminating the hassle of deploying models.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=1910s">31:50</a> More Examples</strong><br>
Steven discusses additional examples covering fine-tuning models for function calling and question answering.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=2211s">36:51</a> Evaluations</strong><br>
Evaluating language model outputs can involve simple automated checks for specific formats or more complex evaluations by other models or graders for aspects like style, tone, and content inclusion.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=2326s">38:46</a> OpenAI on Fine-Tuning Models on Custom Data</strong><br>
Customers control their data lifecycle; OpenAI does not train on customer data used for fine-tuning.</p>
<p><strong><a href="https://www.youtube.com/watch?v=Q0GSZD0Na1s&amp;t=2617s">43:37</a> General Discussion</strong><br>
A general discussion on agents, the assistance API, and other related topics.</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p>Links to resources mentioned in the talk:</p>
<ul>
<li><a href="https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb">Fine-Tuning OpenAI Models with Weights &amp; Biases</a>: Explore how to fine-tune OpenAI models using Weights &amp; Biases.</li>
<li><a href="https://cookbook.openai.com/examples/chat_finetuning_data_prep">Data Prep and Analysis for Fine-Tuning</a>: Guidelines for preparing and analyzing data for fine-tuning.</li>
<li><a href="https://cookbook.openai.com/examples/fine_tuning_for_function_calling">Fine-Tuning for Function Calling</a>: Learn how to fine-tune models specifically for function calling.</li>
<li><a href="https://cookbook.openai.com/examples/how_to_finetune_chat_models">How to Fine-Tune Chat Models</a>: A comprehensive guide on fine-tuning chat models.</li>
<li><a href="https://platform.openai.com/docs/guides/fine-tuning">Fine-Tuning Documentation</a>: Detailed documentation and guidance on when to use fine-tuning.</li>
<li><a href="https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant">Fine-Tuning for Retrieval-Augmented Generation (RAG)</a>: Techniques for fine-tuning models for RAG.</li>
</ul>
</section>
<section id="notes" class="level2">
<h2 class="anchored" data-anchor-id="notes">Notes</h2>
<ul>
<li><strong>When to fine-tune</strong>:
<ul>
<li><strong>Good for</strong>:
<ul>
<li>Following a given format or tone for the output</li>
<li>Processing the input following specific, complex instructions</li>
<li>Improving latency</li>
<li>Reducing token usage</li>
</ul></li>
<li><strong>Not good for</strong>:
<ul>
<li>Teaching the model new knowledge (Use RAG or custom models instead)</li>
<li>Performing well at multiple, unrelated tasks (Do prompt-engineering or create multiple FT models instead)</li>
<li>Including up-to-date content in responses (Use RAG instead)</li>
</ul></li>
</ul></li>
</ul>
<section id="dataset-for-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="dataset-for-fine-tuning">Dataset For Fine-tuning</h3>
<p>Some guidelines when fine-tuning the data: - Have 50 - 100 examples. There should be at least 10 examples - Ensure that each fine-tuned model is for one task only - Keep system and user prompts similar between training and production</p>
<p>The dataset to be used for finetuning should have the following format:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"messages"</span><span class="fu">:</span><span class="ot">[</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"role"</span><span class="fu">:</span> <span class="st">"system"</span><span class="fu">,</span> </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"content"</span><span class="fu">:</span> <span class="st">"Marv is a factual chatbot that is also sarcastic."</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span><span class="ot">,</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"role"</span><span class="fu">:</span> <span class="st">"user"</span><span class="fu">,</span> </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"content"</span><span class="fu">:</span> <span class="st">"What's the capital of France?"</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span><span class="ot">,</span> </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"role"</span><span class="fu">:</span> <span class="st">"assistant"</span><span class="fu">,</span> </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"content"</span><span class="fu">:</span> <span class="st">"Paris, as if everyone doesn't know that already."</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="ot">]</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="best-practices" class="level3">
<h3 class="anchored" data-anchor-id="best-practices">Best Practices</h3>
<ul>
<li><strong>Curate examples carefully:</strong>
<ul>
<li>Datasets can be difficult to build, start small and invest intentionally.</li>
<li>Optimize for fewer high-quality training examples.</li>
<li>Consider “prompt baking”, or using a basic prompt to generate your initial examples.</li>
<li>If your conversations are multi-turn, ensure your examples are representative.</li>
<li>Collect examples to target issues detected in evaluation.</li>
<li>Consider the balance &amp; diversity of data.</li>
<li>Make sure your examples contain all the information needed in the response.</li>
</ul></li>
<li><strong>Iterate on hyperparameters:</strong>
<ul>
<li>Start with the defaults and adjust based on performance.</li>
<li>If the model does not appear to converge, increase the learning rate multiplier.</li>
<li>If the model does not follow the training data as much as expected, increase the number of epochs.</li>
<li>If the model becomes less diverse than expected, decrease the number of epochs by 1-2.</li>
</ul></li>
<li><strong>Establish a baseline:</strong>
<ul>
<li>Often users start with a zero-shot or few-shot prompt to build a baseline evaluation before graduating to fine-tuning.</li>
</ul></li>
<li><strong>Automate your feedback pipeline:</strong>
<ul>
<li>Introduce automated evaluations to highlight potential problem cases to clean up and use as training data.</li>
<li>Consider the G-Eval approach of using GPT-4 to perform automated testing using a scorecard.</li>
</ul></li>
<li><strong>Optimize for latency and token efficiency:</strong>
<ul>
<li>When using GPT-4, once you have a baseline evaluation and training examples, consider fine-tuning 3.5 to get similar performance for less cost and latency.</li>
<li>Experiment with reducing or removing system instructions with subsequent fine-tuned model versions.</li>
</ul></li>
</ul>
<p><br></p>
</section>
</section>
<section id="full-transcript" class="level2">
<h2 class="anchored" data-anchor-id="full-transcript">Full Transcript</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expand to see transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><br>[0:04] Steven Heidel: So briefly, what is fine-tuning? Fine-tuning is training a model to follow a set of given input and output examples. So basically when faced with some sort of input, if you don’t like the way that the public model is responding by default, you can teach it to output responses in a particular format. or whatnot when it sees that input in the future. So when do you want to use fine-tuning is a question that we get a lot. It’s very good for following a specific format, processing the input, following specific complex instructions. <br>[0:50] Steven Heidel: So if the base model just kind of isn’t following some instructions that you’re giving it, fine-tuning can help with that. It’s also good for improving latency and reducing token usage. So your alternative here is like multi-shot examples take up a lot of space in the input prompt and a lot of you know additional cost and latency. <br>[1:12] Steven Heidel: If you can teach the Model 2 output or behave in the way you want it without all those examples you can save on both the the latency, the processing time, as well as you know the amount of money you’re sending to us. So those are those those are some things that fine tuning is good for. Some things that fine tuning is not good for that I think people still try but it doesn’t work that well is firstly teaching the model new knowledge. <br>[1:41] Steven Heidel: So again we’re really using fine tuning mostly to do to follow a given format or tone or to do some cost savings but not to kind of add things that it previously doesn’t know. Reg or if we’re in our case the the custom models offering that we have are better options for that. It’s also not great at performing multiple unrelated tasks. So the best use case for fine tuning is really sort of the same task over and over again, and not a combination of tasks. <br>[2:18] Steven Heidel: If you have a combination of tasks, either do prompt engineering, or just create one fine tune model for each one of those tasks. <br>[2:27] Steven Heidel: And then finally, kind of on the first one, a similar theme to the teaching the model new knowledge uh it’s not helpful to use fine tuning for um including up-to-date content um because a you can’t learn new knowledge but b you’re you know you can’t fine tune and deploy that quickly um to get up-to-date knowledge into the response so um what do you mean by custom models in this in that in this uh yeah yeah good question so we have uh kind of a range of fine tuning offerings there’s the self-serve api where you can <br>[3:02] Steven Heidel: just go into our platform sign up upload your training files and get a fine-tuned model out and then we have a custom models program which is actually you’re sort of partnering with us over multiple months kind of multiple million dollars engagements and we take a large corpus of your data and work with you to train, retrain the model at a sort of deeper level. And so the, you know, we’re able to incorporate techniques and research and so forth there kind of on a case by case basis that the self-serve API just doesn’t have. <br>[3:42] Steven Heidel: And so that’s for kind of a select group of partners that we’ve started working with. That’s the way that we’ve been able to get them models that understand new knowledge. So for instance. One case study that we have is Harvey, which is this legal company we’ve trained on a bunch of case law that the base model was not very good at, and given them a model that performs much better on the case law-related tasks that they have. <br>[4:16] Hamel Husain: That’s a really interesting example. Is GPT-4 not trained on case law? <br>[4:24] Steven Heidel: GPT, you should look at the case study because they have some data there on the output. But the base model tends to hallucinate more cases. There was, I think, that news article of a lawyer who got called out basically for making up a fake citation from ChatGPT or some other model. And so for their legal product, they obviously care a lot about reducing hallucinations down to as close to zero as possible. And the custom model does a lot better for that. <br>[5:00] Hamel Husain: Gotcha. I have one actually related question from Simon Willison. Simon Willison is asking, are there any really good fully fleshed out examples of fine tuning against open AI models with realistic real world examples of fine tuning training data and examples of the kinds of prompts that can be answered using the fine tune model that don’t work as well with the default models? <br>[5:25] Steven Heidel: We have a good example in our cookbook. of a Q&amp;A model that is also that is trying to teach the model to respond with I don’t know more often if the question is sort of not in the theme or not included in the examples and you can see from that that you’re kind of when you measure how often does the model say I don’t know when it actually doesn’t know that that’s higher with the fine-tuned model than it is with the base model. So cookbooksopenai.com is where you can go to find some of those notebooks. <br>[6:08] Steven Heidel: It’s the first one that comes to mind for me. Also, just kind of wanted to call out on our developer site this new guide we have for optimizing LLMs for accuracy. There’s the link there at the bottom. But we talk about kind of… you know, when to use fine tuning, when to use RAG, when to use both, when to use neither. And in that guide, we have this chart here where basically this kind of going up on the chart means you’re adding more context, more sort of new information to the model. <br>[6:45] Steven Heidel: And then going to the right on this chart is optimizing the model or changing rather how it sort of responds and acts to your inputs. And we find that… <br>[6:56] Steven Heidel: kind of a lot of use cases our base models are really really good and so most people end up you know just in sort of this this bottom left quadrant here where all you need is is prompt engineering maybe few shot examples as you go to introduced more contexts uh you know teach the model more things she’ll start to move in kind of the rag direction um uh if you’re primarily looking to change the way the rest the model follows instructions or formats responses, you know, you’re moving in the fine tuning direction. <br>[7:29] Steven Heidel: If you want both, you know, you kind of move up and to the right. But again, just calling out that guide and that link there because it contains sort of the sum of some of the experience that we’ve gained over the past year working with people on when to fine tune, when not to fine tune. and when to use some of these other techniques. So fine tuning actually tends to be, you know, one of the later techniques that we look for when we’re working with someone to try and help them build for their application. <br>[8:08] Steven Heidel: It’s not kind of the first tool that you reach for in your toolbox. It’s for the people who use it. It works very well, but a lot of people are able to, due to the strength of our base models, get away with kind of avoiding fine tuning. <br>[8:22] Hamel Husain: Can I ask you a question about the upper right-hand quadrant? Yeah, go for it. It’s bouncing back between fine-tuning a model and then adding RAG to the training examples. Yeah. What does this mean exactly? You find that people don’t add RAG to their training examples? They’re not adding RAG to their training examples. Doesn’t that mess up the production? Doesn’t that create a drift in production? Just curious what this… graphic means right here. <br>[8:52] Steven Heidel: Yeah, I don’t know if this graphic is supposed to be 100% true or just kind of representing sort of the general trend here. But it sort of depends what you need, right? If you want to add more knowledge or context, you use reg. If you want to change how the model’s responding, you use fine tuning. And if you… <br>[9:18] Steven Heidel: you know want to do both then you’re going to use a combination of both i don’t know if that really answered your question but yeah i mean you kind of did you say like don’t take it too literally don’t take it too literally um and you know this is kind of an example path that someone would take but obviously your application you might you know you might stop here you might not have any need for reg at all and just sort of go straight to the fine-tuning quadrant again we really advise that you find a <br>[9:46] Steven Heidel: set of just in general for optimizing llms on openai or elsewhere that you build for yourself a set of eight evals that you know make sense for you in your application and then just work towards whatever’s the simplest kind of thing to get you the performance you need on those evals and if that’s just prompt engineering great um if it involves fine tuning you know we have that offering too um if it involves retrieval or access to a different additional tools like code interpreter and so forth you know we want to give you those options <br>[10:20] Steven Heidel: but the key thing is that you know what works for one person one use case one customer is not going to work for someone else <br>[10:28] Hamel Husain: New client evals, I think that’s the most important topic in many cases. Do you recommend things to your customers who are fine-tuning in terms of evals, like tools or patterns or starting points or anything, any kind of… <br>[10:50] Steven Heidel: Yeah, I don’t have particular tool recommendations right now, but the thing we always recommend is building your own set of evals. rather than saying like oh look i found mmlu or whatever online um and i think this performs a little bit better on that like it’s not going to matter for you unless you’re actually answering textbook questions like mmlu does uh what’s going to matter is is real prompts from your sort of application and what what the desired responses should be um i wanted to share a cautionary tale when fine-tuning doesn’t work. <br>[11:25] Steven Heidel: It’s kind of a funny example we saw last year. Someone took, their goal was basically to fine-tune a Slack bot that would answer questions, onboarding questions from new employees at their company, and then automatically respond to them versus having to wait for someone to respond for you. <br>[11:48] Steven Heidel: So, you know that that kind of falls into our what not to do category of adding new knowledge and what happened when they fine-tuned the um model against their entire slack corpus was that the model learned uh the format of the responses but not necessarily the information so for instance someone asked you know uh this slack bot write a 500 word blog post on prompt engineering and based on the slack corpus of data the format that it learned was you know i’ll do that tomorrow um and uh you know you can probably get to say <br>[12:25] Steven Heidel: write it now and it’ll say things like okay which was you know occurred far more often than the training data um so here’s where uh i guess that kind of previous uh slide on what to do and what not to do comes into play uh what the model has learned here is how to respond to these inputs with the most common outputs it saw which in this case was sort of deferring work, and it wasn’t learning the new knowledge. <br>[12:55] Steven Heidel: So the better kind of approach to this would likely have been something like RAG on the Slack data. If you’ve used the fine-tuning API, this will be familiar to you. You format your data the same way as the chat completions API. So you have a system message, a user message. But then the only difference is here in the fine tuning data, you’re including the desired response from the assistant. So we recommend using between 50 and 100 examples. <br>[13:37] Steven Heidel: all the advice i said before applies previously like you’re trying to one model should kind of learn one set of tasks and not a diverse set of tasks so lots of examples of the same set of tasks and it’s also important to keep the shape of the system and user prompts that you use in your fine tuning data similar to what you’ll actually use when you call the model so um remember that the fine tuning makes the model learn how to respond to a particular sort of pattern or format of inputs and so if you <br>[14:18] Steven Heidel: want them if you wanted to do that you need to make sure that the inputs are kind <br>[14:22] Hamel Husain: of similar from training off to when you’re actually calling it i have an advanced question for you about this topic so uh emil and i actually face this all the time um So if you have multi-turn conversations and you have like RAG and function calling and all this stuff, and like a lot of times, let’s say you’re using a library like Langchain, a lot of like internal thoughts, quote internal thoughts. So like the first turn of the conversation might include internal thoughts. The second turn of the conversation won’t repeat all of those internal thoughts. <br>[14:55] Hamel Husain: It’ll just give like the result of whatever. <br>[14:58] Hamel Husain: uh you know those thoughts were or whatever and then like the question then quickly becomes like well how do how do you what’s the best way to prepare your fine-tuning data like because yeah like you know should you include those and like because sometimes in production you will have those internal thoughts sometimes you will not have those internal thoughts should you just like shove both in there you know and then like the question becomes oh that’s feels really duplicative <br>[15:27] Steven Heidel: because you know multi-training conversations can be very long and then like whatever internal thoughts and whatever so i just want to throw like see if you have any reaction to that yeah yeah um i mean i don’t know off the top of my head exactly what the best thing to do there would be but we do um you can experiment with the weight parameter which also can be added to this training file input so um it allows you to say you know do you want to only train for instance on the the final assistant response <br>[15:55] Steven Heidel: and learn learn that format or do you also want to look kind of learn the the the chain of thought or the um process it got to get there um in general though the uh kind of from my oops what not to do at some point um uh well it’s on a different slide but um a if you’re trying to fine-tune something that’s too complicated um it’s sort of less likely to work So, you know, the best advice, I guess, is to make some e-bells, try experimenting with the weight parameter. <br>[16:33] Steven Heidel: And then if it just isn’t working for these kind of multi-turn conversations, it could be that it’s too complicated for our self-serve fine tuning. <br>[16:42] Hamel Husain: So when the weight parameter is zero, is that equivalent to setting the label of those tokens to like negative 100 or whatever? You know, like, does it like in the… <br>[16:54] Steven Heidel: is it basically what is what is happening exactly when you it’s not going to learn any information from those messages um so by default the the user for instance uh the the the like user and system messages um uh the weight is set to zero like it’s learning that’s the context for what it’s learning there for the messages where the weight is set to one <br>[17:20] Hamel Husain: Okay, but will it still give that context to the model during training? It’ll just ignore the, it just won’t. <br>[17:27] Steven Heidel: It won’t be learning about how to respond or how to output that message, but it will be using it to learn sort of as input. <br>[17:36] Hamel Husain: Okay, I see. Yeah, <br>[17:38] Emil Sedgh: I do believe that what we did was actually to experiment with the weight parameter, and we finally managed to get it to work. But the one question I had in mind was that, In more complex examples like this, what happened was that there was no weight parameter and it was added silently. So it took us like a month of scratching our heads until we realized there’s a change in the documentation and there’s a new weight parameter. But my question from you would be, generally speaking, how does OpenAI decide what direction to take on these? <br>[18:12] Emil Sedgh: Where is the feedback coming from? Is this your internal products that you’re building? Or is it with… <br>[18:19] Steven Heidel: collaboration with your clients and could that process be more open or more documented in the future yeah um i mean we we get feedback from a lot of places there’s the open ai developer forum we work with with customers 101 um and we have account managers for some of our larger customers we’ll we’ll pull in feedback and then honestly events like like this and conferences and talks and so forth and just going out and talking to people we hear requests and ideas on things we want to add and um kind of you know collate that <br>[18:54] Steven Heidel: and try and decide what’s important so the weight parameter which was introduced recently is something that came from discussions like this um i i’m you know i guess we could have made we should have made more of splashy uh a roll out of that uh given the the amount it’s been uh or how well it’s been received um so um but uh yeah it’s you can expect going forward that we’re going to continue to add more models, more methods, more customization to the offering, but I can’t give you any specifics today. <br>[19:38] Steven Heidel: Here’s a slide you maybe want to just kind of screenshot. These are a collection of some of our learnings from working with people over the past year. on best practices for getting fine-tuning to work, some of which I’ve already covered, like making sure that you have… <br>[19:58] Steven Heidel: fewer high quality training examples that all are on a particular you know a single task um i’ll talk in the next slide about the different hyper parameters we offer currently um again the the evals there on the bottom uh important to make sure you’re using your own evals and running a baseline and optimizing against that. And then, you know, if it’s important for your application to reduce latency and cost, then you can try fine-tuning 3.5 on and comparing that to GPT-4 for your use case. <br>[20:39] Steven Heidel: And sometimes you’ll see that the fine-tuned 3.5 is going to be as good as the base 4, but obviously cheaper and faster. Hyperparameters we offer today, again, this slide was kind of designed to be just a screenshot. You can also look at the documentation. The hyperparameter which affects training the most is epochs. This is basically how many times is your training data iterated over during the training process. If it’s one, then the model, the fine-tuning process will only see each example one time. <br>[21:27] Steven Heidel: By default, if you don’t set it, we will choose something based on the size of the dataset. So if you have a really, really large dataset, we don’t want to iterate over it multiple times. But you also have control over this and can try sweeping over this parameter as necessary. If you set it too high, you’re going to overflow. If you set it too low, you’re not going to learn enough from your training set, depending on the size. Batch size and learning rate multiplier have a smaller impact on the finding tuning, but are still important. <br>[22:01] Steven Heidel: And you can see some recommendations there we have on setting those. But again, the default will try and do the best thing for the most number of people. And if you need to change it, you can. <br>[22:19] Hamel Husain: Are you going to talk about how to interpret the… you know, the training loss graph, whatever telemetry that y’all provide. <br>[22:30] Steven Heidel: Yeah, I’m not an expert on that, actually, that portion of it. So I don’t want to get it wrong. But in general, you want to see that graph go down and to the right to indicate that things are working. And if it’s sort of spiky and all over the place, that means maybe something has gone wrong. <br>[22:50] Hamel Husain: Yeah, I mean, what I always… It’s quite interesting. Whenever I fine-tune an OpenAI model, I see it go down very quickly, like with the default, every single time. And it just kind of bounces around near zero. <br>[23:02] Steven Heidel: Yep. <br>[23:03] Hamel Husain: Even with just one epoch. And it’s very interesting. I’m like, oh, okay. Should I interpret this the same as when I train Open models, like when I’m fine-tuning Open models, is there something special here that I don’t know about? <br>[23:18] Steven Heidel: that i’m like interpreting it wrong i’d like all these thoughts go through my mind uh yeah i mean we’re basically exposing the loss that you know we our infrastructure uses for doing our own training so uh i’d expect that to act behave as as you would for any other loss curve um if it’s going down really quickly immediately then you know you might try with with fewer epochs or a smaller training file and see if you get basically the same performance <br>[23:50] Hamel Husain: okay so it should look the same of what you expect from like open models or whatever something like idiosyncratic about it per se um just <br>[24:05] Steven Heidel: some examples of uh success stories we’ve seen in the past um uh you know as you fine-tune larger models The, for instance, adaptation can get better. And these are both better than the base model. This, we worked with the government of Iceland, there’s a case study, I think, online, to do grammar correction for them. And you can see that, you know, if you remember kind of that two by two grid I was showing about. <br>[24:41] Steven Heidel: optimizing responses that the kind of farther right up and right you go the better in this case examples or results you’re getting so with zero shot prompting you know their their evals were were lower than with few shot prompting which were lower than with 3.5 fine tuning and then which were in turn lower than um gpt4 fine tuning uh so um for you know if it if it’s working well on your evals you can kind of continue moving up that that <br>[25:15] Hamel Husain: mountain and get better and better results we’re moving to the right and that graph and getting better and better results the uh the thing on the left hand side of this slide is confusing me a little bit like this is this saying you move from g point gpt 3.5 to gpt4 and saw a increase in performance. But they’re both fine-tuned. So I guess it doesn’t really. <br>[25:37] Steven Heidel: Yeah, sorry. The graphic here doesn’t have the un-fine-tuned version. But those are also smaller numbers. But you can fine-tune larger models as well and get better performance in some cases. <br>[25:53] Steven Heidel: But on the right here, you can see that, like, this just the 3.5 fine tune is doing better than the gpt4 few shot prompt or zero shot prompt um which you know this this i don’t know if you can see my cursor but this third this middle bar graph here not only is getting better results than the ones on the left but it’s also going to be faster and cheaper than the gpt4 prompts without fine tunes <br>[26:24] Hamel Husain: i think it’s curious in that graph like why is rag like fine tuning plus a rag going down um i don’t know i should have looked that up before including it in my presentation maybe if <br>[26:40] Steven Heidel: the um it could be kind of a cautionary tale of like if the uh use case here did not require you to introduce new context then adding on techniques that are not needed will actually make things more complicated and worse. <br>[27:04] Emil Sedgh: One question I have is, do you know which exact model of GPT-4 are we discussing here? Because to my knowledge, the very early GPT-4 0613 is the one that is available for fine tuning. Is that the model that has been fine tuned in these graphs? or is it the newer Turbo or GPT for all models also included? <br>[27:26] Steven Heidel: To the best of my knowledge, this is the original GPT-4 because we did this engagement last year. And that’s still where we have a beta program with some select partners for that. <br>[27:45] Emil Sedgh: And I think we had the question that I’m… <br>[27:50] Steven Heidel: is relevant now are the newer models like gpt4 all going to become available for fine tuning uh we’re working on it we’re working with a select group of partners on uh on on beta testing that um and um again i can’t really share like timelines or exact plans but but you shouldn’t be surprised if over the next year we have uh more models and more methods more hyper parameters etc available in the fine tuning product <br>[28:19] Hamel Husain: I have a question that you may not be able to answer, but we’ll try. It reminded me because I saw GPT… Okay, so one of the things that is really common is people take GPT-4 and use the data to fine-tune GPT-3.5 or something like that. Now, there’s some confusion. There’s tons of confusion out there whether it’s against the OpenAI terms of service to use GPT-4 data. that is like some data to train your own model, like an open model. <br>[28:53] Hamel Husain: Like you’re not trying to compete with open AI, not trying to sell the model or do anything, but like you’re just trying to train your own like for certain use cases. Do you know if that’s against the terms of service or not? <br>[29:05] Steven Heidel: I’m not the right person to ask. I don’t say something and get someone in trouble. <br>[29:10] Steven Heidel: I know for sure that fine tuning uh or distilling a model onto an open source model and then distributing that um you know is against our terms of service i don’t know the answer to your particular question though about if you use it just in your own application uh whether or not that’s against terms of service um i can tell you that distilling gpt4 into gpt 3.5 fully through our platform for fine tuning is is is completely fine and we have a lot of people using that so um uh but yeah i don’t want to <br>[29:43] Steven Heidel: answer no problem <br>[29:45] Emil Sedgh: i know i knew it was a little bit tricky uh alex has asked the uh a question on topic that is what are the advantages of fine-tuning open ai models over an open source model And when do you think it makes sense to use OpenAI against OpenOne? <br>[30:02] Steven Heidel: So the OpenAI models, when we move forward with the GPT-4 fine tuning and some of the newer models, obviously those are state of the art right now, so you have that advantage. We also feel that the OpenAI models offer tool calling and function calling and a lot of other features, but And there’s some advantages to just not having to worry about deploying your own models and so forth. But, you know, there’s a lot of places within OpenAI’s product offerings API where, like, we’re just not covering stuff that you can do better elsewhere. <br>[30:48] Steven Heidel: And so I think, you know, provided you’re not violating the terms of service, like I said before, the… <br>[30:55] Steven Heidel: uh there’s options for people yeah <br>[30:58] Emil Sedgh: I’m also gonna uh have a stab at answering that um because when you try to do more complex stuff or in large scales none of the we have never managed to get something working as good as open AI even the GPT 3.5 models so that’s my stuff at it if you really want to get to a product that that works I think as of right now still especially if you’re using something like tool calling or you want to call them in large numbers and you don’t want to hit API rate limits or anything like that, <br>[31:29] Emil Sedgh: definitely working with OpenAI is significantly easier option. <br>[31:33] Steven Heidel: Yeah, awesome. That’s great to hear. We put a lot of work into trying to make everything turn key and just work. So I’m glad it’s working for a lot of people. Speaking of function calling, actually, this is another use case where you’ll find some folks have found advantages from doing fine tuning. And that is just kind of instructing or teaching the model to respond exactly with the right function that they need or the right output format. But I guess one caution with fine tuning for function calling is that unlike sort of, unlike when… <br>[32:23] Steven Heidel: The context for functions and putting all the definitions, all the parameters, the descriptions of everything is kind of large enough or complicated enough in many use cases that you cannot get away from including those when you actually do your prompt, if that makes sense. So, you know, sometimes we’re seeing cost savings from other use cases where you have a big, long prompt. and you can fine-tune that prompt away at the chat completion time. But because function calling, with a really long list of complex functions, it doesn’t reliably work right now to fully fine-tune that away. <br>[33:09] Steven Heidel: So sometimes it works, like I said, but just a caution there, we’ve seen this not always works. <br>[33:15] Hamel Husain: That’s interesting. Why do you think that is? If you give a whole bunch of training data that has… lists of function tools and then like how like examples of calling all of those functions and whatever why wouldn’t why why doesn’t that work do you think that’s very all the same category as the trying to teach a model to do a bunch of different unrelated tasks or <br>[33:37] Steven Heidel: i guess these are sort of somewhat related but trying to teach trying to fine-tune too many tasks into one model um uh does not work as well as uh working as reliably as the same task over and over again. <br>[33:55] Hamel Husain: What’s the order of magnitude that you have in mind when you say too many functions, like a list of… I’m just curious. <br>[34:01] Steven Heidel: Yeah, I wish I could give you a number and say at five functions it breaks, but we’ve run a lot of evals internally and seen it fail at small numbers and succeed at larger numbers. So it also depends… <br>[34:19] Steven Heidel: on kind of the the complexity of each individual function and um i also you know whether you’re trying to use parallel function calling some of these other advanced features so i you know i i hate to be a broken record a lot of the time when i’m doing these talks and just say you know well run evals and try it but um uh unfortunately that’s our best answer in a lot of cases um <br>[34:48] Emil Sedgh: Is there an evaluation framework that is focused purely on function calling and the intelligence of function calling that we can look at? <br>[34:58] Steven Heidel: I don’t know. There was the name. <br>[35:03] Hamel Husain: It’s the Berkeley Gorilla leaderboard. Yeah, that’s really focused on function calling. <br>[35:09] Steven Heidel: So that’s a good set of evals for generic function calling. But again, you’ll want to try and like the best evals for your function calling are going to be your functions. So, yeah. <br>[35:25] Steven Heidel: this was actually the answer to um simon’s question from earlier um but here’s this is on our cookbook for the q a um examples where you’re training uh fine-tuning a model to respond more often with i don’t know um and kind of reducing uh hallucinations by adding both sort of questions that you’re allowed i think the cookbook in particular uses questions about the olympics So it says, you know, if the question is about the Olympics and you know about these things, you can answer it. <br>[36:00] Steven Heidel: Otherwise, you should respond with I don’t know if it’s outside that kind of knowledge base. And fine tune does really well at sort of reducing the number of false positives. That’s all I had in terms of slides. I guess it’s kind of up to you guys whether we should do some more questions. Or I can run through a demo of our UI. I’ve got a demo of just running a fine-tuning process mechanically, how it works. <br>[36:34] Hamel Husain: I know that Emil has some good questions. So we should let him ask him some questions. <br>[36:39] Emil Sedgh: Yeah, I’m pretty much sure we can. There’s a lot of YouTube videos on how to use OpenAI, but we don’t find you elsewhere. So maybe we can use the time to extract as much as we can here. On the topic of evals, I also see a lot of people asking questions that it’s very abstract that you should run your evals. Let’s maybe get a little bit more elaborate into that. <br>[37:05] Emil Sedgh: One question I’m generally speaking thought I have is that when speaking about evals, what I’m finding out is that a lot of people think that evals also should all be AI based. It should all be another. <br>[37:19] Emil Sedgh: language model evaluating the output of is that what you mean by eval or but in our case for example we did the opposite we did software doing evaluations for uh for the language model outputs but can you elaborate a little bit in terms of what do we mean by eval exactly yeah <br>[37:35] Steven Heidel: we mean really whatever works for you so it’s obviously cheaper and easier if you don’t have to use a model to do the grading of whether an eval is correct so maybe it’s like if you’re fine-tuning a model to respond in a particular format that you just write some code that checks yes or no, did it respond in that format, and can quickly tell you how well the model is done. <br>[38:05] Steven Heidel: For sort of more complex questions about style, tone, whether or not the model included some details about this or that, you know, that’s when you’d also… need to call a grader. We do this a lot internally as well. We have all manners of graders from like simple set.contains Python function all the way to calling our latest models on the response and asking it a yes-no question about whether it’s met some criteria. Okay, perfect. <br>[38:43] Emil Sedgh: And I see some people asking about the IP and licensing terms for the data they provide for fine-tuning data. I know that you’re not a lawyer and you may not be necessarily the best person to ask this, but what happens to the data that we provide as examples for fine-tuning data? <br>[39:01] Steven Heidel: Yeah. So one thing to say right off the bat is we do not ever train on customer data. So any of our foundation models, there’s… <br>[39:13] Steven Heidel: that’s covered you know it’s i think it’s one of the first lines of our privacy doc and our terms of service but we do not use that data at all for internal purposes and also you have control over the life cycle of that data so you can upload the data for fine tuning once the fine tuning completes you delete it it gets deleted from our servers or you can leave it on leave it with us and fine-tune future models it’s up to you you have full control over that um i’ll uh you know point people <br>[39:44] Steven Heidel: towards our platform enterprise privacy documents um which is available on platformopenai.com and just say that yeah you know we we have a number of customers who are trusting us with their um a number of customers are trusting us with sort of their application their their business use case their um you know their reason for existing and we take that uh responsibility very seriously um <br>[40:11] Hamel Husain: one question i have is like what like in order of magnitude like what percent of prediction requests get routed to fine-tuned models just out of curiosity like how popular is that segment of models <br>[40:25] Steven Heidel: be really interesting to know yeah um i don’t have the numbers in front of me but i would say pretty confidently it’s like less than one percent of our api users are using fine tuning um you know when when i when i said at the beginning of this the presentation that like fine tuning is one of the last tools you should reach for it you know serious about that it’s it’s kind of um It’s what you use when you’re at the final stage of your application and you’re trying to optimize cost or latency. <br>[40:56] Steven Heidel: Or it’s what you use when the few shots, many shot prompting and your standard toolbox of tools isn’t working for you. And to be honest, like, you know, few, I guess those tools work pretty well. And so, you know, not everyone needs fine tuning. But for the people who do end up need fine tuning, they’re really happy with it and they wouldn’t have been able to get there without it. But it’s a fraction of what we do. <br>[41:31] Steven Heidel: It’s kind of the long tail of, you know, tends to be larger customers with more LLM expertise that are working on fine tuning. Less so the. <br>[41:47] Hamel Husain: you know people who are uh using chat completions and and getting getting enough out of that on its own are you finding that a lot of people who were fine-tuning as you continue to release more and more powerful models they like pivot away from it completely and they say okay like let’s stop fine-tuning let’s <br>[42:04] Steven Heidel: you know improve our prompt engineering rag whatever let’s like they were able to get off or just like yeah sometimes uh like the diff between a 3.5 fine-tune and the four base model is large enough that it’s worth sort of abandoning your fine tune and just going to our one of our new you know 4o models sometimes when that diff is small though people will stay with the 3.5 fine tunes because it’s cheaper and faster so <br>[42:34] Emil Sedgh: yeah it really it really depends uh that answer worries me a little bit maybe because you’ve been burned by google so many times but from a sustainability perspective uh <br>[42:45] Steven Heidel: as we continue to invest in our infrastructure for fine-tuning can we count on fine-tuning being available and being pushed forward but by open area as well yeah um uh we you know continue to support any application or like applications that are working well um and uh I don’t want to leave anyone high and dry um with work invested we recognize that like fine-tuning is different than than chat completions and that it’s not something, you know, there’s the work you’ve put into preparing your data set, there’s the work you’ve put in, and the money you’ve already <br>[43:24] Steven Heidel: spent with us actually training the model. So there’s, you know, a higher switching cost there, and we recognize that and respect that. <br>[43:35] Emil Sedgh: That’s great. There’s a lot of speculation about agents and what language models can do in to enable agents finally becoming a reality. Are there some flagship products that you guys have seen on OpenAI that you’re like, this is a great use of OpenAI that ended up creating a good agent or generally speaking, some of products that are doing more than just simple completions. Maybe they’re great use cases for function calling or great RAG use cases that you guys, you maybe can point us to take a look at. <br>[44:12] Steven Heidel: I’m the wrong person to ask for agent stuff. We have another team that works on the assistance API and a lot of the tool calling. I guess speaking for myself and just kind of personal interest, I find the Devon, the GitHub workspaces, GitHub AI workspaces stuff really cool. Basically like the ability to create a plan. engage with a bunch of tools and work, have, you know, different LLM threads working together to solve a more complicated coding task. I think that’s gonna be really cool. <br>[44:51] Steven Heidel: But uh yeah more more broadly than my kind of personal interests i i i’m the wrong person to ask that question is there is there a tension between like okay like if open ai starts to offer higher level services like agents <br>[45:09] Hamel Husain: or functions that you can call that they will execute for you and fine tuning because then where will you get the data for fine tuning if it’s not you’re you don’t log it yourself or whatever you know what i’m trying to ask like If it’s a service, for example, already the assistance API, you all will keep track of the conversation for you. You have to be careful to capture all the data yourself if you’re trying to reuse it for fine-tuning. And then it’s also somewhat not clear sometimes. <br>[45:47] Hamel Husain: You have to wonder, okay, what is the assistant API actually doing? How is it being cut off? Or is it being summarized or doing something fancy? Is there a tension between fine-tuning and then these products are abstracting some of what’s happening away? <br>[46:06] Steven Heidel: I don’t think so. I mean, you can use fine-tuned models within the assistance API, for instance, and for the same reasons that you’d use them in the contract completions API to craft your outputs in a certain way. The assistance API is really helping with, one, giving you access to more tools, but two, also just managing the context for you. So rather than needing to store previous turns in the conversation on your own systems, it’s quicker to get to an MVP or a product through letting us manage that. <br>[46:42] Hamel Husain: Yeah, what I was asking is mainly like, okay, if the assistance API is… truncating context, then when you fine tune your model, I suppose you should truncate that context too. You have to know what it’s doing for you. <br>[46:57] Steven Heidel: Yeah. Another question I’m not qualified to answer, but I don’t know the details on the context truncation within the assistance API right now. So it’d be hard for me to answer that. <br>[47:13] Emil Sedgh: Let me ask another question regarding function calling. Playing with function calling, our understanding is that we provide functions as JSON schemas, but down the road before they are being processed, they are actually translated into a TypeScript-like format before the language model actually processes them. Can you explain that a little bit? Is that true? And is that something that enables us to do something like maybe providing easier… Sure. function definitions for the language model? <br>[47:49] Steven Heidel: Yeah. They’re all great questions. A lot of them I’m not qualified to answer. I can’t tell you the details because I don’t know them, but I can tell you that our models only understand tokens, which are just integers. And so the JSON schema, whatever function, everything that goes into the chat completions eventually turns into tokens in sort of our own format. So… I… <br>[48:17] Hamel Husain: the like I said I can’t show the details because I don’t know them but also I think there’s all these fun experiments like where you can try to get the system prompt or the the prompt and then like people fill around and say oh okay like the function the function like the tools are being like flattened into these like TypeScript definitions <br>[48:38] Steven Heidel: I’ll tell you that I think in general a goal with the API and with chat GPT is that like The experience that we offer through the function calling input or through kind of prompting ChatGPT, the default experience without tricks is the one that we’re trying to make the best. So like, you know, that earlier this year, there was that thing where like you could you could tell ChatGPT we’re going to tip it $20 and it would give you a better response. <br>[49:14] Steven Heidel: uh you know i appreciate that people are like interested in trying to get that extra little percentage of uh of uh performance by understanding what happens under the hood but like our goal is that the default what what you provide us is just the one that works the best without needing to do any kind of weird weird runarounds and tricks and things <br>[49:05] Steven Heidel: You know, we then trained it to just give the good response without having to promise to tip it. So, you know, our kind of.</p>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/parlance-labs\.com\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../education/fine_tuning/abhishek.html" class="pagination-link" aria-label="Train (almost) any LLM using 🤗 autotrain">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Train (almost) any LLM using 🤗 autotrain</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../education/fine_tuning_course/workshop_4.html" class="pagination-link" aria-label="Deploying Fine-Tuned Models">
        <span class="nav-page-text">Deploying Fine-Tuned Models</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/HamelHusain">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hamelsmu">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/parlance-labs/website/edit/main/education/fine_tuning/steven.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>