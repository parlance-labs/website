<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-06-11">

<title>Slaying OOMs with PyTorch FSDP and torchao â€“ Parlance</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../education/fine_tuning/pawel.html" rel="next">
<link href="../../education/fine_tuning/napkin_math.html" rel="prev">
<link href="../../b.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QXGQ6F7NKT"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QXGQ6F7NKT', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<script async="" data-uid="fbcda700a8" src="https://hamel.ck.page/fbcda700a8/index.js"></script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Slaying OOMs with PyTorch FSDP and torchao â€“ Parlance">
<meta property="og:description" content="Have you ever hit an OOM (and wished you had more VRAM)? If youâ€™ve done much fine-tuning, then you have. And if you are just starting, then you will. Hop on the bus with us and feel the road become smoother as we talk about stacking together techniques like FSDP2 + QLoRa+ CPU Offloading + Fused ADAM (thanks Intel) + more in PyTorch native.">
<meta property="og:image" content="https://parlance-labs.com/education/fine_tuning/mark.png">
<meta property="og:site_name" content="Parlance">
<meta property="og:image:height" content="720">
<meta property="og:image:width" content="1280">
<meta name="twitter:title" content="Slaying OOMs with PyTorch FSDP and torchao â€“ Parlance">
<meta name="twitter:description" content="Have you ever hit an OOM (and wished you had more VRAM)? If youâ€™ve done much fine-tuning, then you have. And if you are just starting, then you will. Hop on the bus with us and feel the road become smoother as we talk about stacking together techniques like FSDP2 + QLoRa+ CPU Offloading + Fused ADAM (thanks Intel) + more in PyTorch native.">
<meta name="twitter:image" content="https://parlance-labs.com/education/fine_tuning/mark.png">
<meta name="twitter:creator" content="@HamelHusain">
<meta name="twitter:site" content="@HamelHusain">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image-height" content="720">
<meta name="twitter:image-width" content="1280">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Parlance</span>
    </a>
  </div>
          <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../services.html"> 
<span class="menu-text">Services</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../education/"> 
<span class="menu-text">Education</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../team.html"> 
<span class="menu-text">Team</span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
            <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../education/index.html#fine-tuning">Fine-Tuning</a></li><li class="breadcrumb-item"><a href="../../education/index.html#advanced-topics-in-fine-tuning">Advanced topics in fine-tuning</a></li><li class="breadcrumb-item"><a href="../../education/fine_tuning/slaying_ooms.html">Slaying OOMs with PyTorch FSDP and torchao</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Educational Resources</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/evals/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Evals</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/evals/allaire.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inspect, An OSS framework for LLM evals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/evals/ankur.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLM Eval For Text2SQL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/evals/schoelkopf.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A Deep Dive on LLM Evaluation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/rag/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">RAG</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/rag/jo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Back to Basics for RAG</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/rag/ben.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Beyond the Basics of RAG</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/rag/jason.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Systematically improving RAG applications</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/index.html#fine-tuning" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fine-Tuning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/index.html#should-you-fine-tune" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Should you fine-tune?</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning_course/workshop_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">When and Why to Fine Tune an LLM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/kyle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fine-tuning when youâ€™ve already deployed LLMs in prod</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/emmanuel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Why Fine Tuning is Dead</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/index.html#how-to-fine-tune" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to fine-tune</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/daniel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Creating, curating, and cleaning data for LLMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/mistral_ft_sophia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Best Practices For Fine Tuning Mistral</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/abhishek.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Train (almost) any LLM using ðŸ¤— autotrain</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/steven.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fine Tuning OpenAI Models - Best Practices</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning_course/workshop_4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deploying Fine-Tuned Models</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/index.html#advanced-topics-in-fine-tuning" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced topics in fine-tuning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/napkin_math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Napkin Math For Fine Tuning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/slaying_ooms.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Slaying OOMs with PyTorch FSDP and torchao</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/fine_tuning/pawel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fine Tuning LLMs for Function Calling</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../education/applications/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Applications</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/applications/simon_llm_cli/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLMs on the command line</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/applications/freddy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Building LLM Applications w/Gradio</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/applications/python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Building Full Stack Applications with Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/applications/charles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Modal: Simple Scalable Serverless Services</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../education/prompt_eng/berryman.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapters" id="toc-chapters" class="nav-link active" data-scroll-target="#chapters">Chapters</a></li>
  <li><a href="#slides" id="toc-slides" class="nav-link" data-scroll-target="#slides">Slides</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  <li><a href="#full-transcript" id="toc-full-transcript" class="nav-link" data-scroll-target="#full-transcript">Full Transcript</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/parlance-labs/website/edit/main/education/fine_tuning/slaying_ooms.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../education/index.html#fine-tuning">Fine-Tuning</a></li><li class="breadcrumb-item"><a href="../../education/index.html#advanced-topics-in-fine-tuning">Advanced topics in fine-tuning</a></li><li class="breadcrumb-item"><a href="../../education/fine_tuning/slaying_ooms.html">Slaying OOMs with PyTorch FSDP and torchao</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Slaying OOMs with PyTorch FSDP and torchao</h1>
  <div class="quarto-categories">
    <div class="quarto-category">fine-tuning</div>
    <div class="quarto-category">llm-conf-2024</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 11, 2024</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>Have you ever hit an OOM (and wished you had more VRAM)? If youâ€™ve done much fine-tuning, then you have. And if you are just starting, then you will. Hop on the bus with us and feel the road become smoother as we talk about stacking together techniques like FSDP2 + QLoRa+ CPU Offloading + Fused ADAM (thanks Intel) + more in PyTorch native.</p>
  </div>
</div>


</header>


<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/UvRl4ansfCg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="mobile-only callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Subscribe For More Educational Content
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you enjoyed this content, subscribe to receive updates on new educational content for LLMs.</p>
<center>
<script async="" data-uid="6379a28bdb" src="https://hamel.ck.page/6379a28bdb/index.js"></script>
</center>
</div>
</div>
<section id="chapters" class="level2">
<h2 class="anchored" data-anchor-id="chapters">Chapters</h2>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=0">00:00</a> Introduction</strong></p>
<p>Mark introduces the session on addressing Out of Memory (OOM) errors in PyTorch, discussing tools and techniques to handle these issues more effectively.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=30">00:30</a> Traditional Solutions to OOMs</strong></p>
<p>Mark describes conventional methods of dealing with OOMs, such as reducing batch size or model size, and the limitations of these approaches.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=48">00:48</a> VRAM Constraints</strong></p>
<p>Mark explains VRAM constraints on different GPUs and how it impacts model training, emphasizing the perpetual challenge of being VRAM-starved.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=84">01:24</a> Estimating VRAM Requirements for Your Model</strong></p>
<p>Mark outlines the components involved in estimating a modelâ€™s memory usage, including parameters, gradients, and optimizer states.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=366">06:06</a> Quantization Techniques</strong></p>
<p>Mark introduces quantization techniques, such as 4-bit quantization, to reduce model size and memory requirements. He also demonstrates using Torch compile to generate efficient quantization kernels, avoiding the complexity of writing custom CUDA kernels.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=567">09:27</a> LoRA</strong></p>
<p>Mark introduces the LoRa technique for updating a subset of parameters to save memory.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=596">09:56</a> QLORA Algorithm</strong></p>
<p>Mark details the QLORA algorithm, combining quantized parameters with selective parameter updates to enable efficient fine-tuning.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=651">10:51</a> Implementing QLORA with PyTorch</strong></p>
<p>Discussion on implementing QLORA with PyTorch, highlighting the complexity of writing efficient kernels and the benefits of using Torch compile.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=878">14:38</a> Introducing Janeâ€™s Section on Model Parallelism</strong></p>
<p>Mark hands over to Jane to discuss parallelism techniques and how to manage memory across multiple devices.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=920">15:20</a> Understanding Memory Allocation During Training</strong></p>
<p>Jane illustrates memory allocation during training, showing the impact of activations, gradients, and optimizer states. Jane also explains data parallelism and model sharding as techniques to distribute memory load across multiple GPUs.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=1065">17:45</a> Fully Sharded Data Parallel (FSDP)</strong></p>
<p>Jane introduces Fully Sharded Data Parallel (FSDP) and its mechanism to manage memory efficiently by sharding model parameters.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=1309">21:49</a> CPU Offloading</strong></p>
<p>Jane discusses CPU offloading as a method to handle memory constraints by temporarily storing parameters on the CPU during training.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=1385">23:05</a> Challenges and Improvements in FSDP</strong></p>
<p>Jane outlines the challenges with FSDP1 and introduces FSDP2, which offers more flexibility and efficiency in managing memory and data types.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=1790">29:50</a> Identifying and Addressing Performance Gaps</strong></p>
<p>Jane discusses the process of identifying performance gaps in FSDP2 and the steps taken to optimize and match the performance of FSDP1. Jane discusses benchmarking and profiling techniques that are helpful in debugging performance.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=2226">37:06</a> Overcoming Debugging Challenges</strong></p>
<p>Jane shares insights from debugging and optimizing the performance of FSDP2, highlighting the importance of detailed trace analysis. She also explains the impact of wrapping policy on memory usage.</p>
<p><strong><a href="https://youtu.be/UvRl4ansfCg?t=2858">47:38</a> How You Can Get Started</strong></p>
<p>Jane encourages students to try this process themselves in torchtune.</p>
</section>
<section id="slides" class="level2">
<h2 class="anchored" data-anchor-id="slides">Slides</h2>
<p><object data="ooms.pdf" type="application/pdf" width="100%" height="600"><a href="ooms.pdf" download="">Download PDF file.</a></object></p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p>Links to resources mentioned in the talk:</p>
<ul>
<li><a href="https://drive.google.com/drive/u/0/folders/1HmGNC4v4L5nXhtdDMVCpUBrme1ELp-2C">Slides from the talk + traces</a></li>
<li><a href="https://github.com/pytorch/pytorch/issues/114299">Go from 1 GPU to N GPUs with FSDP2</a></li>
<li><a href="https://github.com/pytorch/torchtune">End to end finetuning examples / A Native-PyTorch Library for LLM Fine-tuning</a></li>
<li><a href="https://pytorch.org/blog/understanding-gpu-memory-1/">Profiling your memory</a></li>
<li><a href="https://dev-discuss.pytorch.org/t/how-to-measure-memory-usage-from-your-model-without-running-it/2024">How to measure memory usage from your model without running it?</a></li>
<li><a href="https://llm-efficiency-challenge.github.io/">NeurIPS Large Language Model Efficiency Challenge: 1 LLM + 1GPU + 1Day</a></li>
<li><a href="https://github.com/TimDettmers/bitsandbytes/blob/main/csrc/kernels.cu">QLoRA implementation code</a></li>
</ul>
<center>
<script async="" data-uid="8a7362bdfa" src="https://hamel.ck.page/8a7362bdfa/index.js"></script>
</center>
</section>
<section id="full-transcript" class="level2">
<h2 class="anchored" data-anchor-id="full-transcript">Full Transcript</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expand to see transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><br>[0:00] Mark: Hi, everyone. Weâ€™re here to talk about slaying OOMs. OOMs are maybe the notoriously and one of the most annoying bugs to deal with in PyTorch code. Both Jane and I are developers on PyTorch Core at Meta. We wanted to talk to you about a lot of the tools weâ€™ve been building to make this process a bit easier to deal with. So traditionally, the way Iâ€™ve seen people deal with OOMs is this way, which is basically people see an OOM, theyâ€™re like, OK, crap, what do I do? <br>[0:30] Mark: And then the sort of two nuclear options are you either reduce your batch size or you reduce the size of your model. You go, oh, this thingâ€™s too big. Let me just have something smaller. But this is a very coarse tool. And thereâ€™s certainly a lot more finer-grained things you could do with a bit more knowledge. So The first constraint you have is essentially like how much VRAM you have on your chip. So for example, for 3090s and 4090s, which are very popular consumer cards, you have about 24 gigs. <br>[0:59] Mark: And then for the A100s, you have like either 40 or 80. I think the H100 is like about 100, if I recall correctly. But my point is, is that like youâ€™re always going to be VRAM starved. And specifically for consumer cards, you know, if I were to speculate, like I would speculate that the 5090 probably also has like around 24 gigs of VRAM. And so weâ€™re always going to be in the VRAM-constrained environment. <br>[1:24] Mark: But again, as weâ€™re thinking about memory for a model, instead of just thinking about, oh, itâ€™s like something is blah gigabytes, letâ€™s be a bit more concrete about whatâ€™s involved in estimating the size of a model. So you have like three core buckets. So for example, letâ€™s say you say, oh, Iâ€™m downloading Lama 7b. 7b is referring, 7b is the number of parameters. And if each of those parameters is an FB16. then you need two bytes per parameter, which means that the total size of the model is about like 14 gigs. <br>[1:55] Mark: And indeed, if you download like Lama 7B on your desk, like thatâ€™s roughly how big itâ€™s going to be. But thatâ€™s not enough. Like, but if that was enough, you know, you could just like run Lama 7B on a 3090, but you canâ€™t. And how come, right? So the reason why is like, well, like if youâ€™re doing fine tuning or training, you also need to basically per parameter, you have gradients and your gradients will also be in FP16. And soâ€¦ you basically end up with another 14 gigs. <br>[2:22] Mark: And then finally, like a detail everyone seems to forget all the time is also the size of your optimizer. So the single most popular optimizer used in the world is Atom. And Atom is like the amount of memory that Atom takes is twice the amount of parameters. So basically, if your parameters are 14 gigs, Atom takes 28. So if you sum all of these up, you get like 14 plus 14 plus 28, which is 56 gigs, which is bigger than 40 gigs, so bigger than most GPUs that people have. <br>[2:46] Mark: So this is sort of the traditional, what people would refer to as a full fine tune. I also sort of neglected to mention activations. So activations are basically the interâ€¦ Letâ€™s say youâ€™re runningâ€¦ For example, you have your weights. It takes times a certain input, so WX. The output of that is your activations. Activations tend to dominate the VRAM of your model. Larger batch sizes and context length. Thatâ€™s why optimizations like Flash Attention are really important. Iâ€™m not going to talk too much about activations like Bert in the beginning. <br>[3:21] Mark: But theyâ€™re a bit harder to estimate. They donâ€™t have as clean of a formula as the gradient sun optimizers do. But there are tools that can help you estimate it, which are a bit better than just doing math that I found can be a bit finicky and error-prone. Anyway, so the first thing you might think, like, looking at this picture, youâ€™re like, okay, why the heck does Adam take this much memory? Like Iâ€™m going to instead use another optimizer, maybe like SGD, which has no memory overhead. Sorry, can I ask a question real quick? <br>[3:49] Mark: Is there a rule of thumb about how much, like, you know, for the activations that you kind of try to give yourself headroom for? Um, I usually always estimate it. Like, Jane, have you found a good heuristic for it? <br>[4:07] Jane: Well, for activations, it usually corresponds to your batch size. Estimating it is doable for things like Lama or transformers, where you could literally sit down, do some math, and figure out the sizes of everything. But otherwise, for other models, thereâ€™s no straight-up formula like, oh, Adam is 2x per rams, that type of stuff. <br>[4:28] Mark: Yeah, so for what itâ€™s worth, though, the reason I showed this picture is if you sort of, as you slowly, as the batch size gets to about 1,000 or the context length gets to about 3,000 or so, 99% of the memory overhead of your model is going to be activations. So I just think of it more mentally as if I go to the large batch size and context length, this is the bottleneck. Otherwise, itâ€™s not. <br>[4:52] Mark: So again, like back to Adam, like you might think like, Hey, like thereâ€™s, thereâ€™s always a new optimizer that comes out and people say, Oh, like thereâ€™s this new fancy optimizer. Itâ€™s more memory efficient. The problem with a lot of that work is that like, Atom is basically so used and so popular because it works. And thereâ€™s like tons of, thereâ€™s like, thereâ€™s a close to a decade worth of like papers and people show anecdotal results showing that itâ€™s like a great optimizer. <br>[5:19] Mark: And that hasnâ€™t been true for a lot of like newer optimizers that have been introduced. So conceptually, you might think this is the bottleneck, but it ends up being like a very poor first thing to try to make, like replacing Atom is very, very challenging as a first step. OK, so letâ€™s instead take a look at the parameters. So like we said, basically, at Lama7b, we have the parameters, and we have 14 gigs at FP16. And you might have heard of 4-bit quantization. <br>[5:44] Mark: And so what weâ€™re going to do is weâ€™re going to basically take every one of those weights and turn them into int4. For an int4, itâ€™s actually a sub-byte D-type. So basically, every int4 is half a byte. And so roughly, you get a model thatâ€™s like about 3 and 1 half gigs this way. So yeah, great. So we found basically a good approach to dealing with the parameters. Conceptually, the way this works, itâ€™s not like a two call. <br>[6:12] Mark: The way a lot of quantization kernels look like, basically, if you wanted to, for example, cast FP32 to an int8, generally the formulas look very similar to this, which is you basically go over all of the elements, every element of your vector or your tensor, you find what the maximum value is, and then you use that to figure out how to correctly scale the values in the new int8 domain. So the formulas for this ends up looking very much like this. <br>[6:40] Mark: And basically, quantizing is going from Fp32 to int8, and then dequantizing is going from int8 back to Fp32. So this terminology is very common in the community. Great. So you might think now, well, how do we make sure that this process is fast? And historically, you would rely on people writing custom CUDA kernels for this. But those are hard to write. <br>[7:02] Mark: And so as a good workaround, like what Iâ€™ve been using a lot has been Torch compile in this case, where I just made a couple of simple changes to this code, where I decorated the functions with Torch compile. And then I also added this environment. Okay, I see a question by Roy. You have to unmute yourself. All right. Maybe Iâ€™ll keep going then. So what weâ€™ve done here is weâ€™ve just decorated these functions with Torch compile. And thereâ€™s also this very handy environment variable called Torch logs output code that I use all the time. <br>[7:43] Mark: And if you enable this, you can actually code generate an efficient Trident kernel that basically, so in this case, this is how the quantization code would work. So the point is that you donâ€™t really need to learn How to write CUDA or Trident kernels in this case. So you can use it as a starting point and get efficient kernels out of it. So this doesnâ€™t require a lot of domain expertise. See, Andres also has a question. All right, people, can they unmute? Maybe thatâ€™s the problem. <br>[8:16] Jane: Thereâ€™s also questions in the Q&amp;A. I can read some of them if you want to address them now, Mark. <br>[8:21] Mark: Yeah, letâ€™s do that. Thank you. <br>[8:22] Jane: Okay, the first one says, how do these calculations for memory also apply to machines with multiple GPUs to get to 24 gigabytes, like two 12-gigabyte cards? <br>[8:31] Mark: 212. I see. So regarding for like if you hypothetically, you know, if NVIDIA released like a hypothetical GPU with like 200 gigs of VRAM, like the same sort of calculations would help. But like for multiple GPUs, I think like Janeâ€™s going to be talking a lot more about how this works with like sharding, model sharding. So Iâ€™m not going to be covering that right now. Oh, this is going to come in a few minutes. Cool. Cool. So letâ€™s go back to gradients. So remember we had the parameters, then we have the gradients. <br>[9:01] Mark: The gradients is you have like, itâ€™s again, like you need a gradient per model parameter. So letâ€™s say we quantize the gradients to four bits. Like, would this work? And the answer is it simply does not anecdotally. Like, your model will not converge because thereâ€™s just, like, not information in the backwards pass. So this is just, like, scientifically, if you were to just run convergence studies, you wouldnâ€™t get anywhere doing this. So thatâ€™s how we know this is not, like, very fruitful. Okay. But thereâ€™s other tricks. <br>[9:29] Mark: Like, basically, the main trick to make this work is LoRa. And what LoRa is, is basically, I think a lot of people might have already talked about LoRa here. But the core idea is that instead of updating the weights for all of your parameters, you pick a subset of the parameters for which you update the weights. And we call these like adapters. And so the idea now is that instead of basically quantizing the gradients directly, we make it so that itâ€™s only a very small percentage of the parameters that actually need gradients. <br>[10:04] Mark: And so this lets us shave off, like, letâ€™s say in the QLORA paper, only like 2% of the total model parameters are trainable and will thus have like gradients and optimizer states associated with them. So basically doing that, plus quantizing your parameters to 4-bit gets you QLORA. So this is exactly what the QLORA algorithm is at a high level. So great. So we got this working. Anecdotally also, because a lot of this stuff is scientific in the sense that we know that Qlora works because everyone uses it and says it works. <br>[10:38] Mark: And last year I helped host a NeurIPS competition about fine tuning. There was no entry that did not use Qlora. It was by far 99% of the meta and all of the winners used Qlora. However, implementing Qlora is kind of tricky. Basically Qlora was mostly implemented by Tim Detmers. If you look at the file, itâ€™s about 4,000 lines of CUDA code. Some people here may know me by the Discord group CUDA mode. <br>[11:06] Mark: The way this term came about was Tim Detmers was describing his process for writing CUDA kernels and it was basically he sits down in a room, no music, no lights, nothing, and he just wrote the kernels in a single night. I personally do not have the ability to write such kernels in a single night. So for me, what has been much more accessible is basically writing those kernels in pure PyTorch and compiling them. <br>[11:29] Mark: And one of the reasons why this file is very long, by the way, is I gave you this nice picture of Qlora, but the algorithm has a lot more details. Basically the weights arenâ€™t in int4, theyâ€™re in a format called NF4, which basically mimics the normal distribution of it better. You also canâ€™t just matrix multiply and have four tensors, so you need to upcast them to be F16 and then do a MathML. Remember when I said that itâ€™s very important to figure out what the max is in the original tensor? <br>[11:55] Mark: Well, this makes you very sensitive to outliers, and thatâ€™s why people do it in blocks. Then Qlora also has basically scales per block, but then it also quantizes the scales in what they call double quantization. And so itâ€™s just like a lot. Like basically itâ€™s just a lot of math and that you need to write to be productive. And the alternative we have now at this point is like basically this kernel was just written by Driscusus, also on the PyTorch team. <br>[12:21] Mark: And what he did was in basically about 900 lines of Python code, got the NF4 tensor from Qlora working for FSTP without writing any custom code. So this is all pure Python. So you can see, for example, here where it says double quantization, the full algorithm is, okay, well, we have these NF4 tensors. Then weâ€™re doing the double quantization, and weâ€™re doing a normalization, and then we return. And this is all Pythonic code, so you can add breakpoints, you can read it, you can understand it. <br>[12:51] Mark: So again, this is not a great intro quantization algorithm to understand. The core idea is really covered here. But if you understand this, youâ€™re well on your way to understanding more complex kernels. So make sure to just go poke around at that when you have some free time. So the other thing, though, is that within PyTorch itself, PyTorch does not have a concept of an NF4 tensor. PyTorch goes down to int8, it has FP16, and it recently has FP8, and thatâ€™s it. But weâ€™ve seen a lot of people experiment with lower D-type kernels. <br>[13:27] Mark: Theyâ€™re just not doing it with PyTorch. Today they have actually a way of creating like native data types. And so this is using some modern machine machinery called tensor subclasses, which is probably the feature that PyTorch devs are the most excited about internally. And what this does is you can basically override PyTorch ops such that, for example, with NF4, the way you do a matrix multiplication over an input is you cast the weight from basically an a fourth or int four to the basically the weight of the input, which is an FP 16. <br>[13:58] Mark: And then you do like a matrix multiplication. You can customize all of this logic using like operations using tensor subclasses in this way. And most notably, you can also define like what the semantics should be for how this data type should be distributed over multiple devices. So if youâ€™re getting QLora not composing with any sort of PyTorch subsystem, generally subclasses is one way of modifying the behavior of PyTorch purely in Python without being a developer on the PyTorch team. <br>[14:28] Mark: So yeah, I know this is a lot, but yeah, I guess now Iâ€™m going to hand it off to Jane, whoâ€™s going to talk a lot more about how we got this working over multiple devices. So let me, Iâ€™ll stop sharing my screen. <br>[14:38] Jane: Cool. Thereâ€™s also a question in the Q&amp;A you can answer in the meantime about fine-tuning full parameter Lama 3 8B on 24 gigabytes. So the question is, so no way to do that, basically? <br>[14:51] Mark: So thereâ€™s no way to fine-tune, like, yeah, floating point Lama 3 8B on 24. Yes, thatâ€™s going to be very challenging. Fundamentally, you would need a smaller model, and itâ€™s kind of why QLore has become such a dominant algorithm to work with this size. <br>[15:05] Jane: Hi, Iâ€™m sharing my screen. Well, Iâ€™m trying to share my screen. There it is. There it is. Okay, can people see it? <br>[15:16] Mark: Not yet. <br>[15:17] Jane: How about now? <br>[15:18] Mark: There we go. There we go. <br>[15:20] Jane: Okay. So following up with your questions, like, oh, dang, if we only have one GPU, it might not be enough to fit all the things we want to do. So on the left here, Iâ€™ve did a little illustration of what memory looks like when youâ€™re training. So it goes left to right. So in your forward, as youâ€™re training, you gain activations that youâ€™re saving for the backward and the backward, you start using them. So they go down. But then in the backwards, youâ€™re building up your gradients for your parameters. So. the grads also goes up. <br>[15:48] Jane: And at one point, the activations are all gone and the grads need to be there for the optimizer step. So I note that optimizer step, the state is an atom W state and it is about 2X bigger than the params. I like measured it. So thatâ€™s the left side. And just huge disclaimer, thereâ€™s more in your model. When youâ€™re doing math, you sometimes need scratch space. So there are intermediate tensors that are not in this illustration, but they will not matter as much. Okay. <br>[16:19] Jane: If people have questions on that already, please ask now because you will be seeing this little boy a lot. No questions? Great. Okay. So letâ€™s say itâ€™s too tall. At the peak here, you see itâ€™s just taller than your GPU. And GPU is sad, but itâ€™s okay. GPU has a twin. So now the question is to you. What happens? What would you do if you had two GPUs? How would you fit it within that? Like, itâ€™s clearly dividable. So whatâ€™s the way to do it? Do people have answers? Has anyone commented on the Discord? <br>[16:53] Jane: I canâ€™t see the Discord right now. <br>[16:57] Mark: No answer in the Discord yet. <br>[16:59] Jane: Wow. Okay. I will answer my own question. So, oh, I guess it wasnâ€™t so obvious after all, but weâ€™ll start with just parallelizing your data. So as mentioned earlier, parallelizing your data will cut down on the batch size, which contributes to this activation memory. So like your params are the same, everything else is the same because they relate to the params like Mark said. But when you slash your activations in half, you get that peak to be lower on each device. Note that everything else is replicated. But letâ€™s make this harder. <br>[17:29] Jane: Letâ€™s say you have less memory than that. And even after doing that data parallelism, it was still oohing. Itâ€™s still out of memory. What else can you do? And here you can get you can go do what Mark was mentioning, but not with quantization. You can shard your parameters in half. You can be like, I want half of my params to live on the first one and half to live on the other. And since the Coradâ€™s and Adam Wâ€™s state correspond with the params, each of them also become halved. <br>[17:59] Jane: So now youâ€™re like, wow, Iâ€™m doing great. This is awesome. I now can go on with my life. But note that this is not the entire picture. Because thereâ€™s some complications when you do sharding across anything. Because when you shard anything, you kind of, at some point, youâ€™re going to need to bring them back. So imagine youâ€™re doing your model training at this current moment. Youâ€™re running on GPU zero. Youâ€™re doing your first linear and youâ€™re like, oh crap, I only have half of my parameters. <br>[18:27] Jane: The other half of me is in that GPU over there. What am I going to do? Well, what youâ€™re going to do is youâ€™re going to be like, yo. Other GPU, we got to talk, we got to exchange some parameters and you will do that. And so what that really looks like is for every step, every layer you run through. youâ€™re going to need a little more memory thatâ€™s just representing the layer thatâ€™s currently getting processed. And FSDP will do this in a way. So, yeah, this technique, itâ€™s called fully sharded data parallel. <br>[18:58] Jane: You donâ€™t need to remember that. Itâ€™s okay. Weâ€™re talking about FSDP the whole time anyway. But, like, it will save, it will bring in the memory you need to do, like, a linear, like a matmul. And once itâ€™s done, itâ€™ll be like, oh, I donâ€™t want this anymore. And then it will put that back and drop it. And it will keep doing that to make sure that you donâ€™t peek too much. <br>[19:17] Mark: All right. <br>[19:19] Jane: But youâ€™re like, how do we know that itâ€™s small? Well, you donâ€™t. Well, you do. Because youâ€™re the user. And you get to determine what gets like what gets considered as a layer in FSDP. So this tree thing might look a little scary, but this is Lama2. Lama2 is a transformer decoder that kind of branches into a bunch of things, including 32 transformer decoder layers. And then those branch into a bunch of things, including attention. <br>[19:47] Jane: And then if you do LoRa, then your LoRa has linears and it just keeps going, dot, dot, dot, dot, dot, dot. But itâ€™s a big tree. And how FSDP wraps things determines what gets brought in when you need something. So if thatâ€™s a little confusing, itâ€™s okay. But you can think of each of these blobs, like this green blob is one layer, this bigâ€¦ <br>[20:08] Jane: blue blob is another layer, but FSDP would wrap, in this case, if youâ€™re specifying linear, then it will be like, okay, I found a linear, thatâ€™s my one layer, and then youâ€™ll find another linear, and thatâ€™s one layer, and it will kind of iterate from bottom up to do that, and because So in this specific wrapping policy, the linears are wrapped and the transformer decoder layers are wrapped. And then everything else gets wrapped. So each blob is its own layer. And if youâ€™re like, Jane, why are you telling me about this? This is so confusing. <br>[20:40] Jane: I am now lost. Donâ€™t worry. So the big key point here is that the blobs correspond to how big this little orange dotted box is going to be. So the bigger the blobs, the more memory youâ€™re going to need to bring in at a time. So the bigger that box is going to be. So the way you wrap can really influence the memory you use. Okay, pausing here. Questions, comments? Okay. Next. <br>[21:11] Mark: Weâ€™re actually getting a question around MOEs, but yeah, itâ€™s like, how does model tensor parallelism work for MOEs? <br>[21:19] Jane: You are getting very ahead. But yeah, so youâ€™re right, thereâ€™s a lot more ways to parallelize, but today weâ€™re going to focus on FSDP. And the cool thing about FSDP2, which weâ€™re going to introduce, is that it will handle that tensor parallelism more easily than todayâ€™s FSDP. Weâ€™ll get into that soon. Okay. So letâ€™s say after you do all of that, you still, like, what can you do now? What else is there? What is left? And thatâ€™s where CPU offloading comes in. <br>[21:51] Jane: And itâ€™s nice because CPU is like your little brother on the side whoâ€™s, like, not doing anything as youâ€™re, like, trying to beef up stuff. But it can hold things. So you can make it hold your parameters as you are iterating. So in this case, youâ€™re just like, letâ€™s just put the parameters in CPU, and when we need it, we will move what we need to GPU, do the all gather, do that sharing of knowledge, sharing of data and parameters, and then move on with our merry lives. <br>[22:17] Jane: And with that, with CPU offloading, you get your memory to be much, much smaller because the biggest chunks here are now living in CPU. So note that we really want the GPU. Like GPUs are accelerators. Theyâ€™re meant to do like beefy work. And your forward and backward are the beefy work in a model usually. So for the optimizer step, people are like, itâ€™s fine. We donâ€™t need the GPU for that. <br>[22:43] Jane: And in this case, weâ€™ll only bring in the parameters for forward backward and be like, OK, we can put that on CPU now, which means the optimizer step runs on CPU. And you also save a lot of space on your GPU if you host your whole atom state there. Ah, so thereâ€™s that. Okay, so you might be wondering, Jane, FSDP has been published for like a long time now. Why are you explaining this to us? What is the point? People use it. In fact, thatâ€™s true. <br>[23:13] Jane: People like answer.ai who are wonderful, they already built out an integration for FSDP and bits and bytes params for bit to make Qlora happen. But itâ€™s kind of annoying to work with FSDP1. They had to do a lot. And we came out with per-parameter FSDP, which I will also call FSDP2 for later. And what is that? So letâ€™s start with the status quo. Like what is it today? Letâ€™s say you, just for our toy example, you have three tensors that you need to distribute across your two GPUs. And they are these shapes. <br>[23:49] Jane: So the goal is that you want to make that exchange of, you know, when youâ€™re talking to the other GPU, that thing efficient. And nickel, which is the software and driver stuff that does it, it requires that each GPU will give the same tensor size. So those are, thatâ€™s the constraint. What does FSDP1 do today? Okay, what it does is it flattens all your tensors. And this is what they look like in memory. So flattening is actually pretty chill. And then itâ€™s going to line them up in a line. <br>[24:20] Jane: And then itâ€™s just going to slice it in the middle. And if itâ€™s not even, it will add a padding at the end. And then, because now itâ€™s just arbitrarily sliced in the middle, it will be like, alright, this half goes to 1, this half goes to 0, oh, I guess 0 and 1. And so youâ€™ll end up with something like this, where tensor 1 and I guess a little more than a third of T2 will live on GPU 0, and then the other half of this, including the padding, will live on GPU 1. <br>[24:51] Jane: And this is nice, but note that the way this is implemented today is that T1 and half of T2 is going to get smooshed into one tensor, which is a big con. Weâ€™ll get into that later. And same thing with T2, T3, and the padding. That gets moved into one tensor. And we call that tensor a flat parameter because itâ€™s so flat. <br>[25:15] Jane: And some reasons why you might already be thinking, hmm, this might not be a good idea after all, is the fact that this forces T1 and T2 and T3 to all have the same D type, to have the same requires gradness, and all the other metadata you might want your tensors to have. Okay. Well, what if we tried down a different path of dividing things in two? So what weâ€™re going to do is weâ€™re going to slice every tensor first. Weâ€™re going to cut T1 in half, T2 in half, and T3 in half. Great. <br>[25:46] Jane: Except we notice that T2 needs some padding because itâ€™s 3 by 3. You canâ€™t really cut that in half. Weâ€™ll do it. Itâ€™s fine. Weâ€™ll shard. And that way, what this will look like is every tensor gets its own representation on this GPU. And this is great. The main pro is that they keep their identity. Like if T1 was UN8 and T2 were BF16, totally fine. They can stay that way. But in the previous case, in FSDP1, you wouldnâ€™t even be able to put them together. <br>[26:18] Jane: Thatâ€™s just like, or youâ€™d have to hack around it a lot. And this gets into the QLORA stuff soon. Very soon as well. Okay. There is a con to this. Because of all the slicing and rearranging you have to do, there are extra copies to FSDB2. So thatâ€™s a con, but the pros it gives are so much more worth it. And just a recap of before, in more clear terms, like a flat parameter would force all three of these tensors to share all the metadata they have that a tensor can have. <br>[26:51] Jane: And in FSDB2, because they are individual separate tensors, we call them detensors because theyâ€™re not like, you know, the full tensor, theyâ€™re a distributed tensor, theyâ€™re smaller. They can be themselves, they get to keep their identities, they can have their own D type, their own requires grad. And so youâ€™re like, okay, but why? So if you think of quantization, which Mark talked about earlier, what if you wanted T1 to be UN8, T2 to be full size, like FP32, any other size? In the first regime, unacceptable. The second regime, totally fine. <br>[27:27] Jane: No one is going to bother you. FSTP will do that. Another thing that is very popular nowadays that LoRa is, is you donâ€™t really want to train the big stuff because that will require big grads, big optimizer step. So what if T2 is frozen, you donâ€™t want it to actually train, and T3 is the LoRa adapter that you do want to train? <br>[27:50] Jane: In that case, In your first world, you still canâ€™t have that because a tensor can only have one requiresGrad, and the flat parameter here will force you to either make it requiresGrad or not requiresGrad, or to force you to do a lot of fancy hacking to make it work. All of these things, all of these concepts that are like, oh, I wish I had this. In FSDP 1 today, it would be difficult. But in FSDP 2, itâ€™s for free. <br>[28:16] Jane: And another thing thatâ€™s really cool about Fsdp2 that is its own other lecture entirely is memory determinism. So one of the major implementation changes is that now Fsdp2 actually guarantees that you will have only that small little sliver of memory before, like this little orange thing. Whereas Fsdp1 actually didnâ€™t do it well enough and could cause memory spikes that are not deterministic. But yeah, for this one, you should read the blog links here if you want more details. Okay. <br>[28:53] Jane: So now that we have Fsdp2 and weâ€™re like, this should be easier to use, letâ€™s do it. Letâ€™s use it. And Wei did do that. We did do that. So Wei, whoâ€™s another dude on the team, he wrote this PR here that puts together Fsdp2 and NF4. And it works. It works. Itâ€™s great. We know like, okay, like FSTP2 is cleaner, itâ€™s more composable. But the last question remains of like, can this actually replace FSTP1? <br>[29:22] Jane: Like we would love to use it, but can you tell us that it is good on perf, that we wonâ€™t be slower than before. And so thatâ€™s what the next few slides are going to be. All right, pausing here to see if people have questions, thoughts. If not, weâ€™re going to go with the plan. All right. So hereâ€™s the plan. The plan is Iâ€™m going to go get some GPUs. Weâ€™re going to run some benchmarks. And then weâ€™re going to make sure those are the same benchmark. <br>[29:50] Jane: And then once they are, weâ€™re going to record some gaps and figure out what the gaps are and if we could make them faster. All right. So getting some GPUs, this is the easiest part of the journey. You just go to Vast AI and then you ask for it. Well, first you need to have money and then you go and youâ€™re like, give me two 3090s or 4090s. And I got to, they are 24 gigabytes each for VRAM. There are some other details here if you care, but theyâ€™re not super relevant this time. <br>[30:19] Jane: Just know that I have two, I have consumer hardware, and they are 24 each. So I ran a bunch of benchmarks on answer.aiâ€™s train.py, which is our baseline, like FSDP1 and BNB. Thatâ€™s our baseline. And Iâ€™m usingâ€¦ the batch size 8 as a baseline, and just so that it works. If youâ€™re curious, if you wanted to run this for yourself, the command is right here. Feel free to copy paste that in the future, but you could just pay attention now. I ran the same thing in the TorchTune recipe. <br>[30:56] Jane: One difference in TorchTune and train.py is that it uses a YAML for the config versus the command line. Itâ€™s just different. And I did have to tweak the YAML quite a bit to make sure that I was running the same config. And since And then these were my results. So peak memory wise, we were doing we were doing better. And for runtime, though, we were like 19% slower. <br>[31:21] Jane: So someone here might be like, FSDP2, we know itâ€™s stricter about memory, we know it requires extra copies, that makes sense that weâ€™re better at memory and worse at runtime, right? But no, no, no, we got to be diligent. And very quickly, if you look at the traces, it reveals some troubling shenanigans. So, hereâ€™s the two traces. On the top is the baseline. On the bottom is our new trace. Can you spot the difference? Okay. Thereâ€™s a lot of differences. So, Iâ€™ll just go with my favorite one. <br>[31:54] Jane: I work on optimizers and those little light blue things are optimizer steps. And immediately I was like, dude, the optimizer is taking so much longer. What could that be? And so, this is where I get into the actual tracing stuff. I wonder if I can actually show you the traces. That would be pretty cool. Okay. Iâ€™m going to stop sharing to reshare and then we can do that. Letâ€™s just share my entire screen. Okay. Do people see traces? Yep. Okay. <br>[32:33] Mark: Awesome. So, <br>[32:34] Jane: Iâ€™m going to go ahead and share my screen. So, on the left is our baseline, on the right is the slow boy. So in our baseline, weâ€™re going to go to the second step because the first step is always full of like warm up stuff and initiating stuff. So weâ€™re just going to ignore that and weâ€™re going to go here because every other step after this is much more normalized. And something you can do, Iâ€™m using Perfetto. I donâ€™t know if people are familiar with Perfetto already, but itâ€™s been pretty helpful. Yeah. Okay. <br>[33:12] Jane: And something that is super useful and nice in Profetto is you can highlight a region and it will give you profile or profile or profile or while I cannot talk today results on here. So here you can see that there are a bunch of it tells you what thing takes the longest. Itâ€™s like the moles take 77 milliseconds and there are 70 768 of them. And on this side, when we do that. weâ€™re going to notice some very different numbers. So here, the mole also takes the longest, but thereâ€™s 1,700 of them compared to 700. <br>[33:48] Jane: And you might be like, what is going on here? But letâ€™s go look at the smallest number. In optimizers, thereâ€™s only one divide. You can just take my word for that. So here we know that there are 384 divs, which means that there are 384 parameters. Here, we see that there are 800. 896, which is like more than double. And so letâ€™s go find a div. Like, where are they? And here you can just like do do do. But you can already notice that everything kind of gets doubled here. Whereas in here. <br>[34:25] Jane: they are just called immediately. So like this A10 mole goes directly to A10-2. This A10 mole though goes to A10 mole again. And youâ€™re like, wait, what is going on? Why is that? And this is where you learn the painful things of tensor subclass dispatch. So since weâ€™re using detensors, it means that it goes into this mole as a detensor. And then detensor is like, all right, let me do my metadata unwrapping. And now you get to go to A10 mole as aâ€¦ playing tensor now. So there are double the amounts. <br>[34:58] Jane: And just to spare you some math, um, spare you some math, it turns out that if we divide this mole by two or the div by two or any of the things that run once, it shows that we actually are running training on more parameters than we thought. So in the left side, weâ€™re only running it for 384, and the right side, weâ€™re running 64 more parameters. Like, can people guess where this came from? I will show you what war story of debugging has shown me. <br>[35:37] Jane: In the end, I realized that this was a config difference, where if you are fast at reading, you might have already spotted the difference here. The answer is that, in train.py, they do not glorify the output projection, whereas in our world, we do do that. And since glorifying means you add two adapters and there are 32 layers, 32 times 2 is 64 more extra parameters to train. So yeah, that was bad. And that was a great lesson because it was like, I was not measuring apple to apples. <br>[36:12] Jane: And I needed to do some other things to make sure that we were. So the first thing is like making sure the data set was the same, making sure that every parameter was the same, changing the wrapping policy to be the same. And after I did all of that, I ran another benchmark. And here, I mean, that was like also weeks of work, by the way, like, like, it was not easy to figure out every little difference and why they were different. <br>[36:36] Jane: But after all of that, I was like, let me run it again, maybe it will be better. But no, it was still slow. It was actually slower than before. But the peak memory was a lot better. So however, I was still happy, because even though it may feel like we took a step back, we actually made a giant leap forward. on blocking the first real step, which is like now that we have apples to apples, there are things that will match. And then we should just look at things that are different. <br>[37:02] Jane: And so thatâ€™s where I could start playing my very long game of spot the difference. The first thing I did, though, was like measure their runtime. And here you can see that the forward, backward and optimizer were like the culprits. And thatâ€™s how I knew what to focus on. OK, so this is a big slide. I, okay. If at this point you have not looked at the traces yet, but you would like to, I sent a link to the Google Drive in the Discord, and there are the traces that you want. <br>[37:38] Jane: Thereâ€™s like, there are four of them, and the two you care about to look at now are the ones that donâ€™t start with. bad or final, the other two. Do the Answer AI one and the TorchTune one. But Iâ€™m going to, I already found these gaps for you, and itâ€™d be fun, if you find it fun, if you want to learn more about traces, itâ€™d be fun if you could find each of these yourself, like where they are located, and do the same exploration I did. <br>[38:02] Jane: But because this is a presentation and I do want to save time, weâ€™re going to dive right into what these gaps were and where, like, how we ended up fighting them. So the very first gap is, yeah, so weâ€™re going to go. And this is the link, by the way. Okay, the very first gap is the optimizer step was still slower. But remember how earlier I hinted that there was all this overhead here for detensor? And the way it works is because of C++ dispatch, it just takes a long time to do that kernel call. <br>[38:34] Jane: And because there are so many calls, all of this overhead adds up to make the optimizer step three times slower. And also another minor detail, if you look at this, this is 64 microseconds and this is 81. The answer for that is because the parameter is not contiguous, but thatâ€™s more minor. So the solution here is actually really chill. We worked with Intel, we were like, hey, what if we just had a fused atom? Like your optimizer step, but all happening in one kernel, so you can dispatch just once and get results. <br>[39:08] Jane: So this avoids all that overhead because thereâ€™s just one kernel now versus the other kernel. 384 times however many ops there are. And it also leverages vectorization. So we go from about one second to 120 milliseconds, which is like an 8x speedup. So thatâ€™s one gap that is now gone. All right. Pausing here to see if people have questions. <br>[39:38] Mark: Iâ€™ve been speed answering everyone on Discord. <br>[39:41] Jane: Okay, nice. Okay. I was like, are they lost? But no. Okay. <br>[39:46] Mark: Theyâ€™re very happy, actually. People are saying they love this kind of debugging. And yeah, people love it. <br>[39:51] Jane: Okay. Well, letâ€™s keep going. So the next three are a little boring, but weâ€™re gonna go through. And there was a lot of pain in this one, actually. This second gap was crazy. So I went and the way I literally did this was the most brute force way you can imagine. Like you open your trace, you find the first forward, you find the first GPU kernel, and youâ€™re just like, do they match in size, shape, input, everything? And I would do that for you. <br>[40:18] Jane: But we are a little short on time, so Iâ€™m going to just show you what the difference is here. And the first difference that was major was that the second all-gather in every linear, like every layer, was somehow five milliseconds longer. And that was when I needed to click on them and figure out how big they were. on the left side for train.py, there were just fewer things getting all gathered. Like it was just not the same thing. And I was like, why is it not the same thing? <br>[40:51] Jane: So I did a lot of printing and I hacked around FSDB2. And what that yielded was me writing down the sizes of every tensor that got packed up in the all gather and realizing that the difference was because in our NF4 metadata, whereâ€¦ Answer.ai did not pack their scalars and quantization factor. They just like for bits and bytes when they did their four bit thing, they use a dictionary. They have their own method. <br>[41:19] Jane: And this is actually the reason they couldnâ€™t pack it is because FSDP1 had restrictions, by the way, like it just wouldnâ€™t do it for them. So they needed to work around that. So that was one piece of the puzzle. where we just packed everything in one go. But in the other bigger piece of the puzzle, the big, big difference, like that was just like 1 million bytes, whatever. It doesnâ€™t matter. But the other thing was like so many more. It was like 12 million bytes. <br>[41:45] Jane: And that was when we realized that when we opted out of LoRa, the output projection did not get quantized in our world. So it was like four times the size of Answer.aiâ€™s version. And I was like, Why donâ€™t we do that? And then I talked to the TorchTune team and theyâ€™re like, oh yeah, we should do that. And so we should do that. Thatâ€™s the conclusion. We should do that. The first one is intended behavior. So we donâ€™t really need to worry about that. But the second one, we should do it. <br>[42:13] Jane: And we will hit you up when that happens. So this gap I would mark as yellow. Like we know what to do. Itâ€™s solved. Okay, third gap is realizing that there were just like more overhead. And remember what Mark was saying how when you have NF4, you have to you canâ€™t just like put that through your gem kernel like CUDA is going to complain, you need to de quantize get it to the BF 16. And then put it through the CUDA kernel. <br>[42:39] Jane: And it turns out that Tim Detmers, you know, brilliant guy already wrote a really fast version of that, whereas we just get stuck with the little raggedy version that tries every op. So thatâ€™s also where we are slower just because of additional overhead. But again, this is not a major problem. Solutions. We could use Torch Compile. I tried it. It was not trivial, so I was like, I will look into this later when I have a month of time. And then, or when I donâ€™t have a presentation on Monday. <br>[43:05] Jane: And then the second step is to just use Triton kernels. So Driss, our coworker, already wrote them. I didnâ€™t want to copy paste them for the sake of this presentation. But if you want to copy paste them, go for it. No oneâ€™s judging. Theyâ€™re good. They work. And so weâ€™re like, okay, we also know how to fix this one. The third one was really stupid. This one is definitely the worst gap, where basically there were just very different ops happening before the SDPA. And this was just because we used two different ropes. algorithms. <br>[43:41] Jane: TorchTune was like, we are traditional. We are going to use the original meta algorithm. We work there. So there will be no numerical differences. And everybody else in the world is like, oh, but we want it to be faster. So itâ€™s fine. And then the real solution here is just for TorchTune to offer more of these. And thatâ€™s also in progress. So, yeah, but okay. Letâ€™s talk about the most fun one. The most fun gap I noticed is I realize this is a little hard to read. <br>[44:07] Jane: But on the left side, note that there are two things happening. The first, this green thing here, is the CPU offload where you go from CPU to GPU. And the second one is when you after youâ€™ve moved it to the GPUs, you like have them talk to each other. And here in train.py, in FSTP1, weâ€™re like, wait, how come this is overlapped? Like, you see how this is, thereâ€™s no gap here. But this one is so badly overlapped. Look at that. Look at that exposition. Itâ€™s bad. <br>[44:36] Jane: And this was really frustrating because this was costing us like 10 milliseconds each step. And I was like, I wonder why this is. But actually, this is very much expected behavior. And this is part of the bigger memory constraints that FSTP2 promises you. So, FSTP2 is promising that, hey, weâ€™re not only looking at all gathers, weâ€™re also going to make sure that before we bring CPUs to GPUs that you have the space you have. So it is guaranteeing the constraint that only two layers of parameters will be allowed at a time. <br>[45:12] Jane: And that is why on the left, because of how FSTP1 was implemented, it didnâ€™t do that very well. So youâ€™d get random memory spikes. And FSDP2, youâ€™re promised to never get that. But someone looking at this trace will be like, but this is kind of, what if Iâ€™m okay with non-deterministic memory? Like itâ€™s not happening now. Like maybe I can just go on with my life. But no, no, no, we have an answer for you. And the answer is the reason itâ€™s so exposed is not because FSDP is too strict. Thatâ€™s not the problem. <br>[45:41] Jane: The problem is that the overlap of computation and communication was too different. The computation here is super duper tiny because it corresponds to this tiny linear here. And then here when youâ€™re CPU offloading, youâ€™re actually trying to bring this big boy back in. So itâ€™s like the mismatch in the layer sizes was really causing this problem. So what could we do? Well, itâ€™s fine. Weâ€™ll just change the wrapping policy, have bigger layers. And itâ€™s really just, hey, donâ€™t wrap those linears by themselves. <br>[46:16] Jane: Just group them in so we can just have every transformer decoder layer be its own layer. And note that this is only possible with FSDP2. You canâ€™t have the right-hand side in FSDP1. Why? Because the lower linears have the base weights. The base weights are quantized. Theyâ€™re NF4 tensors. Theyâ€™re going to be small. Theyâ€™re UN8. Whereas your linear tensors, those are BF16 or whatever training thing you want. And in FSDB1, they canâ€™t be brought together under the same family because theyâ€™re going to be forced into one big flat parameter. But in FSDB2, they can coexist. <br>[46:55] Jane: And the change is actually really easy to do. The policy is just a matter of commenting out these two lines here. And once we do that, the solution is like theyâ€™re both overlapped. Itâ€™s kind of crazy. Look, like thereâ€™s no more exposition at all, where even in the first case, even before here in the train.py one, this all gather was exposed, also due to the same reason. Thatâ€™s not even true at all here. And this wrapping policy, this new one, is only possible because of fsdp2, which is a great point here. All right. <br>[47:32] Jane: So now weâ€™ve fought all our gaps and things work. So itâ€™s your turn. You should try them out. You should really, like, it doesnâ€™t have to be NF4. If you have another favorite lower precision thing, you can try it. If you want to try out more Lama stuff with it, you can. There are now scripts ready to do that. So yeah. One disclaimer, though, we are working on this. This is not a forever thing. Pointing. of just Fsdp and Qlora does not work yet. So thatâ€™s just FYI, weâ€™re working on it. <br>[48:06] Jane: Sorry, you canâ€™t do that. But you can try, you can just like try and play with it, among other things. So yeah, here I would love to, I mean, Mark and I are speaking here, but this was really made possible by all these amazing people. So Driss wrote the original NF4 stuff. Andrew is the main dude who designed FSDP2. Wei ended up taking Andrewâ€™s work and making it compose withâ€¦ Drissâ€™s work, so he like amalgamated both of them. And then Torch Tomb people, so like Rohan and Evan, they were super helpful. <br>[48:40] Jane: They wrote the Laura recipes and theyâ€™re the foundation of which we built and showcased our work. And of course, Mark, whoâ€™s amazing. So thanks, Mark. <br>[48:52] Mark: Yeah, so I really hope this gives people some more context around what weâ€™re thinking here. We did want to showcase a useful recipe with Qlora and FSTP composition. But really, this is kind of like our call to action here would really be if youâ€™re doing interesting research at the intersection of quantization and distributed, weâ€™d really, really love to hear from you. So if youâ€™re working with more exotic D types or more exotic forms of parallelism, a lot of this work should really, really be helpful. <br>[49:21] Mark: And we have all these links here that can give you some more context. I guess weâ€™ll pause here if people have any questions. Otherwise, weâ€™ll probably be hanging out on Discord for a couple more hours if people have any questions.</p>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/parlance-labs\.com\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../education/fine_tuning/napkin_math.html" class="pagination-link" aria-label="Napkin Math For Fine Tuning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Napkin Math For Fine Tuning</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../education/fine_tuning/pawel.html" class="pagination-link" aria-label="Fine Tuning LLMs for Function Calling">
        <span class="nav-page-text">Fine Tuning LLMs for Function Calling</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/HamelHusain">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hamelsmu">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/parlance-labs/website/edit/main/education/fine_tuning/slaying_ooms.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>