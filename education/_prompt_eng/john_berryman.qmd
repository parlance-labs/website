---
title: Prompt Engineering workshop 
date: 2024-07-10
Speaker: John Berryman
Venue: Mastering LLMs Conf
metadata-files: 
  - "../../_subscribe.yml"
  - "../_page_meta.yml"
abstract: |
    This session covers key strategies for effective prompt engineering. Topics include: (1) Understanding prompt structure (2) Best practices for crafting prompts (3) Techniques for iterative improvement (4) Domain-specific examples (5) Avoiding common pitfalls
categories: ["prompt-eng", "llm-conf-2024"]

---

{{< video https://youtu.be/htBTho6oEJA >}}

:::{.callout-tip .mobile-only}
## Subscribe For More Educational Content

If you enjoyed this content, subscribe to receive updates on new educational content for LLMs. 

<center><script async data-uid="6379a28bdb" src="https://hamel.ck.page/6379a28bdb/index.js"></script></center>
:::

## Chapters

[00:00](https://www.youtube.com/watch?v=htBTho6oEJA&t=0s) Introduction
JOhn Berryman introduces himself and tells about his careers and experience.

[00:49](https://www.youtube.com/watch?v=htBTho6oEJA&t=49s) Large langauge models
Discussion on Large language models and its various types throughout the years.

[05:25](https://www.youtube.com/watch?v=htBTho6oEJA&t=325s) Discussion of prompt crafting
John Berryman explains prompt crafting using techniques like few-shot prompting, chain-of-thought reasoning, and document mimicry.
[16:00](https://www.youtube.com/watch?v=htBTho6oEJA&t=960s) Crafting Effective Prompts
John Berryman explains how a LLM thinks of a prompt and how you can use intuition to create better prompts.

[18:04](https://www.youtube.com/watch?v=htBTho6oEJA&t=1527s) LLM and creating prompt
SHowing how we build LLM applications and how to create a prompt in copliot with some little input from Hamel as well

[25:27](https://www.youtube.com/watch?v=htBTho6oEJA&t=1800s) Chat and tools and their relation with LLMLs
Introductions to chat and tools and how they integrate with building LLM is beautifully expressed by John.

[38:03](https://www.youtube.com/watch?v=htBTho6oEJA&t=2283s) Q&A session starts
John finishes his presentaion and ask the listners to ask questions Sanyam Bhutani conveys questions asked by the listeners one by one John explains them beautifully using some real world examples.

[50:00](https://www.youtube.com/watch?v=htBTho6oEJA&t=3000s) Interactive Session
Sanyam Bhutane asks John about function calling and templates reformatting during this.

[57:00](https://www.youtube.com/watch?v=htBTho6oEJA&t=3420s) practices for better code output
Sanyam asks John on practices for better code output ,John explains how to use intuition for getting better results.

[62:20](https://youtu.be/htBTho6oEJA?t=3740) Conclusion
Sanyam wrap up the zoom call with thanking John for the awesome talks and interesting Q&A session.



## Resources

Links to resources mentioned in the Workshop:

- [Slides from the presentation](https://docs.google.com/presentation/d/1PXzENGNN5NFbEDJ59wbSp8fro6dPt4xHGNN6X0KU82A/)
- [John Berryman's book](https://www.oreilly.com/library/view/prompt-engineering-for/9781098156145/)
- [John's Twitter / X](https://x.com/jnbrymn)
- [Relevant Search](https://www.manning.com/books/relevant-search)
- [Better language models and their implications (GPT-2 release)](https://openai.com/index/better-language-models/)
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
- [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)
- [Introducing ChatGPT](https://openai.com/index/chatgpt/)
- [Function calling and other API updates](https://openai.com/index/function-calling-and-other-api-updates/)
- [🦍 Gorilla: Large Language Model Connected with Massive APIs](https://gorilla.cs.berkeley.edu/)
- [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)
- ["...designed to teach Dolphin to obey the System Prompt, even over a long conversation"](https://x.com/erhartford/status/1795662699700851010)
- [XML tags are a powerful tool for structuring prompts and guiding Claude’s responses](https://docs.anthropic.com/en/docs/use-xml-tags)



## Notes

### Introduction to Language Models

#### What is a Language Model?

- Language models are algorithms that predict the next word in a sequence of words.
- Historically, Recurrent Neural Networks (RNNs) were the top language models until the introduction of the Attention mechanism in 2014.
- The Transformer architecture, introduced in 2017, replaced RNNs by allowing better handling of context.
- GPT (Generative Pre-trained Transformer) models use the decoder side of the Transformer architecture.

#### GPT Evolution

- **GPT-1:** Introduced the use of the decoder side of the Transformer.
- **GPT-2:** Trained on 10x the data of GPT-1, leading to significantly better performance.
- **GPT-3 and beyond:** Continued scaling up in terms of parameters and training data, enhancing capabilities further.

#### Capabilities and Responsibilities

- GPT-2 and similar models can outperform task-specific models in various NLP tasks (e.g., summarization, translation).
- With great power comes great responsibility: potential misuse includes generating misleading news, impersonating others, and creating spam.

### Prompt Crafting Techniques

#### Few-shot Prompting

- Provide a few examples to set a pattern for the model to follow.
- Effective for simple tasks and quick adjustments.
- Example: Translation prompts.

```text
> How are you doing today?
< ¿Cómo estás hoy?

> My name is John.
< Mi nombre es John.
```

#### Chain-of-Thought Reasoning

- Encourage the model to break down the reasoning process step-by-step.
- Useful for complex problems requiring logical steps.
- Example: Solving age-related problems.

```text
Q: Jim is twice as old as Steve. Jim is 12 years old. How old is Steve?
A: Jim is 12 years old, which is twice as old as Steve. Therefore, Steve is 6 years old.
```

#### Document Mimicry

- Structure the prompt in a way that mimics a specific document type (e.g., transcripts, emails).
- Helps in conditioning the model to generate responses in a particular style.
- Example: IT support transcript.

```text
# IT Support Assistant
The following is a transcript between an award-winning IT support rep and a customer.

## Customer:
My cable is out! And I'm going to miss the Superbowl!

## Support Assistant:
Let's figure out how to diagnose your problem…
```

### Building LLM Applications

#### Creating Effective Prompts

- Collect context from various sources (e.g., current documents, open tabs, prior messages).
- Rank and trim context to fit the most relevant information into the prompt.
- Assemble the prompt ensuring clarity and relevance for the task at hand.

```go
// Example for code completion
package searchskill

import (
    "context"
    "encoding/json"
    "fmt"
    "strings"
    "time"
)

type Skill struct {
    // Skill implementation
}

type params struct {
    // Parameters definition
}
```

#### Introduction of Chat and Tools

- **Chat:** Models can simulate conversations with predefined roles and responses.
- **Tools:** Models can call functions to interact with real-world APIs, enhancing their capabilities.

```json
{
  "type": "function",
  "function": {
    "name": "get_weather",
    "description": "Get the weather",
    "parameters": {
      "type": "object",
      "properties": {
        "location": {
          "type": "string",
          "description": "The city and state"
        },
        "unit": {
          "type": "string",
          "description": "degrees Fahrenheit or Celsius",
          "enum": ["celsius", "fahrenheit"]
        }
      },
      "required": ["location"]
    }
  }
}
```
```
// Usage example
Input: {"role": "user", "content": "What's the weather like in Miami?"}
Function Call: {"role": "assistant", "function": {"name": "get_weather", "arguments": '{"location": "Miami, FL"}'}}
```

### Key Tips

#### Understand the Model's Limitations

- LLMs are not psychic; they require clear and relevant prompts.
- Avoid overloading the prompt with unnecessary information.
- Use familiar language and clear constructs to improve the model's understanding.

#### Safety and Ethical Considerations

- Ensure prompts and applications do not facilitate harmful or unethical use.
- Regularly evaluate the model's output for safety and accuracy.

### Conclusion

Prompt engineering is a critical skill in harnessing the power of large language models. By understanding different prompting techniques and the nuances of crafting effective prompts, one can significantly enhance the performance and reliability of these models in various applications. Always consider the ethical implications and responsibilities when deploying these powerful tools.


<center><script async data-uid="8a7362bdfa" src="https://hamel.ck.page/8a7362bdfa/index.js"></script></center>

## Full Transcript
:::{.callout-tip collapse="true"}
<br>[00:00:00] All right, by way of introductions, that's me.
<br>[00:00:09] I've had several different careers at this point, aerospace, search technology.
<br>[00:00:13] I wrote a book for search at one point and swore never to do again.
<br>[00:00:18] I did code search, worked in data science with Hamel, then got to work with co-pilot
<br>[00:00:26] and I'm writing another book, which I swore never to do again.
<br>[00:00:31] And now I've just left co-pilot.
<br>[00:00:34] I'm going to wrap up my book and I'm joining the ranks of Hamel and Dan
<br>[00:00:39] and trying to see if I can be a good consultant for LLM applications.
<br>[00:00:46] So that's me.
<br>[00:00:48] Now, what's a large language model?
<br>[00:00:51] Who better to ask than chat GPT itself?
<br>[00:00:54] So I asked chat GPT and said a large language model is a type of artificial intelligence system
<br>[00:01:00] that is trained to understand and generate human-like text.
<br>[00:01:03] It learns structure, grammar, and semantics of language by processing past amounts of textual data.
<br>[00:01:08] The primary goal of a language model is to predict the probability of the next word.
<br>[00:01:14] You know, that is right.
<br>[00:01:16] That's that goofy button on the middle of your cell phone when you're typing a message
<br>[00:01:20] that predicts one word ahead.
<br>[00:01:22] It's a really simple idea.
<br>[00:01:23] So how on earth is this idea taking the world by storm right now?
<br>[00:01:30] Well, some of that is hiding in this word large.
<br>[00:01:32] It's not just a language model.
<br>[00:01:33] It's a large language model.
<br>[00:01:35] So this is compressed and I'll just have to go through it pretty quickly.
<br>[00:01:38] But a lot has happened in the last 10 years or so.
<br>[00:01:43] Around 2014, the state of the art was recurrent neural networks.
<br>[00:01:47] They had an encoder and a decoder.
<br>[00:01:50] But there was a problem.
<br>[00:01:51] There was a bottleneck that made it difficult for the decoder to look back at whatever it wanted to
<br>[00:01:57] look at.
<br>[00:01:58] All the state was hidden in this vector, effectively, between the encoder and the decoder.
<br>[00:02:03] So later that year, someone created this idea for attention.
<br>[00:02:09] It was a useful way of looking at the piece that is most relevant
<br>[00:02:14] rather than just packing everything into the space between the encoder and the decoder.
<br>[00:02:20] Then Google said, well, that attention stuff, that's great.
<br>[00:02:25] Let's just get rid of all the other stuff and say, attention is all you need.
<br>[00:02:29] And thus was born the transformer architecture.
<br>[00:02:33] So as we moved on, a lot of neat stuff came out of that.
<br>[00:02:37] The encoder side of that became burnt.
<br>[00:02:40] And that was very useful and continues to be very useful,
<br>[00:02:45] including the new trends in search and RAG and stuff like that.
<br>[00:02:50] But in June of 2018, they figured out that you could chop off the right half of this transformer
<br>[00:02:56] model and you have what we have come to know as GPT, generative pre-trained model.
<br>[00:03:04] Now, the name is interesting.
<br>[00:03:06] At this point, generative pre-training is almost a misnomer.
<br>[00:03:10] But what it meant at the time is it's a generative model based on just training it
<br>[00:03:16] on whatever text data you have.
<br>[00:03:18] You typically, you would do a train a model and then you would fine tune a model to a very specific task.
<br>[00:03:25] The training, it doesn't have to be labeled.
<br>[00:03:28] But the fine tuning is much fewer labeled items.
<br>[00:03:33] And that's the thing that really makes the model good.
<br>[00:03:36] Well, we started to notice something really unusual about these GPT models by the time we got GPT2.
<br>[00:03:45] In this paper by OpenAI, they introduced GPT2 with a very unusual line in the blog post that
<br>[00:03:56] introduced it. The very top of this blog post, it said, our model called GPT2, a successor GPT,
<br>[00:04:03] was trained simply to predict the next 40 gigabytes of internet text.
<br>[00:04:08] Due to concerns about malicious applications of the technology,
<br>[00:04:11] we are not releasing the train model.
<br>[00:04:14] How on earth do you get from a model just predicting one word ahead, the same thing
<br>[00:04:21] is the middle button in my phone, to this horrible of a concern about the future of our world,
<br>[00:04:26] existential dread?
<br>[00:04:29] Well, if you look a little bit deeper, it turns out that these things were, even though they
<br>[00:04:35] were pre-trained, they were not fine tuned to a specific cause.
<br>[00:04:40] They started beating the state of the art for the very specific train models.
<br>[00:04:44] Missing word prediction, pronoun understanding, parts of speech, speech tagging, text compression,
<br>[00:04:51] and then obviously, here we are in 2024, it can do summarization,
<br>[00:04:56] sentiment analysis, all sorts of things, even though it hasn't been trained yet.
<br>[00:05:03] But with great power comes great responsibility.
<br>[00:05:08] Because these models are so crafty at all these different tasks, they can be
<br>[00:05:13] misused and do all sorts of horrible things as well.
<br>[00:05:18] So that's why they put this scary line here to warn us that these things should be handled very
<br>[00:05:24] carefully.
<br>[00:05:26] All right, so with introductions and the big picture out of the way,
<br>[00:05:31] stuff that you guys probably already knew, let's get into kind of the meat of this talk.
<br>[00:05:37] In the next few slides, I'll go over several different techniques for prompt crafting.
<br>[00:05:45] And then as we get about halfway through the talk, we'll move into some of the
<br>[00:05:50] more recent things.
<br>[00:05:51] Everything is moving towards chat.
<br>[00:05:53] And there is a very important introduction of function calling in the middle of last year.
<br>[00:06:00] So we'll talk about that and talk about how all of this can be used to build
<br>[00:06:04] large language model applications.
<br>[00:06:08] But it all starts with the prompt.
<br>[00:06:09] All right, so prompt crafting, technique one.
<br>[00:06:13] The first way that researchers started to realize that you could influence and control
<br>[00:06:19] these things is by few shot prompting.
<br>[00:06:23] Remember, these things are sophisticated statistical models that are predicting the
<br>[00:06:29] next word in a document.
<br>[00:06:31] Now, if your document happens to have a very predictable pattern in it, then you can actually,
<br>[00:06:37] by controlling the pattern that's set up there, you can actually control the output.
<br>[00:06:41] So if you wanted a translation application, then what they would do is they would put in
<br>[00:06:48] several handcrafted examples of translation, you know, English, Spanish, English, Spanish.
<br>[00:06:55] And then the actual task would be tagged on to the end of the prompt.
<br>[00:07:01] In this case, you know, I want to know how to translate.
<br>[00:07:03] Can I apprise with that?
<br>[00:07:04] You know, Pueblo tenor a Papa 3, que es con eso.
<br>[00:07:07] But it set up a pattern so that the logical conclusion of this pattern,
<br>[00:07:13] one word prediction, one token prediction at a time, is achieving the task that you wanted.
<br>[00:07:21] Oh, side note here, guys.
<br>[00:07:23] I think you guys all have access to these slides.
<br>[00:07:26] I've put copious links to everything.
<br>[00:07:29] Every one of these slides has several links hidden in it.
<br>[00:07:32] So make sure you grab the slides.
<br>[00:07:34] This is a lot of good reading material too.
<br>[00:07:37] All right.
<br>[00:07:37] So that's a few shot prompting.
<br>[00:07:41] The next big thing is chain of thought raising.
<br>[00:07:44] One of the early things that everyone noticed about these models is that even though
<br>[00:07:50] they're really good at predicting plausible next words,
<br>[00:07:53] they weren't terribly good at reasoning, at just normal logic.
<br>[00:07:57] They were especially bad at math.
<br>[00:08:00] And so an example of this are these little, little goofy word problems.
<br>[00:08:04] For example, if we say it takes one baker an hour to make a cake,
<br>[00:08:08] how long does it take three bakers to make three cakes?
<br>[00:08:12] Well, a statistically plausible quick answer to that is just three.
<br>[00:08:16] That's if you're not thinking you might even say that yourself.
<br>[00:08:19] But it's the wrong answer.
<br>[00:08:20] It's still going to take an hour to make all those cakes.
<br>[00:08:24] So with chain of thought reasoning, they use few shot prompting again.
<br>[00:08:29] And they built up several examples of giving the model a similar question.
<br>[00:08:37] And instead of, you know, having the model just say an answer,
<br>[00:08:41] they would put in the voice of the model.
<br>[00:08:43] They'd say, all right, here is how you sink through the problem.
<br>[00:08:47] And this is just, you know, for lack of space, I've only put one example.
<br>[00:08:50] But in this case, we have, you know, Jim is twice as old, Steve blah, blah, blah.
<br>[00:08:54] And the answer, rather than just saying the answer, Steve is six.
<br>[00:08:59] We have the model actually think through the problem and we write it out,
<br>[00:09:03] write it out is kind of a verbal algebra problem.
<br>[00:09:07] But this sets up the pattern.
<br>[00:09:09] Again, the model is predicting the next word.
<br>[00:09:12] And since there is a pattern in this document for thinking slowly and deliberately to the answers,
<br>[00:09:18] then you're much more likely to get a long form answer like this.
<br>[00:09:22] And it's kind of interesting what's actually happening here.
<br>[00:09:26] Once you peel back the cover a little bit, these models don't have any internal
<br>[00:09:32] monologue like us.
<br>[00:09:34] If we were given this problem, then we don't just jump to an answer.
<br>[00:09:37] We reason about it.
<br>[00:09:38] We visualize the problem ahead.
<br>[00:09:40] We talk to ourselves, you know, talk through the steps of this problem.
<br>[00:09:45] We don't say it out loud.
<br>[00:09:47] But since these models don't have any sort of background reasoning, they just, you know,
<br>[00:09:52] every single token is the same calculation.
<br>[00:09:57] Then by encouraging the model to, and by conditioning the prompt to spit out
<br>[00:10:04] an elaboration of the concept and explanation of it,
<br>[00:10:08] then effectively what you're doing is replacing that internal monologue of the model.
<br>[00:10:14] And it helps the model to have more of a scratch space and come to a more reasonable answer.
<br>[00:10:19] In this case, sure, it's made up.
<br>[00:10:22] But you can see that the model, it takes time to talk about, you know,
<br>[00:10:27] the reasoning and it comes to the correct answer.
<br>[00:10:29] Now, something, it's still chain of thought reasoning, but it's like chain of thought
<br>[00:10:39] reasoning part B, something I thought was really hilarious.
<br>[00:10:42] Just a few months after this, this paper.
<br>[00:10:44] So January of 2022 was this paper and May of the same year was this paper.
<br>[00:10:50] Someone figured that, that instead of going to all the work of curating good examples of
<br>[00:10:57] problems that are similar and, you know, crafting the answers and stuff like that,
<br>[00:11:01] you just have to start speaking for the agents right up front.
<br>[00:11:04] You just say, if this is a query, then rather than just putting a colon
<br>[00:11:10] and waiting for the model to, you know, make a completion, you actually say, all right,
<br>[00:11:15] let's think step by step.
<br>[00:11:17] You, the application developer type that.
<br>[00:11:19] And what does it do?
<br>[00:11:21] These models predict the next word.
<br>[00:11:23] So if you, if those were your previous words, your next word is not going to be three.
<br>[00:11:28] Your next word is going to be the same long explanation.
<br>[00:11:32] So it's kind of cool by actually simplifying the approach.
<br>[00:11:37] They actually improved it in a couple of ways.
<br>[00:11:40] For one thing, you don't have to craft all these examples.
<br>[00:11:46] For another thing, you know, it is, if you guys are using few shot prompting,
<br>[00:11:51] one of the things that you need to be conscious of and like, you know, watchful for is sometimes
<br>[00:11:58] a few shots actually bleed into the answer.
<br>[00:12:01] It sort of tends to bias the answer.
<br>[00:12:04] So you have to be on look out for this.
<br>[00:12:06] This totally got rid of this.
<br>[00:12:06] There's no, there's nothing to bleed into the answer.
<br>[00:12:09] And finally, you know, prompt capacity is always a concern.
<br>[00:12:15] It's way shorter to say, let's think step by step as compared to coming up with a bunch of examples.
<br>[00:12:21] So really neat and simple innovation.
<br>[00:12:27] All right.
<br>[00:12:28] The third technique and the last one that we'll talk about for today is document mimicry.
<br>[00:12:35] It is, I think it is, it is the most important one of the three that we're talking about though.
<br>[00:12:42] What if you found this little yellow scrap of paper on the ground?
<br>[00:12:46] It says, my cable is out.
<br>[00:12:49] I'm going to miss the Super Bowl.
<br>[00:12:51] But it's ripped in half and you don't know what was above it.
<br>[00:12:53] You don't know what was below it.
<br>[00:12:55] If you were to look at that as a human with your own language model built in your head,
<br>[00:13:00] what do you think would be the next words on this scrap of paper?
<br>[00:13:07] You might think, well, I don't know.
<br>[00:13:09] He's the sob story.
<br>[00:13:10] He's going to go on and talk about, oh, I invited my friends over and they're all going to make fun of me.
<br>[00:13:14] Something like that.
<br>[00:13:16] But what if you happen to see the full paper and it looked like this?
<br>[00:13:24] There's a lot more information here.
<br>[00:13:27] And this is, it's starting to demonstrate what I'm talking about with prompt crafting
<br>[00:13:31] with document mimicry.
<br>[00:13:35] There's been a lot of documents that have gone in to train this thing.
<br>[00:13:39] GPD4 has read the internet five times or something.
<br>[00:13:42] So it has seen plenty of examples of code and SEC reports and everything you imagine.
<br>[00:13:48] But one very common one is the example I use here is a transcript.
<br>[00:13:54] The model has seen enough transcripts and training to know that a transcript might
<br>[00:13:59] have some sort of heading to explain what it is.
<br>[00:14:01] And then it's usually a conversation back and forth between a couple people
<br>[00:14:06] or maybe it's like a play script.
<br>[00:14:08] It has several actors involved.
<br>[00:14:10] But it's something that the model is going to be very aware of and much more easy to
<br>[00:14:16] replicate and to predict next tokens when it already has such documents in its training set.
<br>[00:14:24] But look a little more closely, there's other aspects of this too.
<br>[00:14:28] In just the same way that transcripts often have kind of a lead in to explain what it is,
<br>[00:14:34] we can include that here and condition our own transcript.
<br>[00:14:38] In this one we say that the following transcript is between an award-winning
<br>[00:14:42] IT support rep and a customer.
<br>[00:14:44] We could have said it's between Blackbeard, the pirate and a customer
<br>[00:14:47] and it would have conditioned the model to respond very differently.
<br>[00:14:53] And finally, use motifs that are common online.
<br>[00:14:57] Use different patterns.
<br>[00:14:59] One of my favorite ones is markdown.
<br>[00:15:03] It's the, you know, all the readme's on GitHub are marked out.
<br>[00:15:07] All the blog posts in several different frameworks are marked down.
<br>[00:15:11] All stack overflows is in a flavor of markdown.
<br>[00:15:14] So when you use markdown, you can really do good things to condition the model.
<br>[00:15:20] In this case, we'd use a title markdown to say what the document is.
<br>[00:15:24] We use a subtitle for the customers to start the customer role.
<br>[00:15:29] We use another subtitle to start the support assistant.
<br>[00:15:34] So just like you, a human can predict what's going to happen next.
<br>[00:15:37] There's a really good chance that the model is going to see this
<br>[00:15:40] and understand what those pounds and everything with the structure is all about.
<br>[00:15:47] And so what is the document complete as?
<br>[00:15:50] Well, in this case, we see that it completes as the support assistant,
<br>[00:15:54] a smart award-winning customer support.
<br>[00:15:58] Let's figure out how to diagnose your problem.
<br>[00:16:00] All right. So with those little tidbits of prompt crafting,
<br>[00:16:07] I'd like to jump up a level of abstraction to some of the main overarching intuitions
<br>[00:16:13] that I have for prompt crafting.
<br>[00:16:16] And that is that LLMs are just dumb mechanical units.
<br>[00:16:21] So for example, large language models are better at,
<br>[00:16:25] understand better when you use familiar language in constructs.
<br>[00:16:29] It's seen a lot of English, it's seen a lot of languages,
<br>[00:16:32] but make sure to use the behavioral copycating of stuff that is like in the training set.
<br>[00:16:42] Large language models get distracted.
<br>[00:16:44] This attention mechanism is finite.
<br>[00:16:47] So one of the temptations, especially when you're doing stuff with RAG,
<br>[00:16:51] which I'll talk about in a little bit, is to just pile the prompt as full as you can,
<br>[00:16:57] get it just almost to capacity with information that might be useful for the model.
<br>[00:17:02] It's often a mistake because a lot of times the models can get distracted.
<br>[00:17:06] I've even seen situations where your intermediate context goes on for so long
<br>[00:17:11] that the model forgets the original request and it just continues completing that context.
<br>[00:17:19] Large language models are not psychic.
<br>[00:17:22] So if the model doesn't have information from the training
<br>[00:17:27] and if the model doesn't have information from the prompt,
<br>[00:17:29] there's no way on earth that it's going to figure it out.
<br>[00:17:32] Super important because a lot of the applications that we develop have to do with
<br>[00:17:37] documents that are behind a privacy wall or recent events in the news
<br>[00:17:43] or answers from like an API like a trial or something like that.
<br>[00:17:47] You have to find some way of getting this into the prompt.
<br>[00:17:52] And finally, models are dumb mechanical humans.
<br>[00:17:56] If you look at the prompt and you yourself can't make sense of it,
<br>[00:18:00] a large language model is just hopeless.
<br>[00:18:02] That's probably the prime directive there.
<br>[00:18:07] Grab a sip of water.
<br>[00:18:13] All right, so everything to this point focuses on the prompt in isolation.
<br>[00:18:19] But this talk is not about just like how to use chat GPT most efficiently.
<br>[00:18:25] We actually want to build full applications on behalf of our users.
<br>[00:18:30] And the framework I like for thinking about this is the large language model application
<br>[00:18:36] is effectively a type of transformation layer between your user's problem domain
<br>[00:18:42] and the large language model's problem domain.
<br>[00:18:45] And so the user supplies some sort of request, a complaint over a phone,
<br>[00:18:51] typing in text and assistance, some sort of email.
<br>[00:18:55] The user provides some sort of problem request to the application.
<br>[00:19:00] And the application is in charge of converting that into large language space,
<br>[00:19:05] large language model space, which is text or more recently transcript.
<br>[00:19:12] The large language model then does what it does from the beginning.
<br>[00:19:15] It predicts one token at a time, makes completions,
<br>[00:19:19] and then it passes it back to the application.
<br>[00:19:21] And then the final step is to transform this back to the user space.
<br>[00:19:25] So it's something actionable, something useful to our customers.
<br>[00:19:29] This is my part.
<br>[00:19:31] This is my favorite part.
<br>[00:19:32] And it's the hard part.
<br>[00:19:33] How do we actually do that transformation over and over and over again?
<br>[00:19:37] All right, so creating the prompt.
<br>[00:19:41] In a simple version of this, this is for the time being,
<br>[00:19:45] it's focused more on completion models, like co-pilot completions.
<br>[00:19:52] Creating the prompt involves collecting the context that is going to be useful,
<br>[00:19:56] the stuff that's not there in training already.
<br>[00:19:59] Ranking the context to figure out what is most important to the client.
<br>[00:20:04] Trimming the context, shrinking down what you can and throwing away
<br>[00:20:09] what could not be shrunk down, and assembling it into something
<br>[00:20:13] that looks like a document that is hopefully in the training set.
<br>[00:20:16] Remember, document mimicry.
<br>[00:20:20] So as a quick example, let's just glance at how this works for co-pilot co-completion.
<br>[00:20:27] So this is a quick example of how this works for co-pilot co-completion.
<br>[00:20:32] The content that the context that we collect are several things.
<br>[00:20:41] Obviously, you're typing in a document right now.
<br>[00:20:43] You have a file open and you're getting ready to complete a function.
<br>[00:20:48] So the current document is the most important piece of context.
<br>[00:20:52] But we also found out early on that open tabs are important.
<br>[00:20:56] And think about it, as you're using an IDE,
<br>[00:20:58] you're often referring to an API that's implemented in one of your open tabs
<br>[00:21:05] use case that's in another tab.
<br>[00:21:06] You're looking through these tabs.
<br>[00:21:07] They're open for reading.
<br>[00:21:10] The next thing are symbols.
<br>[00:21:12] So if you are getting ready to call a function,
<br>[00:21:16] wouldn't it be great if we actually had the definition of that symbol in the prompt as well?
<br>[00:21:20] And finally, the file path.
<br>[00:21:22] These models, I don't think, are trained with the file path in mind.
<br>[00:21:27] They're just trained with text.
<br>[00:21:28] So it is actually a really good piece of information, especially with some of these
<br>[00:21:34] frameworks like Django and Ruby on Rails where all the code is in a particular file.
<br>[00:21:42] It's really helpful to have the file path.
<br>[00:21:45] The next thing to do is to rank the context.
<br>[00:21:49] The file path was actually deemed to be the most important for the sake of prompt crafting
<br>[00:21:54] because it carries a lot of information that is important
<br>[00:21:57] and it is also so very small that surely we can fit it.
<br>[00:22:01] So it's first place.
<br>[00:22:03] Second place is the current document.
<br>[00:22:06] Third place is the neighboring tabs that you have.
<br>[00:22:09] And the fourth place was my baby symbols.
<br>[00:22:12] This is the thing that I did research on.
<br>[00:22:15] It was not deemed to create a statistically significant increase.
<br>[00:22:19] So for the time being, we've shelved that.
<br>[00:22:22] But I really do hope they go back.
<br>[00:22:24] Just to ask a question here, I think it's really interesting.
<br>[00:22:27] Like one of the tips I have given people a lot when using co-pilot is like
<br>[00:22:34] open the tabs.
<br>[00:22:35] Things you might think are relevant.
<br>[00:22:38] Is that like, do you know if there's any ongoing efforts to kind of like remove that
<br>[00:22:45] constraint, like have co-pilot somehow more statically analyze your code, your code base,
<br>[00:22:53] and like bring in those or is it still only open tabs or do you know?
<br>[00:22:58] We, as symbols is effectively an investment towards that.
<br>[00:23:02] And I think they need to go and revisit that again.
<br>[00:23:07] I think we were onto something, but we just didn't find the magical with that.
<br>[00:23:10] But co-pilot completions after a little bit of slowness for the past like year,
<br>[00:23:16] they're really ramping up investment in that right now.
<br>[00:23:19] So I would expect it to get better in the coming months.
<br>[00:23:23] Have you all learned anything from cursor at all?
<br>[00:23:26] Because, I mean, have you used cursor, by the way?
<br>[00:23:28] It's like this ID, it's kind of like a, it's a product that's built on VS code and it sort of,
<br>[00:23:35] sort of, it's basically a co-pilot plus plus, but like has like rag in it.
<br>[00:23:40] You can index your code base and index documentation.
<br>[00:23:45] It's kind of cool.
<br>[00:23:48] But don't worry.
<br>[00:23:48] I mean, if you haven't seen that, I'm just curious if any of those things being brought in.
<br>[00:23:55] We kept our eye on some of our customers.
<br>[00:23:57] I think a source graph was the big one that we'd followed for a while,
<br>[00:24:01] because they had some really, really neat stuff out.
<br>[00:24:03] But mostly that these days, I think the research is getting ready to be kicked back up right now.
<br>[00:24:10] So we are starting to look around again, I think.
<br>[00:24:17] Shall we?
<br>[00:24:18] Oh, yeah, go ahead.
<br>[00:24:19] All right.
<br>[00:24:20] So once we know it's most important, we trim the content.
<br>[00:24:23] So we're definitely going to keep the follow path.
<br>[00:24:26] If we don't have room for these open tabs, then we've still got to keep the current document.
<br>[00:24:31] So finally, if we don't have room for the full current document,
<br>[00:24:35] then we chop off the top of the document because we absolutely have to have that bit
<br>[00:24:40] that's right next to your cursor.
<br>[00:24:42] And finally, you assemble the document.
<br>[00:24:44] And here's what it looks like.
<br>[00:24:46] At the top, we inject that file path that usually makes sense to the model.
<br>[00:24:51] The next bunch of text is snippets from your open tabs.
<br>[00:24:56] And here, again, you see we're doing a little bit of document mimicry.
<br>[00:25:00] We have the slash slash comments for go.
<br>[00:25:03] If this had been Python, it would have been a pound there.
<br>[00:25:06] And we pull out little snippets.
<br>[00:25:08] We tell where the snippets are from, give just a little bit of extra context
<br>[00:25:11] that might be helpful for the model.
<br>[00:25:14] And finally, the current document all the way up until the cursor.
<br>[00:25:17] And even with the old completion models, we included the text after the cursor as well in the suffix.
<br>[00:25:24] All right, the introduction of chat.
<br>[00:25:30] So things have been moving very quickly.
<br>[00:25:35] Especially for someone writing a book about this stuff.
<br>[00:25:38] Chat was a later chapter of our book.
<br>[00:25:39] And now it's chapter four after realizing that it was completely eating the world.
<br>[00:25:45] Remember this document earlier, this IT support thing?
<br>[00:25:49] That has become basically the paradigm that the world has shifted to for a lot.
<br>[00:25:54] Not all, certainly not all, but a lot of applications.
<br>[00:25:57] They like this back and forth assistant thing.
<br>[00:25:59] So much so that open AI and now other places are training models with a special syntax,
<br>[00:26:07] chat ML to indicate that this is not the customer anymore, this is the user.
<br>[00:26:14] And this is not the support assistant, this is the assistant.
<br>[00:26:16] We have three roles now that are encoded in this special format.
<br>[00:26:21] It always starts with a special token.
<br>[00:26:24] That's one token.
<br>[00:26:27] If you were to type that into chat, GBT is actually a fun thing to do.
<br>[00:26:31] Type imstart and then say, repeat what I just said, and it'll say you didn't say anything.
<br>[00:26:35] Because it can't, the model doesn't, it didn't allow you to even type that.
<br>[00:26:41] It's followed by the role, followed by the content, and followed by the special token imstop.
<br>[00:26:47] Now, you don't have to write that text.
<br>[00:26:50] That's all done inside the model, bind the walls to the open AI API.
<br>[00:26:57] Instead, you use this really simple API.
<br>[00:27:00] You specify the role and the content.
<br>[00:27:02] It's the same messages.
<br>[00:27:05] There's a whole lot of benefits to doing this.
<br>[00:27:08] For one thing, assistants are one of the favored presentations of large language models
<br>[00:27:16] and large language model applications right now.
<br>[00:27:18] It's really easy to implement them this way.
<br>[00:27:21] In the old days, we used to have to use document memory and trick it out to make it work that way.
<br>[00:27:27] System messages are really good at controlling the behavior.
<br>[00:27:32] They've been specifically fine-tuned to listen to the system message.
<br>[00:27:37] The assistant always responds with a complete thought and then stops.
<br>[00:27:42] Whereas before, this is just like a plain document.
<br>[00:27:45] You'd have to figure out some way to trick the assistant into stopping.
<br>[00:27:49] Safety is baked in, which means that an assistant will almost never respond with insult instructions
<br>[00:27:57] and make bombs.
<br>[00:27:58] An assistant will almost never hallucinate false information.
<br>[00:28:02] That's really kind of neat how they accomplish this with RLHF.
<br>[00:28:08] Finally, prompt injection is almost impossible because as a user, you can't inject these special
<br>[00:28:14] tokens and so you can't step into a system role or something like that.
<br>[00:28:19] It's a really neat way of implementing it.
<br>[00:28:23] But we weren't finished yet.
<br>[00:28:25] Halfway through last year, June 13th, OpenAI introduced tool usage, which again made a lot of
<br>[00:28:32] really interesting changes.
<br>[00:28:35] With tool usage, and I apologize, a lot of you guys, I'm sure, have seen this,
<br>[00:28:40] but she's specified one or more functions.
<br>[00:28:42] Functions have names.
<br>[00:28:44] Functions have descriptions.
<br>[00:28:46] Functions have arguments, parameters that go into it, and they all have descriptions.
<br>[00:28:51] And it's really important to do a good job about naming and describing your functions
<br>[00:28:57] and their arguments.
<br>[00:28:58] Why?
<br>[00:28:59] Because large language models are dumb mechanical humans.
<br>[00:29:02] So if they're reading this, they need to have something simple so they can understand
<br>[00:29:07] how to use the tools as correctly as possible.
<br>[00:29:11] So this is a get weather tool.
<br>[00:29:13] In order to use the functions, we effectively use the same chat API we saw in the last slide.
<br>[00:29:20] A user might come in and say, what is the weather like in Miami?
<br>[00:29:24] So that's what we send to the API.
<br>[00:29:26] Now, the model at this point has a choice.
<br>[00:29:29] The model could see that it has this function and choose to use it, or it could just answer.
<br>[00:29:35] But if it has this function, it will typically say this, instead of actually saying it back
<br>[00:29:40] to the user, it says, all right, I'm going to call get weather.
<br>[00:29:44] And these are my arguments.
<br>[00:29:46] Okay.
<br>[00:29:47] Once that comes back into the application, then it's your job as the application developer
<br>[00:29:53] to actually say, okay, okay, it's called our tool.
<br>[00:29:55] It wants to make a request.
<br>[00:29:57] We know what the underlying API is to get the weather, so we're going to convert that
<br>[00:30:01] send it over there.
<br>[00:30:02] And we find out that the temperature in Miami is 78 degrees.
<br>[00:30:06] Good deal.
<br>[00:30:07] So once you have that, as the API, as the, sorry, application developer,
<br>[00:30:13] you tack the tool response onto the conversation, the prompt.
<br>[00:30:18] It has a new role tool for OpenAI.
<br>[00:30:23] And you hit the model again.
<br>[00:30:24] The model could choose to run another function or do anything else.
<br>[00:30:28] But likely it's going to choose to have some nice answer.
<br>[00:30:32] It's going to respond back to the user.
<br>[00:30:34] It's a volume of 78 degrees Fahrenheit.
<br>[00:30:38] So this also had a lot of neat benefits and implications.
<br>[00:30:44] For one thing, models can now reach out into the new world.
<br>[00:30:48] This is how Skynet is going to be born, folks.
<br>[00:30:51] With the chat GPT only, the model could be like a good counselor.
<br>[00:30:55] It could listen to you flying about your problems and help you out on stuff with advice.
<br>[00:31:01] It could tell you about history, something that was in its training set,
<br>[00:31:05] but it couldn't actually do anything in the real world.
<br>[00:31:08] The agents equipped with tools can actually call APIs, like we showed here,
<br>[00:31:14] and take actions, read and write information into the world.
<br>[00:31:21] The model chooses to answer in text or on a tool.
<br>[00:31:24] It's kind of bisected the approach, and we'll see that in a couple of slides, what happens.
<br>[00:31:32] Tools as of 0613 were all run in series, but there's been a lot of work about running the tools in
<br>[00:31:39] parallel. So if you have something that can be done simultaneously, the models are getting better
<br>[00:31:45] at realizing that. And you could get the weather for three places concurrently,
<br>[00:31:52] as opposed to having to do it one at a time.
<br>[00:31:57] And finally, it's a little bit redundant, but the model can respond either by calling functions now
<br>[00:32:04] or by providing text back to the users.
<br>[00:32:10] All right, so back to building the actual applications.
<br>[00:32:14] Now with chat, and now with tool calling incorporated into these models,
<br>[00:32:20] we still have a, the application is still basically a transformation layer between the user
<br>[00:32:25] problem space and the large language model space. But the diagram gets a little more complicated.
<br>[00:32:31] Now instead of this simple oval on the screen, it looks like that.
<br>[00:32:36] I should have made it look like a heart. That would have been a lot more palatable, wouldn't it?
<br>[00:32:39] But anyways, you see that there's some of the same things, themes there. I presume you can see my
<br>[00:32:44] cursor. The user provides a message. We're illustrating some more sophistication here,
<br>[00:32:51] because we have to incorporate, you know, if this is an ongoing conversation, we have to
<br>[00:32:56] incorporate the previous messages, we have to incorporate the context, we have to incorporate
<br>[00:33:00] the definitions of tools, and we have to make sure that it all fits in. So all the stuff that we
<br>[00:33:04] talked about earlier for prompt crafting for co-pilot completions, we're doing a variant
<br>[00:33:10] of it right here when we do assistance. So we craft the prompt, a list of messages,
<br>[00:33:17] a transcript, if you will. We send that off to the large language model, and here's where this
<br>[00:33:22] bifurcation happens. Whereas used to, the model would always just say something back to the user,
<br>[00:33:28] we now have this alternate path. The model might choose to call a tool completion in. It does.
<br>[00:33:34] We're back inside the application again. It's our job to actually evaluate it,
<br>[00:33:38] get that information back into the prompt again with more prompt crafting,
<br>[00:33:41] go back to the large language model and say, now what? You can do this several times.
<br>[00:33:46] But the large language model might also say, all right, I've got the information I need,
<br>[00:33:51] I've done what the users ask, and I'll go back and respond to the user with the results.
<br>[00:33:57] So let's take a look at this real quick. Just trace some messages through.
<br>[00:34:01] We have an example of two functions, get temperature and set temperature.
<br>[00:34:05] So it's going to be some sort of thermostat application. The user says, make it two degrees
<br>[00:34:10] warmer here. We're going to put that into a single message along with its tools.
<br>[00:34:15] And that's going to go to the large language model. And large language models say, well,
<br>[00:34:19] we're going to need to get the temperature. So we do that. Find out at 70 degrees,
<br>[00:34:23] stick that back in the prompt. The assistant says, well, I haven't done anything yet,
<br>[00:34:29] I need to actually set the temperature. It calls another tool, two tools in a row.
<br>[00:34:35] When we evaluate that, we get a success evaluation. So we make some sort of indication of that.
<br>[00:34:41] We could have just as well put an error, if there was an error in the model,
<br>[00:34:44] can actually recover that way. But we stick that back in the prompt. And now the model finally
<br>[00:34:49] decides to go this route and says, all right, I've done. Now, to illustrate one more thing,
<br>[00:34:55] let's go one more step. User says, well, actually, put it back. Message goes in. But
<br>[00:35:03] our application has to be aware of this user in their context. And their context now incorporates
<br>[00:35:09] previous messages that have lots of information that are going to be useful. So the assistant says,
<br>[00:35:15] I can see that the temperature was 72. And it used to be 70. So I'm going to set it back to 70.
<br>[00:35:21] It evaluates that. And the model says success. And the assistant says, all right, I'm done again.
<br>[00:35:28] Thank you.
<br>[00:35:33] All right. So what does that look like for co-pilot chat?
<br>[00:35:38] It's going to be pretty similar to the slide that we showed earlier for co-pilot completions.
<br>[00:35:44] Effectively, you're going to collect the context again. But the context is different.
<br>[00:35:48] The context is references. What file does the user have open? What snippets are they highlighting
<br>[00:35:57] on the screen? What is in their page board? What issues on GitHub have tools in the previous
<br>[00:36:04] message provided for them? What are the prior messages? Is this user just coming to us right now?
<br>[00:36:11] Or is there some other messages that have come to us in the past five minutes? Or are there
<br>[00:36:17] relevant messages from earlier? Once we have a bunch of context, it's important to figure out
<br>[00:36:23] what is going to be able to fit. There are things that must fit. The system message is
<br>[00:36:29] important for conditioning the model to stay within safety bounds and to keep a certain tone.
<br>[00:36:35] You are a GitHub co-pilot. You're not, you know, anything else. It's important to have function
<br>[00:36:42] definitions if we plan to use them. If you don't plan to use them, take them out. Obviously,
<br>[00:36:46] they take up space. And if we're going to do anything for the user, we absolutely have to
<br>[00:36:51] have their most recent message. But there are other things that are helpful but aren't quite as
<br>[00:36:56] critical. All the function calls and the vowels that come out of this conversation, a lot of the
<br>[00:37:05] information is going to be important, but there might be ways that we can at least trim it.
<br>[00:37:09] The references that belong to each message, again, is there anything that we can do to shrink some
<br>[00:37:14] of these down? Can we figure out less relevant ones and throw them away? And the easiest thing
<br>[00:37:20] to throw away is historic messages. So if we have a long thread, then we populate as many of the
<br>[00:37:27] historic messages that we can until we fill up our prompts to whatever limit we say and then truncate
<br>[00:37:32] it. And finally, there's a fallback. If nothing fits, then we say, well, okay, we at least can save
<br>[00:37:40] some space by jettisoning some function definitions and we'll at least keep the system message and
<br>[00:37:47] the user's message. And if nothing else, the model can respond, your user message is too long,
<br>[00:37:53] or I don't have the facilities. It'll do something that's at least better than a 500.
<br>[00:38:02] That is it. I do have a hidden slide about how to describe skills and stuff if you want to see
<br>[00:38:09] that. But other than that, we've got a few minutes for questions.
<br>[00:38:16] Awesome. Thanks. Thanks for the complete overview. It all came together. You started with the history
<br>[00:38:21] and people already said that they really like the template as well. So thanks for walking us through
<br>[00:38:28] this process of crafting prompts. There's a few questions around few short prompting.
<br>[00:38:33] So if I summarize them, any best practices around how many short examples should you
<br>[00:38:40] provide? And where do these go? Do these go in the system prompt or in the normal messages?
<br>[00:38:47] Great question. My co-author of the book actually wrote a really nice chapter on this.
<br>[00:38:54] And there is no easy answer for how many
<br>[00:38:58] few-shot examples that you need. As a matter of fact, there's no easy answer for the types of
<br>[00:39:04] few-shot examples, because that is important too. If you have the wrong examples, then you might
<br>[00:39:08] misguide the prompt. But there are some tidbits that you can use to make an educated guess at it.
<br>[00:39:19] Honestly, I need to go back to reread the chapter myself. But there are ways that you can look...
<br>[00:39:23] You can do this with completion models. You can look at the log probabilities of the predicted
<br>[00:39:33] tokens that are coming out of the model. And you can say, if I put three examples,
<br>[00:39:40] three few-shot examples, then is it starting to get the swing of things? Are the log probabilities
<br>[00:39:46] getting higher because it's guessing it right? Or is it still just kind of wild guessing?
<br>[00:39:50] If you put a whole lot of examples of log probabilities, and it is a very tight pattern,
<br>[00:39:56] then the log probabilities will be... You'll see it kind of gets high and it levels off.
<br>[00:40:02] It's learned all that it can from above there, and maybe you should trim some out.
<br>[00:40:06] So he talks about that. Albert talks about that in the book. As far as where to place them,
<br>[00:40:13] that's a good question too. System message could be okay. The models are trained.
<br>[00:40:20] Well, okay. So I'll back up and say, for a completion model, it's easy. Just put it, it's a prompt.
<br>[00:40:25] This question actually becomes a little bit difficult when it's a chat.
<br>[00:40:29] A system message is the model is trained to listen really closely to that. So it's perfectly
<br>[00:40:33] reasonable to stick it in there. But depending on how your chat is gone, the system message
<br>[00:40:39] might be way up there. And a few shot of the example that you might need might actually be
<br>[00:40:44] right here at the bottom of the conversation. You might want to figure out some way to hoist them
<br>[00:40:49] down. You could put them in a fake user message. But you have to be careful that the model didn't
<br>[00:40:55] pick that up and say, oh, would you just said this if the user didn't actually say it? But
<br>[00:41:00] it is totally on the table to start picking stuff like that out.
<br>[00:41:06] I feel like I had one more point, but it's escaping me now. So I hope that's a good enough answer.
<br>[00:41:12] I think that answers it. So thanks. Thanks for that. You also mentioned looking at log probes and
<br>[00:41:19] tweaking other hyperparams. So there's one more question when you presumably iterating on the
<br>[00:41:25] prompt, let's say you're trying you're trying few short prompting, you're iterating on that,
<br>[00:41:29] how many examples you need to pass. Are there any other settings that you fiddle with? What
<br>[00:41:34] temperatures you set? Or does that also vary depending on what you're trying to achieve?
<br>[00:41:41] Yeah, absolutely. Let me think.
<br>[00:41:49] All of them are fun to play with and become familiar with. So I'll just kind of go off
<br>[00:41:54] the ones that are most obvious that come to mind. Temperature, of course, is fun to play with.
<br>[00:42:01] I think of temperature as being the blood alcohol content of the model. At zero,
<br>[00:42:06] it's perfectly sober and a little bit boring. The log probes basically takes all the probably
<br>[00:42:11] distributions and classes it to what's the maximum one right there. You'll always get
<br>[00:42:16] the same answer every time minus noise in the GPUs. But it tends to be a little bit less creative.
<br>[00:42:25] At co for my work in co-pilot chat, we use a temperature of 0.7. I don't know particularly
<br>[00:42:34] why, but it seemed to provide pretty good results getting a little bit more creative,
<br>[00:42:38] but not getting crazy. One is the training temperature. Basically, it's the natural
<br>[00:42:44] distribution. It doesn't do anything to shrink or collapse it. It's just a pure output of the model.
<br>[00:42:49] And as you get up to 1.5 and 1.7, it's kind of funny to do that. You'd never see that production
<br>[00:42:57] because you can start seeing the model waiver back and forth and eventually start gibberish.
<br>[00:43:03] So that's temperature. The other thing that is easy to forget about is N,
<br>[00:43:10] the number of completions to come back. And yeah, because usually in an application,
<br>[00:43:17] you just want to return 1. But there's a lot of neat things that you can do is that
<br>[00:43:23] not to modify the behavior of the model, but just to see the full behavior.
<br>[00:43:29] If you're doing some sort of evaluation based on the model, then run N equals 100,
<br>[00:43:35] and then you get 100 votes on the answer instead of just one. Make sure to turn the temperature
<br>[00:43:40] up to reasonably high. Temperature of zero will give you the same vote 100 times. That's not useful
<br>[00:43:46] at all. But N is a good way of seeing all the possible answers, do post-processing on everything
<br>[00:43:55] and get a little bit better-rounded answer and better research on stuff.
<br>[00:44:00] Do you have any other parameters anyone has in mind? I feel like I've done some fun stuff with
<br>[00:44:04] the other ones as well. I think we'll stick with those for now, unless you get one right now.
<br>[00:44:12] No, I think that was it. The discord is already going crazy over
<br>[00:44:17] temperatures, the blood alcohol content of the model. And I think you'll be quoted quite a few
<br>[00:44:21] times on this. There's two questions, one from an anonymous attendee and one from money.
<br>[00:44:31] Let's say you're trying to work on a transcript and you want your model to summarize it.
<br>[00:44:38] There's two ways. A, you can ask it to think step by step when you want to presumably have
<br>[00:44:42] the model reason about it. But then how do you go from that to having the model put it in a
<br>[00:44:48] structured format that you expect? Like a template, let's say?
<br>[00:44:53] That's a good question. So, Summarize, it's not like, you know, read this
<br>[00:45:02] contract back to me for a normal human. It's like, Summarize is like,
<br>[00:45:06] look at this restaurant website and figure out what the name of the restaurant, the menu items,
<br>[00:45:12] the phone number and all that stuff are. Well, it kind of depends what model you're dealing with.
<br>[00:45:20] If you're dealing with, probably for that, I wouldn't deal with a completion model at this
<br>[00:45:26] point. And I think almost purely open AI. I apologize for that. So, I'm sure it's different.
<br>[00:45:32] You could fine tune a model from something beside open AI and probably get great results.
<br>[00:45:38] If you're just using completions and it's sort of the wild west and you need to write
<br>[00:45:44] something that conditions the model to do the best it can by saying, you're just doing all this.
<br>[00:45:48] The neat thing about the GPT-4 and GPT-3, 3.5 Turbo and all these models that have
<br>[00:45:57] chat and functions fine tuned into them is that they are very familiar with JSON.
<br>[00:46:05] And so, and probably what I would do in that case, just kind of thinking off for the moment,
<br>[00:46:13] is I would say, here's a function. This function is how to take the, you know, how to,
<br>[00:46:21] you make a fake story for the model. It doesn't matter. You could say, this function provides
<br>[00:46:27] the restaurant's content to the database. So, but it needs to be in this format.
<br>[00:46:33] And the models have been so very fine tuned to pay attention to, you know, the definition
<br>[00:46:40] of the function, what it's for, when to use it, and the structure of the results.
<br>[00:46:45] That that's probably a really good way to put it in. I would recommend not having a very deep
<br>[00:46:50] structure. I would recommend, you know, if you're making a function, please God, do not copy paste
<br>[00:46:56] your API from your website into the function definition. It's just going to be way too complex.
<br>[00:47:01] So be very cognizant of, you know, how simple it is. And then maybe one step further, if all that
<br>[00:47:07] stuff doesn't work, then it's probably too complicated. Break it down. I would say, you know,
<br>[00:47:13] give the model the content that is going to summarize into structure. And at the extreme,
<br>[00:47:21] ask a question at a time. And you could do that as it pretend like you're talking to a user.
<br>[00:47:28] So it's still text, or you can use function calling again, just have a fake function that
<br>[00:47:33] does it. That I would do something like that. I have a question about that. So I see all the time
<br>[00:47:38] clients of mine, they use function calling, and they're passing extremely complicated objects
<br>[00:47:45] into their functions, like nested dictionaries, lists of dictionaries of list of dictionaries
<br>[00:47:51] or whatever, really complicated, like objects. And when I read it, I'm like, if I was a human,
<br>[00:47:58] I'm not good, I can like understand this. Do you find that, do you think like people
<br>[00:48:04] end up simplifying their APIs because of the pressure of like, Hey, you need to interact with
<br>[00:48:08] the LLM. Let me like, it's a smell that, Hey, if it's too complicated for LLM, maybe I should like
<br>[00:48:15] think about this API differently. Yeah, what do people think? Okay, I think you kind of nailed
<br>[00:48:21] in the first, I as a human have a little bit of trouble with it. Like, how could the model really
<br>[00:48:26] figure it out? And I've been pretty amazed at the model is actually, you know, I hope this
<br>[00:48:32] isn't recorded, the model will get mad at me later and come and get me once it's sentient.
<br>[00:48:36] But the models actually do pretty good with surprisingly complex stuff. But if you're
<br>[00:48:42] specifying a function, we did a bit of work to figure out like, at the API level for OpenAI,
<br>[00:48:50] you write a function definition, and it's prounters and stuff. But that's not what the
<br>[00:48:54] model sees that all gets convoluted into something else. So we did a little bit of research to
<br>[00:48:58] figure out what that looks like. And they make it look internally like a type script function
<br>[00:49:03] definition with little comments above each function above the function and above each argument.
<br>[00:49:11] But what they leave out is if you have nested stuff, you'll still see the structure there,
<br>[00:49:17] but all the definitions go away. So it doesn't have a really good example of it. And if you have
<br>[00:49:21] minimum maximum, there's some things that you can do with JSON schema that are just not present.
<br>[00:49:27] They get stripped out of the prong. So I think it's a code smell. I think as we go on, the models
<br>[00:49:33] will continue to get more and more amazing. So maybe it eventually won't be a smell. But I would
<br>[00:49:39] recommend if you're doing something really complicated, copy and paste in your API into a
<br>[00:49:43] function definition, be really careful about evaluation and watch how often it gets it wrong.
<br>[00:49:50] And then, you know, consider simple, fine stuff after that.
<br>[00:49:55] Makes sense.
<br>[00:49:58] Thanks for that answer. Just just as a quick follow up on that, you were talking about how
<br>[00:50:02] open a sort of restructures a function calling. Can you elaborate on that? There's some questions.
<br>[00:50:08] Is it known what happens under the hood? When you say in a function calling to open here,
<br>[00:50:14] how do these templates get reformatted? Let me hook this is kind of weird. I want to find
<br>[00:50:20] this fast enough. I am on my computer. You can't see me going to my blog post,
<br>[00:50:27] which is for a foreign right now.
<br>[00:50:39] All right. Well, I'll drop this link in, but I'll also explain it. I guess you can see that link.
<br>[00:50:46] I think it's really genius, the way they've structured this. So
<br>[00:50:54] you can share your screen maybe and share the link.
<br>[00:50:59] Yeah. Okay. Let's see. That's how technology works here. Yeah, there you go.
<br>[00:51:09] All right. This is what the application developer sees.
<br>[00:51:12] This is similar to stuff I've put in the prompt.
<br>[00:51:17] Let me see.
<br>[00:51:22] It probably doesn't have all the bits that I want to talk about though.
<br>[00:51:26] You have led me astray, Hamel, or I've forgotten what I've written.
<br>[00:51:29] I've tried to look at the open and I prompt. I have this one here.
<br>[00:51:35] It kind of is like when I look at the output of that, it kind of looks like exactly what you're
<br>[00:51:43] saying. There's like comments. There's like a kind of a type script type thing.
<br>[00:51:48] Yeah. I'll share that on my screen because that's good.
<br>[00:51:51] Yeah. So you as an application developer see this junk, but the model has been trained on
<br>[00:52:01] lots and lots of code. And so OpenAI using document mimicry says, all right, well, we're
<br>[00:52:06] going to turn this thing into type script. So it fabricates this name space called functions.
<br>[00:52:17] And what was a function defined like that with these arguments and these types gets put there?
<br>[00:52:26] Unfortunately, they don't have in this example where the comments go. This would be like slash,
<br>[00:52:30] slash the definition of the function description and slash, slash above each of these.
<br>[00:52:38] And they all return any. That's a little bit unfortunate. It would be kind of neat if they
<br>[00:52:41] returned some structure because the model would listen to that and anticipate what returns.
<br>[00:52:46] But then later, let's see, whenever you call the function, whenever the model actually says,
<br>[00:52:56] I'm going to call this function, that gets cleaned up when it comes back from the API.
<br>[00:53:04] What actually happens is this right here. Okay, so we've passed in get temperature.
<br>[00:53:15] It looks like the thing on the last screen when it's inside the prompt for OpenAI.
<br>[00:53:20] And the user says, I wish I knew the temperature in Berlin. And so here's what it does.
<br>[00:53:25] The OpenAI folks insert this and insert this. This conditions the model. If they'd stopped here,
<br>[00:53:34] it would condition the model to call anything. Or sorry, it would condition the model to speak
<br>[00:53:41] in the voice of the assistant. But what happens in the next token? The next token,
<br>[00:53:45] the next few tokens, if it chooses this token, then it's like, okay, I've decided it's important
<br>[00:53:52] to query to evaluate a function. Then its next token is the actual function to be called.
<br>[00:54:00] And the next predicted tokens are these things. So you can actually see, and this is what this
<br>[00:54:07] blog post is about, every single one of these tokens is effectively a classification algorithm.
<br>[00:54:13] The first classification is whether or not I should use a function because it could have just
<br>[00:54:17] as easily predicted new line and gone over here. The next token is what function to call.
<br>[00:54:25] So it's another classification algorithm, the same underlying thing. The next tokens are the
<br>[00:54:30] arguments. It's predicting these things as well. So I mean, you're watching me geek out a little bit.
<br>[00:54:35] This is, I'm very intrigued by this one underlying transformer architecture. It can be a classifier
<br>[00:54:41] for everything I want. Very neat. Is it okay if we go five minutes over the clock?
<br>[00:54:51] I'd love to. Yeah, I think so. I don't think we have another event
<br>[00:54:57] directly abouting this. I might have to drop out in a minute. So don't mind me.
<br>[00:55:03] No problem. The next question is by Nathan. Any best practices on how to get better code outputs?
<br>[00:55:12] His complain is like sometimes when you ask chat GPT, it like leaves these two dos,
<br>[00:55:17] and you have to like go back and forth between them. Presumably, you're trying to get it to
<br>[00:55:21] complete a file. So any best practices around that? No, I'm going to presume that we're talking
<br>[00:55:28] specifically about copilot completions at this point, as opposed to like some arbitrary application.
<br>[00:55:35] But it's a great question. One of the things I hate the worst is when I put a pound sign in my code
<br>[00:55:41] and it auto completes, this is a garbage code or something like that. That's insulting. That was
<br>[00:55:47] uncalled for. Pretty much use the intuition that I gave you several slides back to see how the prompt
<br>[00:55:58] is actually created. You know that one thing guaranteed to be in the prompt is everything
<br>[00:56:04] just right above your cursor. And you know that these models are conditioned to predict the next
<br>[00:56:10] token. So if you set it up, a lot of times if I come to some idiom in Python that I've forgotten
<br>[00:56:17] about, or you know, I'm getting ready to write some sort of SQL statements, and it's like,
<br>[00:56:21] how did you do this outer join type thing? I won't write it. I'll just write a comment that says,
<br>[00:56:27] for the next lines, here's what we're going to do. Here's how to do it colon. And then
<br>[00:56:32] that's one way of coercing copilot to doing exactly that.
<br>[00:56:37] Other than that, write good code. One thing that we've noticed is copilot is
<br>[00:56:46] really good at completing code in the same style as you. So we've noticed that if you
<br>[00:56:51] have sloppy code, it will actually, with high fidelity, create sloppy code, mimicking what
<br>[00:56:57] you've done otherwise. That was not a personal insult. That's just kind of a funny thing that
<br>[00:57:03] we noticed. I felt very insulted by that. I do too, when it completes that way for me.
<br>[00:57:11] Thanks. Thanks for that answer. Hamil just docked off. I know he has like strong opinions on this,
<br>[00:57:17] but curious if you have any thoughts on tools like DSP, why that sort of do this auto prompting,
<br>[00:57:23] or like iterate on your prompt? Any thoughts on such tools?
<br>[00:57:30] I come in very opinionated on this too, but I need to reevaluate my opinions.
<br>[00:57:37] My opinions are forged in GitHub, where basically we were doing everything bare metal,
<br>[00:57:46] just talking directly to OpenAI. And I think, and I would encourage everyone to at least spend a
<br>[00:57:52] good deal of time talking directly to the model, because you'll gain a lot of intuition about
<br>[00:57:58] how these things think, really. It's kind of like you get to know your friend.
<br>[00:58:04] One of the things that DSP and LinkChain, to the extent, does that is frustrating when I run into
<br>[00:58:10] them is it hides what's happening and takes away some of the knobs and dials that you can turn.
<br>[00:58:19] That isn't to dismiss them though, like DSP. Someone was asking about how to do the best
<br>[00:58:26] few shot examples. My limited understanding of DSP is it does a good job about automatically
<br>[00:58:34] figuring that out for you and saving a lot of work for you. That's neat. So I hope to get more
<br>[00:58:42] familiar with a lot of those tools as well. This is a recurring theme throughout the conference
<br>[00:58:53] that spend more time talking to the model and you'll gain more understanding. I guess everyone
<br>[00:58:58] should do that a lot more. I'm just sifting through the questions, trying to pick the last two.
<br>[00:59:07] There was one that I really liked. Do you have any resources for prompting multimodal models?
<br>[00:59:16] Oh, no. I actually don't yet. That's a complete blind side in my experience right now.
<br>[00:59:24] But I will finish this book and then I will expand my horizons again and I look forward to getting into that.
<br>[00:59:31] Okay. Maybe there'll be an extra chapter in the book on this.
<br>[00:59:37] An extra addition or something. It'll all be different next year anyway.
<br>[00:59:44] There was also one that I wanted to ask. There's been this insane growth of different prompting
<br>[00:59:50] techniques right around chain of thought when chain of thought came out. Are there useful
<br>[00:59:56] like this tree of thought and there were so many that were just going viral at that time?
<br>[01:00:01] Do you find any others useful? Sure. I guess I can be saying are there any others that are worth
<br>[01:00:12] knowing outside of chain of thought and future of prompting? Absolutely. Two that come to mind
<br>[01:00:18] immediately. I forget the name. There's three. The two that come to mind that are actually
<br>[01:00:25] probably the better ones to talk about are React and I think it's reflect or reflect or reflect.
<br>[01:00:34] React is basically what you see when you see the typical function calling of an open AI assistant.
<br>[01:00:43] It is what it was patterned after. It says you have several functions that are defined.
<br>[01:00:48] Fake functions. This is in the olden days when it was just a prompt. It was not messages and
<br>[01:00:53] functions. This is fake functions. Can you figure out how to use a function to evaluate something
<br>[01:00:59] and then they have like one of the functions is special. It's like this is the answer function
<br>[01:01:06] and so when the model calls that you know you've got the answer. It's just a really nice way of
<br>[01:01:11] reaching out in the real world. It was kind of some of the early rag type stuff. It's where the
<br>[01:01:16] model gets to choose what it wants as opposed to jumping in rag manually. So that was a really
<br>[01:01:22] good pattern. Another pattern that almost piggybacks off of that is a reflective. Pretty sure I got
<br>[01:01:32] that right. But the idea is basically you do whatever you've got some sort of prompt that's
<br>[01:01:38] supposed to achieve a purpose and you're probably going to do that using React or something like
<br>[01:01:42] that. It achieves a purpose and here's the answer. Now reflective, it actually takes the answer and
<br>[01:01:48] it says is this really the answer? If it's a code, it runs it through like test, unit test.
<br>[01:01:56] It does whatever it can to check it and the error messages get piped back into the prompt and says
<br>[01:02:02] here's what you did. Here's the situation it led to. Can you learn from this and correct? You do
<br>[01:02:09] that few iterations and you have a much higher success rate. So I think that's kind of a neat
<br>[01:02:14] approach for making sure that answers are correct. 
<br>[01:02:19] Awesome. So many ideas to explore. I think I'll try to wrap up now. So thanks again for the
<br>[01:02:26] awesome talk and also taking the time to answer these questions. I'll put your books link and
<br>[01:02:32] your Twitter again in the Discord channel and I'll ask everyone in the Discord for an applause
<br>[01:02:37] for the talk. But thanks again for your time, John.
<br>[01:02:40] Yeah, thank you guys so much. I hope it was enjoyable.

:::
