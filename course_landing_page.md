# AI Evals For Engineers & PMs

**Featured in Lenny‚Äôs List**

## Instructors
- **Hamel Husain**: ML Engineer with 20 years of experience
- **Shreya Shankar**: ML Systems & Applied AI Evals Researcher

---

## Eliminate the guesswork of building AI applications with data-driven approaches.

All students in this course get:
- üóÑÔ∏è **Lifetime access** to all materials!
- ü§ñ 6 months of unlimited access to **our new AI Eval Assistant** (more info below).
- üßë‚Äçüè´ **8+ hours of office hours** to maximize the value of live interaction**.**
- üè´ **Lifetime Access to a Discord community** with 2k+ students and instructors.

---

**Do you catch yourself asking any of the following questions while building AI applications?**
1. How do I test applications when the outputs are stochastic and require subjective judgements?
2. If I change the prompt, how do I know I'm not breaking something else?
3. Where should I focus my engineering efforts? Do I need to test everything?
4. What if I have no data or customers, where do I start?
5. What metrics should I track? What tools should I use? Which models are best?
6. Can I automate testing and evaluation? If so, how do I trust it?

**If you aren't sure about the answers to these questions, this course is for you.**

**This is a flipped classroom setting**. All lectures are professionally edited and recorded with an emphasis on live office hours and student interaction.

---

## What you‚Äôll learn

Learn proven approaches for quickly improving AI applications. Build AI that works better than the competition, regardless of the use-case.

#### How To Collect Data For Evals
- Understand instrumentation and observability for tracking system behavior.
- Learn approaches for generating synthetic data to maximize error discovery and bootstrap product development.
- Understand how to choose the right tools and vendors for you, with deep dives into the most popular solutions in the evals space.

#### Get immediate clarity and direction with Error Analysis
- Apply data analysis techniques to rapidly find systematic issues in your product regardless of the use case.
- Master the processes and tools to annotate and analyze data quickly and efficiently.
- Learn how to analyze agentic systems (tool calls, RAG, etc.) to quickly identify systematic patterns and errors.

#### Implement Effective Evaluations
- Create evals that are customized to your product and provide immediate value, NOT generic off the shelf evals (which do not work).
- Align evals with stakeholders & domain experts that allow you to scientifically trust the evals.
- Create high-quality LLM-as-a-judge and code based evals with a systematic, iterative process.

#### Master Architecture-Specific Eval Strategies
- Learn how to measure & debug RAG systems for retrieval relevance and factual accuracy.
- Understand how to tame multi-step pipelines to identify error propagation and root-causes of errors quickly.
- Master techniques that apply to multi-modal settings, including text, image, and audio interactions.

#### Run Evals In Production
- Learn how to set up automated evaluation gates in CI/CD pipelines.
- Understand methods for consistent comparison across experiments, including how to prepare and maintain datasets to prevent overfitting.
- Implement safety and quality control guardrails.

#### Ensure That Evals Lead To High ROI
- Develop a strong intuition of when to write an eval, and when NOT write an eval.
- Learn how to design interfaces to remove friction from reviewing data and collect higher quality data with less effort.
- Learn how to avoid common pitfalls surrounding team organization, collaboration, responsibilities, tools, automation, and metrics.

---

## Learn directly from Hamel & Shreya

### Hamel Husain
**ML Engineer with 20 years of experience.**
Hamel Husain is a ML Engineer with over 20 years of experience. He has worked with innovative companies such as Airbnb and GitHub, which included early LLM research used by OpenAI, for code understanding. He has also led and contributed to numerous popular open-source machine-learning tools. Hamel is currently an independent consultant helping companies build AI products.
**Previously At**: Airbnb, GitHub, DataRobot, AlixPartners

### Shreya Shankar
**ML Systems Researcher Making AI Evaluation Work in Practice**
Shreya builds open-source systems for AI-powered data processing. She is a final-year PhD at UC Berkeley. Shreya created DocETL, an open-source system for analyzing unstructured text at scale. DocETL has been deployed across journalism, law, medicine, policy, finance, and urban planning. Her research has been published at top computer science venues including VLDB, SIGMOD, and UIST (including a Best Paper award). Before her PhD, Shreya worked as a machine learning and data engineer at startups. She holds a BS in Computer Science from Stanford University.
**Previously At**: Google, UC Berkeley, Stanford University

---

## Who this course is for

- Engineers & PMs building AI products who are interested in **moving beyond proof-of-concepts**.
- Those interested in moving beyond vibe-checks to data driven **measurements *you can trust*, even when outputs are stochastic or subjective**.
- Founders and leaders who are **unsure of the failure modes** of their AI applications and **where to allocate resources**.

---

## What's included

- **Live sessions**: 2-3 hrs / week. Lectures are professionally recorded & edited to save you time and cut out the fluff. We maximize live interaction through office hours and workshops.
- **Lifetime Access to All Recordings & Materials**: Revisit the materials and lectures anytime. Recordings and slides are made available to all students.
- **150+ Page Course Reader**: We provide a course reader with detailed notes to supplement your learning and act as a future reference as you work on evals.
- **Lifetime Access To Discord Community**: Private discord for questions, job leads, and ongoing support from the community (over 1000+ students and growing).
- **8+ Office Hour Q&As**: Open office hours for questions and personalized feedback.
- **4 Homework Assignments With Solutions & Walkthroughs**: Optional coding assignments & walkthrough videos so you can practice every concept.
- **Certificate of Completion**: Share your new skills with your employer or on LinkedIn.
- **Detailed Vendor & Tools Workshops**: Curated talks from industry experts working on evals, as well as workshops with vendors building eval tools.

---

## Syllabus

10 live sessions ‚Ä¢ 78 lessons

**Week 1**
- Start Here
- Lesson 1: Fundamentals & Lifecycle of Application-Centric Evals
- Lesson 2 & 3: Systematic Error Analysis
- FAQ and Links
- Office Hours

**Week 2**
- Start Here
- Lesson 4&5: Automated Evaluators
- Office Hours
- FAQ and Links

---

## Alumni reviews

- **Review:** "This course offers a thoughtful and distinctive perspective on evaluating AI features. I appreciated the adult-learning format, which lets you set your own pace. It not only clarifies key concepts but also strengthens your critical thinking around AI evaluation. The course doesn‚Äôt just introduce you to the learning materials, it also connects you with a growing community around AI Evals that this course has helped build. The course reader shared by Hamel and Shreya is well-written and spot-on, I found myself wanting to highlight nearly every line."
  - **Anubha**, Senior Software Engineer, Miro

- **Review:** "Coming from a research background, it was helpful to know best practices from an industry perspective and how to work on evaluation with PM and engineering colleagues."
  - **Jenny**, Applied Scientist

- **Review:** "Incredible course. Thoughtfully organized and thought provoking content. Definitely recommend for anyone looking to level up in evals."
  - **Mariah**, Group Product Manager, Intuit

- **Review:** "Well-constructed, incredibly helpful, thoughtful, and well-paced. I'm a newbie to coding and was able to complete the homework with the included workshops, discord, and some elbow grease. Well done, I'll be taking lessons learned into everything I do with AI from now on."
  - **Sean**, VP of Product, Quartz Network

- **Review:** "If you're interested in building evaluations for Gen AI applications, I highly recommend this course as it teaches you the right way to think about rigorous and systematic evaluations to build high quality products for your customers. This course will unpack concepts and unblocks one from and unblocks one from effectively testing, deploying, and optimizing their AI solutions in real-world scenarios."
  - **Ajita**, Senior PMT, Amazon

## Success stories

- **Quote:** "Hamel has provided exactly the tutorial I was needing for [evals], with a really thorough example case-study ... Hamel's content is fantastic, but it's a bit absurd that he's single-handedly having to make up for a lack of good materials about this topic across the rest of our industry!"
  - **Simon Willison**, Creator of Datasette

- **Quote:** "Hamel and is one of most knowledgeable people about LLM evals. I've witnessed him improve AI products first-hand by guiding his clients carefully through the process. We've even made many improvements to LangSmith because of his work."
  - **Harrison Chase**, CEO, Langchain

- **Quote:** "Shreya and Hamel are legit. Through their work on dozens of use cases, they've encountered and successfully addressed many of the common challenges in LLM evals. Every time I seek their advice, I come away with greater clarity and insight on how to solve my eval challenges."
  - **Eugene Yan**, Senior Applied Scientist

- **Quote:** "Hamel and Shreya technically goated, deeply experienced engineers of AI systems who just so happen to have impeccable vibes. I wouldn't learn this material from anyone else."
  - **Charles Frye**, Dev Advocate - Modal

- **Quote:** "When I have questions about the intersection of data and production AI systems, Shreya & Hamel are the first people I call. It's often the case that they've already written about my problem. You can‚Äôt find more qualified folks to teach this; anywhere."
  - **Bryan Bischof**, Director of Engineering, Hex

- **Quote:** "I was seeking help with LLM evaluation and testing for our products. Hamel's widely-referenced work on evals made him the clear choice. He helped us rethink our entire approach to LLM development and testing, creating a clear pathway to measure and improve our AI systems."
  - **George Siemens**, CEO, Matter & Space

---

## Frequently asked questions

**What happens if I can‚Äôt make a live session?**
All lessons are recorded and are made exclusively available to students in the course. Guest lectures will be made publicly available.

**I work full-time, what is the expected time commitment?**
Time commitment is 3-4 hours per week for 4 weeks.

**Do I need to be a coder to benefit from this course?**
No. While the course is titled "AI Evals For Engineers & Product Managers," you'll learn valuable evaluation processes whether you code or not. The course covers prioritization strategies, LLM judges, evaluation approaches, and analytical techniques for identifying AI failure modes. The instructors provide generous office hours to ensure everyone c

**Are there any prerequisites for the course?**
While the course is designed to be accessible to people without extensive ML or data science backgrounds, some familiarity with AI applications and basic understanding of how LLMs work would be helpful. The course is particularly well-suited for:
- Engineers building AI applications with limited ML experience
- Those looking to move beyond vibe checks

**What materials or resources will be provided?**
The course includes:
- Lifetime access to all course materials
- In-depth lessons with practical exercises
- Projects to apply what you've learned
- Access to a private community of peers
- $1,000 in free Modal compute credits per student
- Guided feedback and reflection
- Course certificate upon completion

**How is this different from other LLM or AI courses?**
This course stands out by focusing specifically on evaluation and improvement rather than just building AI systems. Rather than just teaching tools, this course delivers the processes for systematically improving AI applications regardless of the specific technologies used.

**What skills will I take away that I can apply immediately in my role?**
You'll gain practical skills for:
- Building custom annotation tools for error analysis
- Creating evaluation dashboards and data viewers
- Designing targeted test suites for specific AI architectures
- Implementing automated evaluation gates in CI/CD pipelines
- Developing strategies for collecting effective human feedback

**How does this course address evaluating early-stage AI products?**
You'll learn concrete methods for evaluation with limited or no user data:
- Synthetic data generation techniques
- Creating evaluation frameworks that scale with your product
- Balancing qualitative "vibe checks" with quantitative measurements
- Low-cost approaches to collect initial feedback
- Tests that uncover issues before users experience them

**How will this course prepare me for complex AI systems?**
The course covers evaluation for sophisticated AI architectures:
- Debugging interactions between multiple AI components
- Isolating failures in agent workflows
- Testing agent collaboration
- Measuring emergent behaviors
- Evaluating overall system reliability versus component performance

**Will I learn to align AI outputs with business needs?**
Yes. The course teaches you to:
- Translate business requirements into evaluation metrics
- Ensure AI alignment with brand values
- Measure what matters to users, not just generic metrics
- Balance competing priorities (accuracy vs. speed vs. cost)
- Communicate AI performance to stakeholders in business terms

**What tools and technologies will be covered?**
The course is tool-agnostic but will introduce you to popular frameworks and technologies for evaluation. The focus is on teaching approaches that work regardless of your specific tech stack. You'll learn evaluation patterns that can be implemented with various tools based on your organization's needs.

**Will I learn about evaluating specific types of AI?**
Yes. The course covers evaluation strategies for different AI architectures including RAG systems, chatbots, multi-step agents, and multi-modal systems. You'll learn techniques specific to each architecture's unique challenges and failure modes.



